##############
# 20/01/2026 #
##############

### Output 8 ###
root@MasterK8s:~/mec-lab/manifests# python3 ~/mec-lab/analyze_case.py \
  --input ~/mec-lab/logs/gateway_allpods_pd_heavy.jsonl \
  --case pd_heavy_allpods
cat ~/mec-lab/results/pd_heavy_allpods/summary.csv
=== KPI SUMMARY ===
case: pd_heavy_allpods
alg: pd
n_requests: 1354
duration_s_est: 301
throughput_rps_est: 4.498338870431894
latency_p50_ms: 135.5
latency_p95_ms: 259.0
latency_p99_ms: 267.0
sla_miss_rate: 0.4150664697193501
avg_queue_wait_ms: 49.91728212703102
avg_exec_ms: 95.94534711964549
Wrote: /root/mec-lab/results/pd_heavy_allpods
case,alg,n_requests,duration_s_est,throughput_rps_est,latency_p50_ms,latency_p95_ms,latency_p99_ms,sla_miss_rate,avg_queue_wait_ms,avg_exec_ms
pd_heavy_allpods,pd,1354,301,4.498338870431894,135.5,259.0,267.0,0.4150664697193501,49.91728212703102,95.94534711964549
root@MasterK8s:~/mec-lab/manifests#

root@MasterK8s:~/mec-lab/manifests# python3 ~/mec-lab/analyze_case.py \
  --input ~/mec-lab/logs/gateway_allpods_pd_heavy.jsonl \
  --case pd_heavy_allpods
cat ~/mec-lab/results/pd_heavy_allpods/summary.csv
=== KPI SUMMARY ===
case: pd_heavy_allpods
alg: pd
n_requests: 1354
duration_s_est: 301
throughput_rps_est: 4.498338870431894
latency_p50_ms: 135.5
latency_p95_ms: 259.0
latency_p99_ms: 267.0
sla_miss_rate: 0.4150664697193501
avg_queue_wait_ms: 49.91728212703102
avg_exec_ms: 95.94534711964549
Wrote: /root/mec-lab/results/pd_heavy_allpods
case,alg,n_requests,duration_s_est,throughput_rps_est,latency_p50_ms,latency_p95_ms,latency_p99_ms,sla_miss_rate,avg_queue_wait_ms,avg_exec_ms
pd_heavy_allpods,pd,1354,301,4.498338870431894,135.5,259.0,267.0,0.4150664697193501,49.91728212703102,95.94534711964549
root@MasterK8s:~/mec-lab/manifests#
root@MasterK8s:~/mec-lab/manifests#
root@MasterK8s:~/mec-lab/manifests# kubectl set env -n mec deploy/gateway-scheduler ALG=rr
kubectl rollout status -n mec deploy/gateway-scheduler
deployment.apps/gateway-scheduler env updated
Waiting for deployment "gateway-scheduler" rollout to finish: 0 out of 3 new replicas have been updated...
Waiting for deployment "gateway-scheduler" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "gateway-scheduler" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "gateway-scheduler" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "gateway-scheduler" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "gateway-scheduler" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "gateway-scheduler" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "gateway-scheduler" rollout to finish: 1 old replicas are pending termination...
Waiting for deployment "gateway-scheduler" rollout to finish: 1 old replicas are pending termination...
deployment "gateway-scheduler" successfully rolled out
root@MasterK8s:~/mec-lab/manifests#
root@MasterK8s:~/mec-lab/manifests#
root@MasterK8s:~/mec-lab/manifests# kubectl set env -n mec deploy/trafficgen LOAD=heavy DURATION_S=120 SEED=3
kubectl rollout status -n mec deploy/trafficgen
kubectl logs -n mec deploy/trafficgen --follow
deployment.apps/trafficgen env updated
Waiting for deployment "trafficgen" rollout to finish: 1 old replicas are pending termination...
Waiting for deployment "trafficgen" rollout to finish: 1 old replicas are pending termination...
Waiting for deployment "trafficgen" rollout to finish: 1 old replicas are pending termination...
deployment "trafficgen" successfully rolled out
Found 2 pods, using pod/trafficgen-5879f68b57-phrxc
Collecting requests
  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)
Collecting charset_normalizer<4,>=2 (from requests)
  Downloading charset_normalizer-3.4.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)
Collecting idna<4,>=2.5 (from requests)
  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)
Collecting urllib3<3,>=1.21.1 (from requests)
  Downloading urllib3-2.6.3-py3-none-any.whl.metadata (6.9 kB)
Collecting certifi>=2017.4.17 (from requests)
  Downloading certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)
Downloading requests-2.32.5-py3-none-any.whl (64 kB)
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 64.7/64.7 kB 3.5 MB/s eta 0:00:00
Downloading certifi-2026.1.4-py3-none-any.whl (152 kB)
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 152.9/152.9 kB 2.9 MB/s eta 0:00:00
Downloading charset_normalizer-3.4.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (151 kB)
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 151.6/151.6 kB 15.2 MB/s eta 0:00:00
Downloading idna-3.11-py3-none-any.whl (71 kB)
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 71.0/71.0 kB 46.2 MB/s eta 0:00:00
Downloading urllib3-2.6.3-py3-none-any.whl (131 kB)
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 131.6/131.6 kB 18.5 MB/s eta 0:00:00
Installing collected packages: urllib3, idna, charset_normalizer, certifi, requests
Successfully installed certifi-2026.1.4 charset_normalizer-3.4.4 idna-3.11 requests-2.32.5 urllib3-2.6.3
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv

[notice] A new release of pip is available: 24.0 -> 25.3
[notice] To update, run: pip install --upgrade pip
{"sent": 50, "ok": 50, "load": "heavy"}
{"sent": 100, "ok": 100, "load": "heavy"}
{"sent": 150, "ok": 150, "load": "heavy"}
{"sent": 200, "ok": 200, "load": "heavy"}
{"sent": 250, "ok": 250, "load": "heavy"}
{"sent": 300, "ok": 300, "load": "heavy"}
{"sent": 350, "ok": 350, "load": "heavy"}
{"sent": 400, "ok": 400, "load": "heavy"}
{"sent": 450, "ok": 450, "load": "heavy"}
{"sent": 500, "ok": 500, "load": "heavy"}
{"sent": 550, "ok": 550, "load": "heavy"}
{"sent": 600, "ok": 600, "load": "heavy"}
{"sent": 650, "ok": 650, "load": "heavy"}
{"sent": 700, "ok": 700, "load": "heavy"}
{"sent": 750, "ok": 750, "load": "heavy"}
{"done": true, "sent": 758, "ok": 758, "load": "heavy", "duration_s": 120}
root@MasterK8s:~/mec-lab/manifests#

root@MasterK8s:~/mec-lab/manifests# for p in $(kubectl get pods -n mec -l app=gateway-scheduler -o name); do
  out=~/mec-lab/logs/$(echo $p | tr '/' '_' ).log
  kubectl logs -n mec $p --since=5m > $out
done
cat ~/mec-lab/logs/pod_gateway-scheduler-*.log > ~/mec-lab/logs/gateway_allpods_raw.log
grep '^{' ~/mec-lab/logs/gateway_allpods_raw.log > ~/mec-lab/logs/gateway_allpods_rr_heavy.jsonl
wc -l ~/mec-lab/logs/gateway_allpods_rr_heavy.jsonl
2454 /root/mec-lab/logs/gateway_allpods_rr_heavy.jsonl

root@MasterK8s:~/mec-lab/manifests# python3 ~/mec-lab/analyze_case.py --input ~/mec-lab/logs/gateway_allpods_rr_heavy.jsonl --case rr_heavy_allpods
=== KPI SUMMARY ===
case: rr_heavy_allpods
alg: pd
n_requests: 2454
duration_s_est: 1035
throughput_rps_est: 2.371014492753623
latency_p50_ms: 124.5
latency_p95_ms: 257.0
latency_p99_ms: 266.0
sla_miss_rate: 0.3895680521597392
avg_queue_wait_ms: 47.466585167074165
avg_exec_ms: 94.27057864710676
Wrote: /root/mec-lab/results/rr_heavy_allpods



### L·ªãch s·ª≠ 8 ###
B∆∞·ªõc 3 ‚Äî Ph√¢n t√≠ch case pd_heavy_allpods
3.1 N·∫øu anh ch∆∞a c√≥ analyze_case.py th√¨ t·∫°o l·∫°i (1 l·∫ßn)

Ch·∫°y:

ls -la ~/mec-lab/analyze_case.py


N·∫øu file t·ªìn t·∫°i ‚Üí b·ªè qua b∆∞·ªõc t·∫°o.

N·∫øu kh√¥ng t·ªìn t·∫°i ‚Üí t·∫°o nhanh:

cat > ~/mec-lab/analyze_case.py <<'EOF'
import argparse, json
from pathlib import Path
import pandas as pd

ap = argparse.ArgumentParser()
ap.add_argument("--input", required=True)
ap.add_argument("--case", required=True)
args = ap.parse_args()

inp = Path(args.input)
outdir = Path(__file__).parent / "results" / args.case
outdir.mkdir(parents=True, exist_ok=True)

rows = []
for line in inp.read_text(encoding="utf-8").splitlines():
    line = line.strip()
    if not line.startswith("{"):
        continue
    try:
        rows.append(json.loads(line))
    except json.JSONDecodeError:
        pass

df = pd.DataFrame(rows)
df["ts"] = pd.to_numeric(df["ts"], errors="coerce")
df = df.dropna(subset=["ts"])
df["ts"] = df["ts"].astype(int)

duration_s = max(1, df["ts"].max() - df["ts"].min() + 1)
summary = {
  "case": args.case,
  "alg": df["alg"].mode().iloc[0],
  "n_requests": len(df),
  "duration_s_est": duration_s,
  "throughput_rps_est": len(df)/duration_s,
  "latency_p50_ms": float(df["e2e_latency_ms"].quantile(0.50)),
  "latency_p95_ms": float(df["e2e_latency_ms"].quantile(0.95)),
  "latency_p99_ms": float(df["e2e_latency_ms"].quantile(0.99)),
  "sla_miss_rate": float(df["sla_miss"].mean()),
  "avg_queue_wait_ms": float(df["queue_wait_ms"].mean()),
  "avg_exec_ms": float(df["exec_ms"].mean()),
}
pd.DataFrame([summary]).to_csv(outdir / "summary.csv", index=False)
df.to_csv(outdir / "raw.csv", index=False)

print("=== KPI SUMMARY ===")
for k,v in summary.items():
    print(f"{k}: {v}")
print(f"Wrote: {outdir}")
EOF

3.2 Ch·∫°y ph√¢n t√≠ch
python3 ~/mec-lab/analyze_case.py \
  --input ~/mec-lab/logs/gateway_allpods_pd_heavy.jsonl \
  --case pd_heavy_allpods
cat ~/mec-lab/results/pd_heavy_allpods/summary.csv


üëâ G·ª≠i t√¥i output c·ªßa summary.csv ƒë·ªÉ t√¥i ch·ªët KPI v√† so s√°nh.

B∆∞·ªõc 4 ‚Äî Ch·∫°y th√™m 2 case heavy (RR v√† latency) theo c√πng quy tr√¨nh
Case A: rr_heavy_allpods

Set ALG:

kubectl set env -n mec deploy/gateway-scheduler ALG=rr
kubectl rollout status -n mec deploy/gateway-scheduler


Ch·∫°y trafficgen 120s:

kubectl set env -n mec deploy/trafficgen LOAD=heavy DURATION_S=120 SEED=3
kubectl rollout status -n mec deploy/trafficgen
kubectl logs -n mec deploy/trafficgen --follow


Th·∫•y "done": true ‚Üí Ctrl+C.

Gom log 3 pods (gi·ªëng y h·ªát, ch·ªâ ƒë·ªïi t√™n file):

for p in $(kubectl get pods -n mec -l app=gateway-scheduler -o name); do
  out=~/mec-lab/logs/$(echo $p | tr '/' '_' ).log
  kubectl logs -n mec $p --since=5m > $out
done
cat ~/mec-lab/logs/pod_gateway-scheduler-*.log > ~/mec-lab/logs/gateway_allpods_raw.log
grep '^{' ~/mec-lab/logs/gateway_allpods_raw.log > ~/mec-lab/logs/gateway_allpods_rr_heavy.jsonl
wc -l ~/mec-lab/logs/gateway_allpods_rr_heavy.jsonl


Analyze:

python3 ~/mec-lab/analyze_case.py --input ~/mec-lab/logs/gateway_allpods_rr_heavy.jsonl --case rr_heavy_allpods

Case B: latency_heavy_allpods
L√†m y h·ªát, ch·ªâ ƒë·ªïi:
ALG=latency
file: gateway_allpods_latency_heavy.jsonl
case: latency_heavy_allpods

# B1) Set thu·∫≠t to√°n gateway = latency
kubectl set env -n mec deploy/gateway-scheduler ALG=latency
kubectl rollout status -n mec deploy/gateway-scheduler

# B2) Ch·∫°y trafficgen heavy 120s (ƒë·ªïi SEED ƒë·ªÉ kh√°c case tr∆∞·ªõc)
kubectl set env -n mec deploy/trafficgen LOAD=heavy DURATION_S=120 SEED=4
kubectl rollout status -n mec deploy/trafficgen

# B3) Follow log ƒë·∫øn khi th·∫•y "done": true, r·ªìi Ctrl+C
kubectl logs -n mec deploy/trafficgen --follow

# B4) Export log 3 pods gateway (5 ph√∫t g·∫ßn nh·∫•t) + merge + l·ªçc JSON
rm -f ~/mec-lab/logs/pod_gateway-scheduler-*.log
for p in $(kubectl get pods -n mec -l app=gateway-scheduler -o name); do
  out=~/mec-lab/logs/$(echo $p | tr '/' '_' ).log
  kubectl logs -n mec $p --since=5m > $out
done

cat ~/mec-lab/logs/pod_gateway-scheduler-*.log > ~/mec-lab/logs/gateway_allpods_raw.log
grep '^{' ~/mec-lab/logs/gateway_allpods_raw.log > ~/mec-lab/logs/gateway_allpods_latency_heavy.jsonl

# B5) Quick check dataset
wc -l ~/mec-lab/logs/gateway_allpods_latency_heavy.jsonl
tail -n 2 ~/mec-lab/logs/gateway_allpods_latency_heavy.jsonl

# B6) Analyze + in summary
python3 ~/mec-lab/analyze_case.py \
  --input ~/mec-lab/logs/gateway_allpods_latency_heavy.jsonl \
  --case latency_heavy_allpods

cat ~/mec-lab/results/latency_heavy_allpods/summary.csv


Anh g·ª≠i ƒë√∫ng 1 th·ª© ƒë·ªÉ t√¥i x√°c nh·∫≠n v√† h∆∞·ªõng d·∫´n ti·∫øp (kh√¥ng d√†i)

Sau khi ch·∫°y B∆∞·ªõc 3.2, g·ª≠i n·ªôi dung:

cat ~/mec-lab/results/pd_heavy_allpods/summary.csv


T√¥i s·∫Ω x√°c nh·∫≠n PASS v√† chu·∫©n b·ªã ‚Äúb·∫£ng so s√°nh 3 thu·∫≠t to√°n heavy (3 replicas)‚Äù theo format ƒë∆∞a v√†o lu·∫≠n vƒÉn.


### Output 7 ###
root@MasterK8s:~/mec-lab/manifests# kubectl get pods -n mec -l app=gateway-scheduler -o name
pod/gateway-scheduler-5ff998d99-4qv4s
pod/gateway-scheduler-5ff998d99-dgf4k
pod/gateway-scheduler-5ff998d99-kj84z
root@MasterK8s:~/mec-lab/manifests#
root@MasterK8s:~/mec-lab/manifests# rm -f ~/mec-lab/logs/gw_pod_*.log ~/mec-lab/logs/gateway_allpods_raw.log ~/mec-lab/logs/gateway_allpods_pd_heavy.jsonl

for p in $(kubectl get pods -n mec -l app=gateway-scheduler -o name); do
  out=~/mec-lab/logs/$(echo $p | tr '/' '_' ).log
  kubectl logs -n mec $p --since=5m > $out
done

wc -l ~/mec-lab/logs/pod_gateway-scheduler-*.log
   872 /root/mec-lab/logs/pod_gateway-scheduler-5ff998d99-4qv4s.log
   966 /root/mec-lab/logs/pod_gateway-scheduler-5ff998d99-dgf4k.log
   870 /root/mec-lab/logs/pod_gateway-scheduler-5ff998d99-kj84z.log
  2708 total
root@MasterK8s:~/mec-lab/manifests#
root@MasterK8s:~/mec-lab/manifests# cat ~/mec-lab/logs/pod_gateway-scheduler-*.log > ~/mec-lab/logs/gateway_allpods_raw.log
grep '^{' ~/mec-lab/logs/gateway_allpods_raw.log > ~/mec-lab/logs/gateway_allpods_pd_heavy.jsonl

wc -l ~/mec-lab/logs/gateway_allpods_pd_heavy.jsonl
tail -n 2 ~/mec-lab/logs/gateway_allpods_pd_heavy.jsonl
1354 /root/mec-lab/logs/gateway_allpods_pd_heavy.jsonl
{"ts": 1768895001, "pod": "gateway-scheduler-5ff998d99-kj84z", "alg": "pd", "request_id": "e4ee6659-9d18-4826-a3f6-5f56a46a9c87", "cpu_ms": 50, "deadline_ms": 120, "priority": 1, "task_size": 1024, "decision_site": "cloud", "queue_wait_ms": 44, "exec_ms": 50, "e2e_latency_ms": 94, "sla_miss": false}
{"ts": 1768895002, "pod": "gateway-scheduler-5ff998d99-kj84z", "alg": "pd", "request_id": "e7dd3c79-712d-41d3-9675-e419ba051b77", "cpu_ms": 200, "deadline_ms": 80, "priority": 3, "task_size": 1024, "decision_site": "cloud", "queue_wait_ms": 45, "exec_ms": 200, "e2e_latency_ms": 245, "sla_miss": true}
root@MasterK8s:~/mec-lab/manifests#

‚úÖ PASS. ƒê√¢y ch√≠nh l√† dataset ‚Äúallpods/3 replicas‚Äù ƒë√∫ng chu·∫©n.
3 pods gateway ƒë·ªÅu c√≥ log ƒë√°ng k·ªÉ (872 / 966 / 870 d√≤ng).
File JSONL h·ª£p l·ªá: 1354 d√≤ng JSON.
C√≥ c·∫£ sla_miss: true/false, ƒë·ªß ƒë·ªÉ t√≠nh KPI tail latency + miss rate.
Gi·ªù b∆∞·ªõc ti·∫øp theo (ng·∫Øn, kh√¥ng nhi·ªÖu): ph√¢n t√≠ch KPI cho allpods v√† ch·∫°y th√™m 2 case heavy c√≤n l·∫°i (rr, latency) ƒë·ªÉ c√≥ b·∫£ng so s√°nh 3 thu·∫≠t to√°n.

### L·ªãch s·ª≠ 7 ###
2) √ù nghƒ©a KPI anh v·ª´a c√≥ (ƒë√∫ng ng√¥n ng·ªØ lu·∫≠n vƒÉn)

Anh ƒëang ·ªü case: ALG=pd, LOAD=heavy

KPI Summary (anh c√≥)

n_requests = 1540
S·ªë t√°c v·ª• (tasks) ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω v√† log h·ª£p l·ªá.

duration_s_est = 872
Th·ªùi l∆∞·ª£ng ∆∞·ªõc t√≠nh d·ª±a tr√™n timestamp log.
L∆∞u √Ω: Con s·ªë n√†y kh√¥ng nh·∫•t thi·∫øt b·∫±ng 180s v√¨:

log ƒëang l·∫•y --since=15m v√† gateway c√≥ th·ªÉ ghi r·∫£i r√°c theo th·ªùi gian;

v√†/ho·∫∑c ch·ªâ log t·ª´ 1 pod gateway (t√¥i s·∫Ω ch·ªânh ·ªü m·ª•c 4 ƒë·ªÉ gom ƒë·ªß 3 pods).

throughput_rps_est = 1.766 req/s
Th√¥ng l∆∞·ª£ng ∆∞·ªõc t√≠nh t·ª´ log.

latency_p50 = 137 ms
50% t√°c v·ª• c√≥ E2E latency ‚â§ 137ms.

latency_p95 = 255 ms, p99 = 263 ms
ƒê·ªô tr·ªÖ ƒëu√¥i (tail latency). ƒê√¢y l√† ph·∫ßn h·ªôi ƒë·ªìng hay h·ªèi nh·∫•t.

sla_miss_rate = 0.386 (~38.6%)
T·ª∑ l·ªá vi ph·∫°m deadline. ƒê√¢y l√† ch·ªâ s·ªë then ch·ªët ƒë·ªÉ so s√°nh RR vs PD vs Latency-aware.

avg_queue_wait = 49.35 ms
ƒê·ªô tr·ªÖ do h√†ng ƒë·ª£i (m·ªôt ph·∫ßn c·ªßa E2E latency), gi√∫p gi·∫£i th√≠ch ‚Äúbottleneck‚Äù.

avg_exec = 94.75 ms
Trung b√¨nh ‚Äúcpu_ms‚Äù m√† anh m√¥ ph·ªèng.

üëâ V·ªõi c√°c s·ªë n√†y, anh c√≥ th·ªÉ vi·∫øt 2‚Äì3 ƒëo·∫°n ph√¢n t√≠ch ngay:

PD c√≥ th·ªÉ ∆∞u ti√™n deadline/priority nh∆∞ng ·ªü heavy load v·∫´n g√¢y SLA miss cao do queue buildup.

Tail latency (p95/p99) quan tr·ªçng h∆°n average.

3) ƒêi·ªÅu quan tr·ªçng c·∫ßn s·ª≠a ƒë·ªÉ ‚Äútest scale 3 pods‚Äù ƒë√∫ng nghƒ©a

Hi·ªán t·∫°i khi export log, kubectl b√°o:

Found 3 pods, using pod/...

T·ª©c l√† anh ƒëang l·∫•y log c·ªßa 1 pod gateway, kh√¥ng ph·∫£i c·∫£ 3.

Mu·ªën lu·∫≠n vƒÉn ‚Äúscale-out 3 replicas‚Äù ch·∫∑t ch·∫Ω, anh c·∫ßn gom log c·∫£ 3 pods.

C√°ch ƒë√∫ng (gom theo label)

Ch·∫°y:

kubectl logs -n mec -l app=gateway-scheduler --since=15m --prefix=true > ~/mec-lab/logs/gateway_allpods_raw.log
grep '^{' ~/mec-lab/logs/gateway_allpods_raw.log > ~/mec-lab/logs/gateway_allpods_pd_heavy.jsonl
wc -l ~/mec-lab/logs/gateway_allpods_pd_heavy.jsonl
tail -n 2 ~/mec-lab/logs/gateway_allpods_pd_heavy.jsonl


K·ª≥ v·ªçng: s·ªë d√≤ng s·∫Ω tƒÉng ƒë√°ng k·ªÉ so v·ªõi 1540 (v√¨ 3 pods c√πng x·ª≠ l√Ω).

üëâ N·∫øu anh mu·ªën, t√¥i s·∫Ω c·∫≠p nh·∫≠t analyze_logs.py ƒë·ªÉ ƒë·ªçc file gateway_allpods_*.jsonl v√† t·ª± g·∫Øn nh√£n ‚Äúreplicas=3‚Äù.

4) B√¢y gi·ªù l√†m g√¨ ƒë·ªÉ ho√†n ch·ªânh lu·∫≠n vƒÉn?

Anh ƒë√£ c√≥ 1 case ‚Äúpd + heavy‚Äù. Lu·∫≠n vƒÉn c·∫ßn b·∫£ng so s√°nh t·ªëi thi·ªÉu:

ALG	LOAD	throughput	p95	p99	SLA miss
rr	light	‚Ä¶	‚Ä¶	‚Ä¶	‚Ä¶
rr	medium	‚Ä¶	‚Ä¶	‚Ä¶	‚Ä¶
rr	heavy	‚Ä¶	‚Ä¶	‚Ä¶	‚Ä¶
pd	light	‚Ä¶	‚Ä¶	‚Ä¶	‚Ä¶
pd	medium	‚Ä¶	‚Ä¶	‚Ä¶	‚Ä¶
pd	heavy	(ƒë√£ c√≥)	(ƒë√£ c√≥)	(ƒë√£ c√≥)	(ƒë√£ c√≥)
latency	light	‚Ä¶	‚Ä¶	‚Ä¶	‚Ä¶
latency	medium	‚Ä¶	‚Ä¶	‚Ä¶	‚Ä¶
latency	heavy	‚Ä¶	‚Ä¶	‚Ä¶	‚Ä¶
5) T√¥i ƒë∆∞a lu√¥n ‚ÄúRunbook 9-case‚Äù ƒë·ªÉ anh ch·∫°y li√™n t·ª•c (kh√¥ng h·ªèi l·∫°i)
5.1 Th·ªëng nh·∫•t t√™n file log & k·∫øt qu·∫£ theo case

Quy ∆∞·ªõc:

log JSONL: ~/mec-lab/logs/gateway_<alg>_<load>.jsonl

k·∫øt qu·∫£: ~/mec-lab/results/<alg>_<load>/...

5.2 L·ªánh ch·∫°y 1 case (copy/paste, ch·ªâ ƒë·ªïi alg/load)
(A) Set thu·∫≠t to√°n
kubectl set env -n mec deploy/gateway-scheduler ALG=rr
kubectl rollout status -n mec deploy/gateway-scheduler

(B) Set load + ch·∫°y trafficgen 180s
kubectl set env -n mec deploy/trafficgen LOAD=medium DURATION_S=180 SEED=1
kubectl rollout status -n mec deploy/trafficgen
kubectl logs -n mec deploy/trafficgen --tail=5

(C) Export log (gom c·∫£ 3 pods) + l·ªçc JSON
kubectl logs -n mec -l app=gateway-scheduler --since=10m --prefix=true > ~/mec-lab/logs/gateway_raw.log
grep '^{' ~/mec-lab/logs/gateway_raw.log > ~/mec-lab/logs/gateway_rr_medium.jsonl
wc -l ~/mec-lab/logs/gateway_rr_medium.jsonl

(D) Ph√¢n t√≠ch (t√¥i s·∫Ω c·∫≠p nh·∫≠t script ƒë·ªÉ nh·∫≠n input theo tham s·ªë)

Ngay d∆∞·ªõi ƒë√¢y t√¥i g·ª≠i phi√™n b·∫£n analyze_logs.py nh·∫≠n input file ƒë·ªÉ anh ch·∫°y t·ª´ng case.

6) C·∫≠p nh·∫≠t script ph√¢n t√≠ch: ch·∫°y ƒë∆∞·ª£c cho m·ªçi case (kh√¥ng ph·∫£i s·ª≠a code)

T·∫°o file m·ªõi: ~/mec-lab/analyze_case.py

cat > ~/mec-lab/analyze_case.py <<'EOF'
import argparse, json
from pathlib import Path
import pandas as pd
import matplotlib.pyplot as plt

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--input", required=True, help="path to jsonl log file")
    ap.add_argument("--case", required=True, help="case name e.g. rr_medium")
    args = ap.parse_args()

    inp = Path(args.input)
    outdir = Path(__file__).parent / "results" / args.case
    outdir.mkdir(parents=True, exist_ok=True)

    rows = []
    with inp.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line.startswith("{"):
                continue
            try:
                rows.append(json.loads(line))
            except json.JSONDecodeError:
                continue

    if not rows:
        raise SystemExit(f"No JSON rows found in {inp}")

    df = pd.DataFrame(rows)

    required = ["ts","alg","request_id","cpu_ms","deadline_ms","priority","decision_site","queue_wait_ms","exec_ms","e2e_latency_ms","sla_miss"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise SystemExit(f"Missing columns: {missing}")

    df["ts"] = pd.to_numeric(df["ts"], errors="coerce")
    df = df.dropna(subset=["ts"])
    df["ts"] = df["ts"].astype(int)

    duration_s = max(1, df["ts"].max() - df["ts"].min() + 1)
    throughput_rps = len(df) / duration_s

    summary = pd.DataFrame([{
        "case": args.case,
        "alg": df["alg"].mode().iloc[0],
        "n_requests": len(df),
        "duration_s_est": duration_s,
        "throughput_rps_est": throughput_rps,
        "latency_p50_ms": df["e2e_latency_ms"].quantile(0.50),
        "latency_p95_ms": df["e2e_latency_ms"].quantile(0.95),
        "latency_p99_ms": df["e2e_latency_ms"].quantile(0.99),
        "sla_miss_rate": df["sla_miss"].mean(),
        "avg_queue_wait_ms": df["queue_wait_ms"].mean(),
        "avg_exec_ms": df["exec_ms"].mean(),
    }])

    df.to_csv(outdir / "raw.csv", index=False)
    summary.to_csv(outdir / "summary.csv", index=False)

    print("\n=== KPI SUMMARY ===")
    print(summary.to_string(index=False))

    # Plot 1: latency CDF
    lat = df["e2e_latency_ms"].sort_values().reset_index(drop=True)
    y = (lat.index + 1) / len(lat)
    plt.figure()
    plt.plot(lat, y)
    plt.xlabel("E2E latency (ms)")
    plt.ylabel("CDF")
    plt.title(f"E2E Latency CDF ({args.case})")
    plt.grid(True)
    plt.savefig(outdir / "latency_cdf.png", dpi=200)

    # Plot 2: SLA miss rate over time (10s bins)
    df2 = df.sort_values("ts").copy()
    df2["sla_miss_int"] = df2["sla_miss"].astype(int)
    df2["tbin"] = (df2["ts"] // 10) * 10
    miss_rate_by_bin = df2.groupby("tbin")["sla_miss_int"].mean().reset_index()
    plt.figure()
    plt.plot(miss_rate_by_bin["tbin"], miss_rate_by_bin["sla_miss_int"])
    plt.xlabel("Time bin (epoch seconds, 10s bins)")
    plt.ylabel("SLA miss rate")
    plt.title(f"SLA Miss Rate over Time ({args.case})")
    plt.grid(True)
    plt.savefig(outdir / "sla_miss.png", dpi=200)

    # Plot 3: decision site share
    site_share = df["decision_site"].value_counts(normalize=True).sort_index()
    plt.figure()
    plt.bar(site_share.index, site_share.values)
    plt.xlabel("Decision site")
    plt.ylabel("Share")
    plt.title(f"Offloading Share by Site ({args.case})")
    plt.grid(True, axis="y")
    plt.savefig(outdir / "site_share.png", dpi=200)

    # Plot 4: queue wait vs cpu
    plt.figure()
    plt.scatter(df["cpu_ms"], df["queue_wait_ms"], s=8)
    plt.xlabel("cpu_ms (task demand)")
    plt.ylabel("queue_wait_ms")
    plt.title(f"Queue Wait vs CPU Demand ({args.case})")
    plt.grid(True)
    plt.savefig(outdir / "queue_vs_cpu.png", dpi=200)

    print(f"\nWrote results to: {outdir}")

if __name__ == "__main__":
    main()
EOF


Ch·∫°y th·ª≠ cho case anh v·ª´a c√≥ (pd_heavy):

python3 ~/mec-lab/analyze_case.py --input ~/mec-lab/logs/gateway_pd_heavy.jsonl --case pd_heavy
ls -la ~/mec-lab/results/pd_heavy

7) Anh n√™n l√†m ti·∫øp ngay b√¢y gi·ªù

Chuy·ªÉn sang gom log c·∫£ 3 pods (M·ª•c 3) ƒë·ªÉ ‚Äúscale test‚Äù ƒë√∫ng nghƒ©a.

Ch·∫°y script analyze_case.py ƒë·ªÉ t·∫°o structure k·∫øt qu·∫£ theo case.

Ch·∫°y th√™m 2 case n·ªØa tr∆∞·ªõc ƒë·ªÉ ki·ªÉm ch·ª©ng:

rr_heavy

latency_heavy

Anh ch·ªçn h∆∞·ªõng:

N·∫øu anh mu·ªën ƒëi nhanh: t√¥i s·∫Ω ƒë∆∞a script bash ch·∫°y t·ª± ƒë·ªông 9 case v√† xu·∫•t ra 1 file summary_all.csv ƒë·ªÉ copy v√†o lu·∫≠n vƒÉn.

N·∫øu anh mu·ªën ƒëi ch·∫Øc: ta ch·∫°y th√™m 1 case rr_heavy tr∆∞·ªõc, t√¥i check KPI r·ªìi m·ªõi batch.

Ch·ªâ c·∫ßn anh tr·∫£ l·ªùi: ‚Äúƒêi nhanh ch·∫°y 9 case‚Äù ho·∫∑c ‚ÄúCh·∫°y rr_heavy tr∆∞·ªõc‚Äù.


### Output 6 ###
root@MasterK8s:~/mec-lab/manifests# mkdir -p ~/mec-lab/logs
root@MasterK8s:~/mec-lab/manifests# kubectl logs -n mec deploy/gateway-scheduler --since=15m > ~/mec-lab/logs/gateway_raw.log
Found 3 pods, using pod/gateway-scheduler-5ff998d99-4qv4s
root@MasterK8s:~/mec-lab/manifests# grep '^{' ~/mec-lab/logs/gateway_raw.log > ~/mec-lab/logs/gateway_pd_heavy.jsonl
root@MasterK8s:~/mec-lab/manifests# wc -l ~/mec-lab/logs/gateway_pd_heavy.jsonl
tail -n 2 ~/mec-lab/logs/gateway_pd_heavy.jsonl
1540 /root/mec-lab/logs/gateway_pd_heavy.jsonl
{"ts": 1768892996, "pod": "gateway-scheduler-5ff998d99-4qv4s", "alg": "pd", "request_id": "d75101d1-5602-4596-b2cd-123a753965d5", "cpu_ms": 50, "deadline_ms": 80, "priority": 2, "task_size": 2048, "decision_site": "mec2", "queue_wait_ms": 43, "exec_ms": 50, "e2e_latency_ms": 93, "sla_miss": true}
{"ts": 1768892997, "pod": "gateway-scheduler-5ff998d99-4qv4s", "alg": "pd", "request_id": "6dd0abaf-9778-43e4-979f-850c6ebee4dc", "cpu_ms": 100, "deadline_ms": 80, "priority": 3, "task_size": 1024, "decision_site": "mec2", "queue_wait_ms": 40, "exec_ms": 100, "e2e_latency_ms": 140, "sla_miss": true}
root@MasterK8s:~/mec-lab/manifests#

root@MasterK8s:~/mec-lab/manifests# python3 ~/mec-lab/analyze_logs.py
=== KPI SUMMARY ===
       case alg  n_requests  duration_s_est  throughput_rps_est  latency_p50_ms  latency_p95_ms  latency_p99_ms  sla_miss_rate  avg_queue_wait_ms  avg_exec_ms
pd_pd_heavy  pd        1540             872            1.766055           137.0           255.0           263.0       0.386364          49.351299    94.753247

Wrote results to: /root/mec-lab/results
root@MasterK8s:~/mec-lab/manifests#

root@MasterK8s:~/mec-lab/manifests# ls -la ~/mec-lab/results
total 476
drwxr-xr-x 2 root root   4096 Jan 20 14:17 .
drwxr-xr-x 5 root root   4096 Jan 20 14:17 ..
-rw-r--r-- 1 root root  48840 Jan 20 14:17 latency_cdf_pd_heavy.png
-rw-r--r-- 1 root root  69745 Jan 20 14:17 queue_vs_cpu_pd_heavy.png
-rw-r--r-- 1 root root 184743 Jan 20 14:17 raw_gateway_pd_heavy.csv
-rw-r--r-- 1 root root  43631 Jan 20 14:17 site_share_pd_heavy.png
-rw-r--r-- 1 root root 115610 Jan 20 14:17 sla_miss_pd_heavy.png
-rw-r--r-- 1 root root    259 Jan 20 14:17 summary_pd_heavy.csv

1) Check output: PASS/FAIL
‚úÖ A) Export log JSONL s·∫°ch ‚Äî PASS
gateway_pd_heavy.jsonl c√≥ 1540 d√≤ng JSON (ƒë·ªß l·ªõn) ‚úÖ
tail -n 2 l√† JSON h·ª£p l·ªá ‚úÖ
C√≥ sla_miss: true ‚úÖ (quan tr·ªçng: ch·ª©ng minh m√¥ h√¨nh c√≥ vi ph·∫°m deadline, kh√¥ng ‚Äúƒë·∫πp gi·∫£‚Äù).

‚úÖ B) Script ph√¢n t√≠ch ch·∫°y OK ‚Äî PASS
Anh ƒë√£ t·∫°o ƒë∆∞·ª£c:
KPI summary ‚úÖ
CSV raw + summary ‚úÖ
4 h√¨nh PNG ‚úÖ

‚úÖ C) Result files ƒë√∫ng nh∆∞ k·ª≥ v·ªçng ‚Äî PASS
Th∆∞ m·ª•c ~/mec-lab/results c√≥ ƒë·∫ßy ƒë·ªß:
raw_gateway_pd_heavy.csv
summary_pd_heavy.csv
latency_cdf_pd_heavy.png
sla_miss_pd_heavy.png
site_share_pd_heavy.png
queue_vs_cpu_pd_heavy.png

=> ƒê√¢y l√† deliverable cho lu·∫≠n vƒÉn.

### L·ªãch s·ª≠ 6 ###
2) S·ª≠a chu·∫©n: xu·∫•t log JSONL s·∫°ch (kh√¥ng l·∫´n INFO)

Tr√™n masterk8s, ch·∫°y ƒë√∫ng 3 l·ªánh sau (copy/paste nguy√™n kh·ªëi):

mkdir -p ~/mec-lab/logs

kubectl logs -n mec deploy/gateway-scheduler --since=15m > ~/mec-lab/logs/gateway_raw.log

grep '^{' ~/mec-lab/logs/gateway_raw.log > ~/mec-lab/logs/gateway_pd_heavy.jsonl

wc -l ~/mec-lab/logs/gateway_pd_heavy.jsonl
tail -n 2 ~/mec-lab/logs/gateway_pd_heavy.jsonl

Output k·ª≥ v·ªçng

wc -l ph·∫£i > 0 (th∆∞·ªùng v√†i trƒÉm ƒë·∫øn v√†i ngh√¨n)

tail ra 2 d√≤ng JSON b·∫Øt ƒë·∫ßu b·∫±ng {

üëâ N·∫øu anh ch·∫°y xong, paste output wc -l v√† tail -n 2.

3) T·∫°o script t√≠nh KPI + xu·∫•t CSV + v·∫Ω bi·ªÉu ƒë·ªì (ƒë·ªÉ ƒë∆∞a v√†o lu·∫≠n vƒÉn)
3.1 T·∫°o file ph√¢n t√≠ch analyze_logs.py

T·∫°o file ·ªü: ~/mec-lab/analyze_logs.py

cat > ~/mec-lab/analyze_logs.py <<'EOF'
import json
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path

INPUT = Path(__file__).parent / "logs" / "gateway_pd_heavy.jsonl"
OUTDIR = Path(__file__).parent / "results"
OUTDIR.mkdir(parents=True, exist_ok=True)

rows = []
with INPUT.open("r", encoding="utf-8") as f:
    for line in f:
        line = line.strip()
        if not line.startswith("{"):
            continue
        try:
            rows.append(json.loads(line))
        except json.JSONDecodeError:
            continue

if not rows:
    raise SystemExit(f"No JSON rows found in {INPUT}")

df = pd.DataFrame(rows)

# Basic sanity checks
required = ["ts","alg","request_id","cpu_ms","deadline_ms","priority","decision_site","queue_wait_ms","exec_ms","e2e_latency_ms","sla_miss"]
missing = [c for c in required if c not in df.columns]
if missing:
    raise SystemExit(f"Missing columns: {missing}")

# Compute throughput: count per second (based on ts)
df["ts"] = pd.to_numeric(df["ts"], errors="coerce")
df = df.dropna(subset=["ts"])
df["ts"] = df["ts"].astype(int)
duration_s = max(1, df["ts"].max() - df["ts"].min() + 1)
throughput_rps = len(df) / duration_s

# KPI summary
lat_p50 = df["e2e_latency_ms"].quantile(0.50)
lat_p95 = df["e2e_latency_ms"].quantile(0.95)
lat_p99 = df["e2e_latency_ms"].quantile(0.99)
sla_miss_rate = df["sla_miss"].mean()

summary = pd.DataFrame([{
    "case": f"{df['alg'].iloc[0]}_pd_heavy",
    "alg": df["alg"].iloc[0],
    "n_requests": len(df),
    "duration_s_est": duration_s,
    "throughput_rps_est": throughput_rps,
    "latency_p50_ms": lat_p50,
    "latency_p95_ms": lat_p95,
    "latency_p99_ms": lat_p99,
    "sla_miss_rate": sla_miss_rate,
    "avg_queue_wait_ms": df["queue_wait_ms"].mean(),
    "avg_exec_ms": df["exec_ms"].mean(),
}])

# Save CSVs
df.to_csv(OUTDIR / "raw_gateway_pd_heavy.csv", index=False)
summary.to_csv(OUTDIR / "summary_pd_heavy.csv", index=False)

print("=== KPI SUMMARY ===")
print(summary.to_string(index=False))

# Plot 1: Latency distribution (CDF-like via sorted values)
lat = df["e2e_latency_ms"].sort_values().reset_index(drop=True)
y = (lat.index + 1) / len(lat)
plt.figure()
plt.plot(lat, y)
plt.xlabel("E2E latency (ms)")
plt.ylabel("CDF")
plt.title("E2E Latency CDF (pd, heavy)")
plt.grid(True)
plt.savefig(OUTDIR / "latency_cdf_pd_heavy.png", dpi=200)

# Plot 2: SLA miss over time (rolling mean per 10s)
df2 = df.sort_values("ts").copy()
df2["sla_miss_int"] = df2["sla_miss"].astype(int)
# bucket by 10s
df2["tbin"] = (df2["ts"] // 10) * 10
miss_rate_by_bin = df2.groupby("tbin")["sla_miss_int"].mean().reset_index()
plt.figure()
plt.plot(miss_rate_by_bin["tbin"], miss_rate_by_bin["sla_miss_int"])
plt.xlabel("Time bin (epoch seconds, 10s bins)")
plt.ylabel("SLA miss rate")
plt.title("SLA Miss Rate over Time (pd, heavy)")
plt.grid(True)
plt.savefig(OUTDIR / "sla_miss_pd_heavy.png", dpi=200)

# Plot 3: Decision site share
site_share = df["decision_site"].value_counts(normalize=True).sort_index()
plt.figure()
plt.bar(site_share.index, site_share.values)
plt.xlabel("Decision site")
plt.ylabel("Share")
plt.title("Offloading Share by Site (pd, heavy)")
plt.grid(True, axis="y")
plt.savefig(OUTDIR / "site_share_pd_heavy.png", dpi=200)

# Plot 4: Queue wait vs CPU
plt.figure()
plt.scatter(df["cpu_ms"], df["queue_wait_ms"], s=8)
plt.xlabel("cpu_ms (task demand)")
plt.ylabel("queue_wait_ms")
plt.title("Queue Wait vs CPU Demand (pd, heavy)")
plt.grid(True)
plt.savefig(OUTDIR / "queue_vs_cpu_pd_heavy.png", dpi=200)

print(f"\nWrote results to: {OUTDIR}")
EOF

3.2 Ch·∫°y ph√¢n t√≠ch
apt update
apt install -y python3-pip
pip3 install pandas matplotlib

python3 -V
python3 ~/mec-lab/analyze_logs.py
ls -la ~/mec-lab/results

Output k·ª≥ v·ªçng

In ra b·∫£ng KPI SUMMARY (p50/p95/p99, SLA miss rate, throughput)

Th∆∞ m·ª•c ~/mec-lab/results/ c√≥:

raw_gateway_pd_heavy.csv

summary_pd_heavy.csv

4 file png bi·ªÉu ƒë·ªì

üëâ Anh paste output c·ªßa:

python3 ~/mec-lab/analyze_logs.py

ls -la ~/mec-lab/results

4) M·ªôt ƒëi·ªÉm r·∫•t quan tr·ªçng cho ‚Äútest scale 3 pods‚Äù

Hi·ªán t·∫°i kubectl logs deploy/gateway-scheduler ... ƒëang b√°o:

Found 3 pods, using pod/...

T·ª©c l√† l·ªánh log ƒëang ch·ªâ l·∫•y 1 pod, kh√¥ng ph·∫£i g·ªôp c·∫£ 3.

ƒê·ªÉ lu·∫≠n vƒÉn ‚Äúscale test‚Äù ƒë√∫ng nghƒ©a, anh c·∫ßn log c·ªßa c·∫£ 3 pods. C√≥ 2 c√°ch:

C√°ch A (nhanh, chu·∫©n nh·∫•t): gom log theo label
kubectl logs -n mec -l app=gateway-scheduler --since=15m --prefix=true > ~/mec-lab/logs/gateway_allpods_raw.log
grep '^{' ~/mec-lab/logs/gateway_allpods_raw.log > ~/mec-lab/logs/gateway_allpods_pd_heavy.jsonl
wc -l ~/mec-lab/logs/gateway_allpods_pd_heavy.jsonl

C√°ch B: xu·∫•t t·ª´ng pod r·ªìi concat

(T√¥i s·∫Ω ƒë∆∞a n·∫øu anh c·∫ßn, nh∆∞ng A l√† chu·∫©n.)

5) K·∫øt lu·∫≠n nhanh cho case hi·ªán t·∫°i

PASS: h·ªá th·ªëng ch·∫°y th·∫≠t, ALG=pd ho·∫°t ƒë·ªông, heavy load sinh ~1100 task, log JSON ƒë·ªß tr∆∞·ªùng.

C·∫ßn ch·ªânh: c√°ch export log ƒë·ªÉ tr√°nh permission + l·∫•y ƒë·ªß 3 pods.

Anh l√†m ngay 2 vi·ªác (ƒë·ªÉ t√¥i ch·ªët ‚Äúready for KPI plots‚Äù)

Ch·∫°y block export log JSONL s·∫°ch (M·ª•c 2) v√† g·ª≠i wc -l + tail -n 2.

Ch·∫°y python3 ~/mec-lab/analyze_logs.py v√† g·ª≠i KPI SUMMARY.

Sau ƒë√≥ t√¥i s·∫Ω h∆∞·ªõng d·∫´n ti·∫øp: ch·∫°y ma tr·∫≠n 9 case (3 thu·∫≠t to√°n √ó 3 m·ª©c t·∫£i) v√† t·∫°o b·∫£ng k·∫øt qu·∫£ t·ªïng h·ª£p ƒë√∫ng format ƒë∆∞a v√†o lu·∫≠n vƒÉn.


### Output 5 ###
root@MasterK8s:~/mec-lab/manifests# kubectl logs -n mec deploy/gateway-scheduler --since=10m > ~/mec-lab/gateway_raw.log
grep '^{'
  ~/mec-lab/gateway_raw.log > ~/mec-lab/gateway_logs.jsonl
wc -l ~/mec-lab/gateway_logs.jsonl
tail -n 2 ~/mec-lab/gateway_logs.jsonl
Found 3 pods, using pod/gateway-scheduler-5ff998d99-4qv4s
 <== B·ªä TREO
^C
-bash: /root/mec-lab/gateway_raw.log: Permission denied
0 /root/mec-lab/gateway_logs.jsonl
root@MasterK8s:~/mec-lab/manifests#

root@MasterK8s:~/mec-lab/manifests# kubectl logs -n mec deploy/trafficgen --tail=50
Collecting requests
  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)
Collecting charset_normalizer<4,>=2 (from requests)
  Downloading charset_normalizer-3.4.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)
Collecting idna<4,>=2.5 (from requests)
  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)
Collecting urllib3<3,>=1.21.1 (from requests)
  Downloading urllib3-2.6.3-py3-none-any.whl.metadata (6.9 kB)
Collecting certifi>=2017.4.17 (from requests)
  Downloading certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)
Downloading requests-2.32.5-py3-none-any.whl (64 kB)
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 64.7/64.7 kB 3.3 MB/s eta 0:00:00
Downloading certifi-2026.1.4-py3-none-any.whl (152 kB)
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 152.9/152.9 kB 2.8 MB/s eta 0:00:00
Downloading charset_normalizer-3.4.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (151 kB)
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 151.6/151.6 kB 14.7 MB/s eta 0:00:00
Downloading idna-3.11-py3-none-any.whl (71 kB)
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 71.0/71.0 kB 23.5 MB/s eta 0:00:00
Downloading urllib3-2.6.3-py3-none-any.whl (131 kB)
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 131.6/131.6 kB 20.6 MB/s eta 0:00:00
Installing collected packages: urllib3, idna, charset_normalizer, certifi, requests
Successfully installed certifi-2026.1.4 charset_normalizer-3.4.4 idna-3.11 requests-2.32.5 urllib3-2.6.3
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv

[notice] A new release of pip is available: 24.0 -> 25.3
[notice] To update, run: pip install --upgrade pip
{"sent": 50, "ok": 50, "load": "heavy"}
{"sent": 100, "ok": 100, "load": "heavy"}
{"sent": 150, "ok": 150, "load": "heavy"}
{"sent": 200, "ok": 200, "load": "heavy"}
{"sent": 250, "ok": 250, "load": "heavy"}
{"sent": 300, "ok": 300, "load": "heavy"}
{"sent": 350, "ok": 350, "load": "heavy"}
{"sent": 400, "ok": 400, "load": "heavy"}
{"sent": 450, "ok": 450, "load": "heavy"}
{"sent": 500, "ok": 500, "load": "heavy"}
{"sent": 550, "ok": 550, "load": "heavy"}
{"sent": 600, "ok": 600, "load": "heavy"}
{"sent": 650, "ok": 650, "load": "heavy"}
{"sent": 700, "ok": 700, "load": "heavy"}
{"sent": 750, "ok": 750, "load": "heavy"}
{"sent": 800, "ok": 800, "load": "heavy"}
{"sent": 850, "ok": 850, "load": "heavy"}
{"sent": 900, "ok": 900, "load": "heavy"}
{"sent": 950, "ok": 950, "load": "heavy"}
{"sent": 1000, "ok": 1000, "load": "heavy"}
{"sent": 1050, "ok": 1050, "load": "heavy"}
{"sent": 1100, "ok": 1100, "load": "heavy"}
{"done": true, "sent": 1102, "ok": 1102, "load": "heavy", "duration_s": 180}
root@MasterK8s:~/mec-lab/manifests#
root@MasterK8s:~/mec-lab/manifests# kubectl logs -n mec deploy/gateway-scheduler --since=2m > ~/mec-lab/gateway_raw.log
grep '^{' ~/mec-lab/gateway_raw.log | tail -n 5
Found 3 pods, using pod/gateway-scheduler-5ff998d99-4qv4s
{"ts": 1768892750, "pod": "gateway-scheduler-5ff998d99-4qv4s", "alg": "pd", "request_id": "e9adfd1b-dd14-4d28-b397-2a56a387d0fc", "cpu_ms": 20, "deadline_ms": 120, "priority": 3, "task_size": 2048, "decision_site": "mec2", "queue_wait_ms": 46, "exec_ms": 20, "e2e_latency_ms": 66, "sla_miss": false}
{"ts": 1768892750, "pod": "gateway-scheduler-5ff998d99-4qv4s", "alg": "pd", "request_id": "f3d36e0f-e388-432a-abcd-1f6440fa8673", "cpu_ms": 20, "deadline_ms": 80, "priority": 2, "task_size": 1024, "decision_site": "mec2", "queue_wait_ms": 38, "exec_ms": 20, "e2e_latency_ms": 58, "sla_miss": false}
{"ts": 1768892752, "pod": "gateway-scheduler-5ff998d99-4qv4s", "alg": "pd", "request_id": "74ca8367-caf3-4350-99b5-7b400cccc334", "cpu_ms": 200, "deadline_ms": 400, "priority": 1, "task_size": 2048, "decision_site": "mec2", "queue_wait_ms": 40, "exec_ms": 200, "e2e_latency_ms": 240, "sla_miss": false}
{"ts": 1768892752, "pod": "gateway-scheduler-5ff998d99-4qv4s", "alg": "pd", "request_id": "3107848e-783a-4260-bd4b-a135bafa3aa8", "cpu_ms": 20, "deadline_ms": 120, "priority": 3, "task_size": 512, "decision_site": "mec2", "queue_wait_ms": 44, "exec_ms": 20, "e2e_latency_ms": 64, "sla_miss": false}
{"ts": 1768892752, "pod": "gateway-scheduler-5ff998d99-4qv4s", "alg": "pd", "request_id": "8f6f99f5-f165-43c0-a5d5-ef49df4aec87", "cpu_ms": 20, "deadline_ms": 120, "priority": 3, "task_size": 2048, "decision_site": "mec2", "queue_wait_ms": 39, "exec_ms": 20, "e2e_latency_ms": 59, "sla_miss": false}
root@MasterK8s:~/mec-lab/manifests#

1) Check output: PASS/FAIL
‚úÖ A) Trafficgen heavy ‚Äî PASS
Trafficgen ch·∫°y xong 180s, g·ª≠i 1102 request, OK 1102 (100% success)
ƒê√¢y l√† s·ªë li·ªáu throughput th·ª±c t·∫ø: ~1102/180 ‚âà 6.12 req/s (l∆∞u √Ω: do m·ªói request ch·ªù response + gateway sleep, n√™n kh√¥ng ƒë·∫°t 100 req/s nh∆∞ ‚Äúsleep=0.01‚Äù; ƒë√¢y l√† b√¨nh th∆∞·ªùng v√† l√† m·ªôt ƒëi·ªÉm c·∫ßn n√™u trong lu·∫≠n vƒÉn).

‚úÖ B) Gateway log ‚Äúpd‚Äù ‚Äî PASS
grep '^{' ... | tail -n 5 cho ra 5 d√≤ng JSON ƒë·∫ßy ƒë·ªß tr∆∞·ªùng:
"alg":"pd" ‚úÖ
cpu_ms, deadline_ms, priority ‚úÖ
decision_site, queue_wait_ms, e2e_latency_ms, sla_miss ‚úÖ
=> D·ªØ li·ªáu ƒë√£ ƒë·ªß ƒë·ªÉ t√≠nh KPI.

‚ö†Ô∏è C) L·ªói thao t√°c ‚ÄúPermission denied‚Äù + grep b·ªã xu·ªëng d√≤ng ‚Äî FAIL (nh·ªè, do command)
D√≤ng n√†y:
grep '^{'
  ~/mec-lab/gateway_raw.log > ~/mec-lab/gateway_logs.jsonl
b·ªã t√°ch th√†nh 2 d√≤ng n√™n shell hi·ªÉu sai, r·ªìi b√°o:
Permission denied v·ªõi /root/mec-lab/gateway_raw.log (th∆∞·ªùng do file/dir permission ho·∫∑c do c√¢u l·ªánh b·ªã parse sai).
Fix: d√πng ƒë√∫ng 1 d√≤ng, v√† ƒë·∫£m b·∫£o file n·∫±m trong th∆∞ m·ª•c anh c√≥ quy·ªÅn ghi.


### L·ªãch s·ª≠ 5 ###
5.1 L√†m s·∫°ch log (l·ªçc JSON)

Sau khi ch·∫°y xong 1 case, xu·∫•t log:

kubectl logs -n mec deploy/gateway-scheduler --since=10m > ~/mec-lab/gateway_raw.log
grep '^{'
  ~/mec-lab/gateway_raw.log > ~/mec-lab/gateway_logs.jsonl
wc -l ~/mec-lab/gateway_logs.jsonl
tail -n 2 ~/mec-lab/gateway_logs.jsonl

üëâ N·∫øu tail ra 2 d√≤ng JSON ƒë√∫ng format l√† OK.

5.2 T√≠nh KPI (p50/p95/p99, SLA miss, throughput) b·∫±ng Python
T√¥i s·∫Ω g·ª≠i script parse + t√≠nh KPI + xu·∫•t CSV + v·∫Ω 4 bi·ªÉu ƒë·ªì chu·∫©n (m·ªói bi·ªÉu ƒë·ªì 1 figure), ƒë√∫ng format lu·∫≠n vƒÉn.

Anh l√†m gi√∫p t√¥i 2 l·ªánh ƒë·ªÉ ch·ªët ‚Äúheavy + pd‚Äù ƒëang ch·∫°y OK, r·ªìi t√¥i ƒë∆∞a script KPI/plot
Theo d√µi log trafficgen 15‚Äì20 gi√¢y:

kubectl logs -n mec deploy/trafficgen --tail=50

Export log gateway 2 ph√∫t g·∫ßn nh·∫•t v√† l·ªçc JSON:

kubectl logs -n mec deploy/gateway-scheduler --since=2m > ~/mec-lab/gateway_raw.log
grep '^{' ~/mec-lab/gateway_raw.log | tail -n 5
G·ª≠i output 2 l·ªánh n√†y, t√¥i s·∫Ω x√°c nh·∫≠n case ‚Äúpd + heavy‚Äù ƒë·ªß d·ªØ li·ªáu, r·ªìi t√¥i ƒë∆∞a ngay script KPI + bi·ªÉu ƒë·ªì.

### Output 4 ###
root@MasterK8s:~/mec-lab/manifests# kubectl apply -f 31-trafficgen-configmap.yaml
configmap/trafficgen-code created

root@MasterK8s:~/mec-lab/manifests# kubectl apply -f 32-trafficgen-deploy.yaml
deployment.apps/trafficgen created

root@MasterK8s:~/mec-lab/manifests# kubectl get pods -n mec -o wide | grep trafficgen
trafficgen-6f7bb7d65-5vkd8          1/1     Running   0             38s     192.168.131.207   ue          <none>           <none>
trafficgen-test                     1/1     Running   2 (56m ago)   176m    192.168.131.206   ue          <none>           <none>

root@MasterK8s:~/mec-lab/manifests# kubectl logs -n mec deploy/trafficgen --tail=20
  Downloading certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)
Downloading requests-2.32.5-py3-none-any.whl (64 kB)
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 64.7/64.7 kB 3.3 MB/s eta 0:00:00
Downloading certifi-2026.1.4-py3-none-any.whl (152 kB)
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 152.9/152.9 kB 2.9 MB/s eta 0:00:00
Downloading charset_normalizer-3.4.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (151 kB)
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 151.6/151.6 kB 14.8 MB/s eta 0:00:00
Downloading idna-3.11-py3-none-any.whl (71 kB)
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 71.0/71.0 kB 56.5 MB/s eta 0:00:00
Downloading urllib3-2.6.3-py3-none-any.whl (131 kB)
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 131.6/131.6 kB 21.8 MB/s eta 0:00:00
Installing collected packages: urllib3, idna, charset_normalizer, certifi, requests
Successfully installed certifi-2026.1.4 charset_normalizer-3.4.4 idna-3.11 requests-2.32.5 urllib3-2.6.3
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv

[notice] A new release of pip is available: 24.0 -> 25.3
[notice] To update, run: pip install --upgrade pip
{"sent": 50, "ok": 50, "load": "medium"}
{"sent": 100, "ok": 100, "load": "medium"}
{"sent": 150, "ok": 150, "load": "medium"}
root@MasterK8s:~/mec-lab/manifests#

root@MasterK8s:~/mec-lab/manifests# kubectl logs -n mec deploy/gateway-scheduler --since=10m > ~/mec-lab/gateway_logs.jsonl
wc -l ~/mec-lab/gateway_logs.jsonl
tail -n 3 ~/mec-lab/gateway_logs.jsonl
Found 3 pods, using pod/gateway-scheduler-6b6b9d8dc-qcxpc
430 /root/mec-lab/gateway_logs.jsonl
INFO:     192.168.131.207:44844 - "POST /api/task HTTP/1.1" 200 OK
{"ts": 1768891865, "pod": "gateway-scheduler-6b6b9d8dc-qcxpc", "alg": "rr", "request_id": "c429ab63-e5de-4d90-818a-0194bcf381f1", "cpu_ms": 20, "deadline_ms": 400, "priority": 1, "task_size": 1024, "decision_site": "mec2", "queue_wait_ms": 55, "exec_ms": 20, "e2e_latency_ms": 75, "sla_miss": false}
INFO:     192.168.131.207:44858 - "POST /api/task HTTP/1.1" 200 OK

root@MasterK8s:~/mec-lab/manifests# kubectl set env -n mec deploy/gateway-scheduler ALG=pd
kubectl rollout status -n mec deploy/gateway-scheduler
deployment.apps/gateway-scheduler env updated
Waiting for deployment "gateway-scheduler" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "gateway-scheduler" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "gateway-scheduler" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "gateway-scheduler" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "gateway-scheduler" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "gateway-scheduler" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "gateway-scheduler" rollout to finish: 1 old replicas are pending termination...
Waiting for deployment "gateway-scheduler" rollout to finish: 1 old replicas are pending termination...
deployment "gateway-scheduler" successfully rolled out
root@MasterK8s:~/mec-lab/manifests#

root@MasterK8s:~/mec-lab/manifests# kubectl set env -n mec deploy/trafficgen LOAD=heavy DURATION_S=180
kubectl rollout status -n mec deploy/trafficgen
kubectl logs -n mec deploy/trafficgen --tail=20
deployment.apps/trafficgen env updated
Waiting for deployment "trafficgen" rollout to finish: 1 old replicas are pending termination...
Waiting for deployment "trafficgen" rollout to finish: 1 old replicas are pending termination...
deployment "trafficgen" successfully rolled out
Found 2 pods, using pod/trafficgen-6f7bb7d65-5vkd8
Collecting requests
  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)
Collecting charset_normalizer<4,>=2 (from requests)
  Downloading charset_normalizer-3.4.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)
Collecting idna<4,>=2.5 (from requests)
  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)
Collecting urllib3<3,>=1.21.1 (from requests)
  Downloading urllib3-2.6.3-py3-none-any.whl.metadata (6.9 kB)
Collecting certifi>=2017.4.17 (from requests)
  Downloading certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)
Downloading requests-2.32.5-py3-none-any.whl (64 kB)
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 64.7/64.7 kB 3.1 MB/s eta 0:00:00
Downloading certifi-2026.1.4-py3-none-any.whl (152 kB)
root@MasterK8s:~/mec-lab/manifests#

3) C√°c output anh g·ª≠i: PASS/FAIL?
‚úÖ A) TrafficGen deployment: PASS
trafficgen ch·∫°y tr√™n node ue ‚úÖ
log trafficgen b√°o:
{"sent": 50, "ok": 50, "load": "medium"}
‚Üí nghƒ©a l√† 50 request g·ª≠i ƒëi, 50 request nh·∫≠n response 200 OK
‚Üí h·ªá th·ªëng ho·∫°t ƒë·ªông t·ªët.
Gi√° tr·ªã d√πng cho lu·∫≠n vƒÉn: ch·ª©ng minh traffic generator ch·∫°y v√† ƒë·∫°t ƒë∆∞·ª£c throughput theo c·∫•u h√¨nh.

‚úÖ B) Gateway logs export: PASS (nh∆∞ng c·∫ßn tinh ch·ªânh nh·ªè)
file log c√≥ 430 d√≤ng ‚úÖ (ƒë√£ c√≥ d·ªØ li·ªáu)
c√≥ d√≤ng JSON chu·∫©n:
{"alg":"rr", "decision_site":"mec2", "queue_wait_ms":55, "e2e_latency_ms":75, "sla_miss":false, ...}
‚Üí ƒê√¢y l√† d·ªØ li·ªáu th√¥ ƒë√∫ng format ƒë·ªÉ t√≠nh KPI.

‚ö†Ô∏è Nh∆∞ng c√≥ 1 v·∫•n ƒë·ªÅ nh·ªè
Trong file log c·ªßa anh c√≥ l·∫´n d√≤ng kh√¥ng ph·∫£i JSON:
INFO: ... "POST /api/task" 200 OK
D√≤ng n√†y ƒë·∫øn t·ª´ uvicorn access log. Khi parse JSONL, c√°c d√≤ng ‚ÄúINFO:‚Äù s·∫Ω l√†m parse l·ªói n·∫øu kh√¥ng l·ªçc.
Fix r·∫•t ƒë∆°n gi·∫£n (t√¥i s·∫Ω h∆∞·ªõng d·∫´n ·ªü b∆∞·ªõc parse):
l·ªçc ch·ªâ nh·ªØng d√≤ng b·∫Øt ƒë·∫ßu b·∫±ng {
ho·∫∑c
t·∫Øt access log trong uvicorn (ƒë·∫πp h∆°n cho lu·∫≠n vƒÉn)
T√≥m l·∫°i: PASS, ch·ªâ c·∫ßn l√†m s·∫°ch log.

‚úÖ C) ƒê·ªïi ALG sang pd v√† rollout: PASS
rollout th√†nh c√¥ng ‚úÖ
‚Üí nghƒ©a l√† thu·∫≠t to√°n ƒë√£ chuy·ªÉn t·ª´ RR sang Priority‚ÄìDeadline.

‚úÖ/‚ö†Ô∏è D) ƒê·ªïi trafficgen sang heavy: PASS nh∆∞ng anh ƒëang nh√¨n nh·∫ßm ƒëo·∫°n log
Anh th·∫•y log l·∫°i hi·ªán pip downloading. ƒê√¢y l√† v√¨:
trafficgen pod m·ªõi rollout, ch·∫°y l·∫°i, c√†i requests l·∫°i t·ª´ ƒë·∫ßu (container python slim)
l√∫c anh --tail=20 th√¨ n√≥ m·ªõi ·ªü ƒëo·∫°n pip, ch∆∞a ch·∫°y ƒë·∫øn ph·∫ßn {"sent": ...}.
Kh√¥ng ph·∫£i l·ªói, ch·ªâ l√† log ch∆∞a ƒë·∫øn ƒëo·∫°n traffic g·ª≠i request.

üëâ ƒê·ªÉ x√°c nh·∫≠n ‚Äúheavy load ƒëang ch·∫°y‚Äù, anh ch·∫°y:
kubectl logs -n mec deploy/trafficgen --follow
ho·∫∑c:
kubectl logs -n mec deploy/trafficgen --tail=50
sau 10‚Äì20 gi√¢y s·∫Ω th·∫•y {"sent": ... "load": "heavy"}.

4) Hi·ªán t·∫°i l√† ‚Äútest nhanh‚Äù hay ‚Äúƒë√£ b·∫Øt ƒë·∫ßu th·ª±c nghi·ªám‚Äù?
ƒê√£ b·∫Øt ƒë·∫ßu th·ª±c nghi·ªám ·ªü m·ª©c k·ªπ thu·∫≠t (data generation).
Tuy nhi√™n ƒë·ªÉ th√†nh ‚Äúk·∫øt qu·∫£ lu·∫≠n vƒÉn‚Äù, anh c·∫ßn th√™m 2 b∆∞·ªõc:
Ch·∫°y c√°c case theo ma tr·∫≠n (rr/pd/latency √ó light/medium/heavy, m·ªói case 3‚Äì5 ph√∫t)
Parse logs ‚Üí CSV ‚Üí v·∫Ω bi·ªÉu ƒë·ªì (p50/p95/p99, SLA miss rate, throughput)

### L·ªãch s·ª≠ 4 ###
2) B∆∞·ªõc ti·∫øp theo: ch·∫°y th·ª±c nghi·ªám ‚Äúc√≥ √Ω nghƒ©a‚Äù (TrafficGen th·∫≠t + thu logs + KPI + bi·ªÉu ƒë·ªì)
Gi·ªù anh ƒë√£ c√≥:
Gateway th·∫≠t (rr/pd/latency)
Scale 3 pods
UE pinning OK
Ta tri·ªÉn khai ti·∫øp theo 3 block:
TrafficGen th·∫≠t (t·∫°o cpu_i, deadline_i, priority_i, ƒëi·ªÅu khi·ªÉn load light/medium/heavy)
Thu logs (gateway logs JSONL) v√† ƒë√≥ng g√≥i th√†nh file
Parse -> CSV + bi·ªÉu ƒë·ªì (p50/p95/p99, SLA miss rate, throughput)
T√¥i s·∫Ω cho anh quy tr√¨nh copy/paste lu√¥n.

3) TRI·ªÇN KHAI TRAFFICGEN TH·∫¨T (pin node UE, c·∫•u h√¨nh load)
3.1 T·∫°o ConfigMap ch·ª©a script trafficgen
Tr√™n masterk8s:
T·∫°o file: 31-trafficgen-configmap.yaml

cat > 31-trafficgen-configmap.yaml <<'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: trafficgen-code
  namespace: mec
data:
  trafficgen.py: |
    import os, time, json, uuid, random
    import requests

    GATEWAY = os.getenv("GATEWAY_URL", "http://gateway-scheduler.mec.svc.cluster.local:8080/api/task")
    LOAD = os.getenv("LOAD", "medium")  # light | medium | heavy
    DURATION_S = int(os.getenv("DURATION_S", "120"))
    SEED = int(os.getenv("SEED", "1"))

    random.seed(SEED)

    # rate control (simple sleep)
    if LOAD == "light":
      sleep_s = 0.20   # ~5 req/s
    elif LOAD == "heavy":
      sleep_s = 0.01   # ~100 req/s
    else:
      sleep_s = 0.05   # ~20 req/s

    # distributions (adjust later if needed)
    cpu_choices = [20, 50, 100, 200]           # ms
    deadline_choices = [80, 120, 200, 400]     # ms
    priority_choices = [1, 2, 3]               # 1 low, 3 high

    t_end = time.time() + DURATION_S
    sent = 0
    ok = 0

    while time.time() < t_end:
      payload = {
        "request_id": str(uuid.uuid4()),
        "task_size": random.choice([512, 1024, 2048]),
        "cpu_ms": random.choice(cpu_choices),
        "deadline_ms": random.choice(deadline_choices),
        "priority": random.choice(priority_choices),
      }
      try:
        r = requests.post(GATEWAY, json=payload, timeout=5)
        sent += 1
        if r.status_code == 200:
          ok += 1
      except Exception:
        sent += 1

      # log locally for sanity
      if sent % 50 == 0:
        print(json.dumps({"sent": sent, "ok": ok, "load": LOAD}), flush=True)

      time.sleep(sleep_s)

    print(json.dumps({"done": True, "sent": sent, "ok": ok, "load": LOAD, "duration_s": DURATION_S}), flush=True)
EOF


Apply:

kubectl apply -f 31-trafficgen-configmap.yaml


Output k·ª≥ v·ªçng: configmap/trafficgen-code created

3.2 Deploy TrafficGen (pin node UE b·∫±ng label + toleration c·ªßa anh)
T·∫°o file: 32-trafficgen-deploy.yaml

cat > 32-trafficgen-deploy.yaml <<'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: trafficgen
  namespace: mec
spec:
  replicas: 1
  selector:
    matchLabels: { app: trafficgen }
  template:
    metadata:
      labels: { app: trafficgen }
    spec:
      nodeSelector:
        node-role.kubernetes.io/ue: "true"
      tolerations:
      - key: "dedicated"
        operator: "Equal"
        value: "ue"
        effect: "NoSchedule"
      containers:
      - name: trafficgen
        image: python:3.11-slim
        env:
        - name: LOAD
          value: "medium"     # ƒë·ªïi th√†nh light/medium/heavy
        - name: DURATION_S
          value: "180"        # ch·∫°y 3 ph√∫t / case
        - name: SEED
          value: "1"
        command: ["sh","-lc"]
        args:
          - |
            pip install --no-cache-dir requests &&
            python /app/trafficgen.py
        volumeMounts:
        - name: code
          mountPath: /app
        resources:
          requests: { cpu: "200m", memory: "128Mi" }
          limits:   { cpu: "500m", memory: "256Mi" }
      volumes:
      - name: code
        configMap:
          name: trafficgen-code
EOF


Apply + ki·ªÉm tra ch·∫°y ƒë√∫ng node ue:

kubectl apply -f 32-trafficgen-deploy.yaml
kubectl get pods -n mec -o wide | grep trafficgen
kubectl logs -n mec deploy/trafficgen --tail=20

Output ƒë√∫ng k·ª≥ v·ªçng

Pod trafficgen Running tr√™n node ue

Log th·∫•y {"sent":..., "ok":...} v√† k·∫øt th√∫c c√≥ {"done": true, ...}

üëâ Anh ch·∫°y v√† g·ª≠i output c·ªßa 2 l·ªánh:

kubectl get pods -n mec -o wide | grep trafficgen

kubectl logs -n mec deploy/trafficgen --tail=50

4) Thu logs gateway ƒë·ªÉ t√≠nh KPI (p50/p95/p99, SLA miss)

V√¨ gateway logs ƒëang print(json.dumps(log)) n√™n ta k√©o log t·ª´ deployment:

kubectl logs -n mec deploy/gateway-scheduler --since=10m > ~/mec-lab/gateway_logs.jsonl
wc -l ~/mec-lab/gateway_logs.jsonl
tail -n 3 ~/mec-lab/gateway_logs.jsonl

Output ƒë√∫ng k·ª≥ v·ªçng

wc -l c√≥ v√†i trƒÉm ƒë·∫øn v√†i ngh√¨n d√≤ng (t√πy load/duration)

tail l√† c√°c d√≤ng JSON ƒë·∫ßy ƒë·ªß tr∆∞·ªùng e2e_latency_ms, sla_miss, priority, deadline_ms, decision_site, pod

üëâ Anh g·ª≠i output:

wc -l ...

tail -n 3 ...

5) Ch·∫°y test scale ƒë√∫ng nghƒ©a: ƒë·ªïi ALG v√† LOAD theo k·ªãch b·∫£n
5.1 ƒê·ªïi thu·∫≠t to√°n (ALG)
2) ALG l√† g√¨? C√≥ li√™n quan thu·∫≠t to√°n RR/Priority/Latency kh√¥ng?
‚úÖ C√≥ li√™n quan tr·ª±c ti·∫øp.
ALG l√† bi·∫øn m√¥i tr∆∞·ªùng (environment variable) trong pod gateway-scheduler, d√πng ƒë·ªÉ ch·ªçn thu·∫≠t to√°n scheduling/offloading m√† gateway ƒëang ch·∫°y.
Trong code gateway c·ªßa ta:
ALG=rr ‚Üí Round Robin
ALG=pd ‚Üí Priority‚ÄìDeadline (priority & deadline aware)
ALG=latency ‚Üí Latency-aware (ch·ªçn site c√≥ queue delay ∆∞·ªõc l∆∞·ª£ng th·∫•p nh·∫•t)
Anh th·∫•y trong log JSON c·ªßa gateway c√≥ tr∆∞·ªùng "alg": "rr" l√† v√¨ bi·∫øn ALG ƒëang l√† "rr" l√∫c ƒë√≥.
V√¨ v·∫≠y, kubectl set env ... ALG=pd ch√≠nh l√† thao t√°c chuy·ªÉn thu·∫≠t to√°n ƒë·ªÉ ch·∫°y th√≠ nghi·ªám A/B/C.

V√≠ d·ª• chuy·ªÉn sang pd:

kubectl set env -n mec deploy/gateway-scheduler ALG=pd
kubectl rollout status -n mec deploy/gateway-scheduler


Sau ƒë√≥ ch·∫°y trafficgen v·ªõi LOAD kh√°c (nhanh nh·∫•t l√† edit env v√† rollout):

kubectl set env -n mec deploy/trafficgen LOAD=heavy DURATION_S=180
kubectl rollout status -n mec deploy/trafficgen
kubectl logs -n mec deploy/trafficgen --tail=20


(ta s·∫Ω l√†m b·∫£ng test: rr/pd/latency √ó light/medium/heavy)

Anh l√†m ti·∫øp theo th·ª© t·ª± n√†y (kh√¥ng b·ªè b∆∞·ªõc)

Apply trafficgen CM + deploy

Verify trafficgen ch·∫°y tr√™n node UE

Export gateway logs JSONL

T√¥i s·∫Ω ƒë∆∞a script parse JSONL -> CSV + bi·ªÉu ƒë·ªì

G·ª≠i t√¥i output c·ªßa c√°c l·ªánh ·ªü M·ª•c 3.2 v√† M·ª•c 4, t√¥i s·∫Ω ch·ªët PASS/FAIL v√† h∆∞·ªõng d·∫´n ti·∫øp ph·∫ßn parse + bi·ªÉu ƒë·ªì/dashboard.

### Output 3 ###
root@MasterK8s:~/mec-lab/manifests# kubectl delete -f 20-gateway-scheduler.yaml
deployment.apps "gateway-scheduler" deleted
service "gateway-scheduler" deleted

root@MasterK8s:~/mec-lab/manifests# kubectl apply -f 21-gateway-code-configmap.yaml
configmap/gateway-scheduler-code created

root@MasterK8s:~/mec-lab/manifests# kubectl apply -f 22-gateway-scheduler-deploy.yaml
deployment.apps/gateway-scheduler created
service/gateway-scheduler created
root@MasterK8s:~/mec-lab/manifests# kubectl rollout status deploy/gateway-scheduler -n mec
Waiting for deployment "gateway-scheduler" rollout to finish: 1 of 3 updated replicas are available...
Waiting for deployment "gateway-scheduler" rollout to finish: 2 of 3 updated replicas are available...
deployment "gateway-scheduler" successfully rolled out
root@MasterK8s:~/mec-lab/manifests# kubectl get pods -n mec -o wide | grep gateway-scheduler
gateway-scheduler-6b6b9d8dc-8f6mt   1/1     Running   0             27s    192.168.43.211    open5gs     <none>           <none>
gateway-scheduler-6b6b9d8dc-h94z4   1/1     Running   0             27s    192.168.30.172    workerk8s   <none>           <none>
gateway-scheduler-6b6b9d8dc-qcxpc   1/1     Running   0             27s    192.168.35.151    masterk8s   <none>           <none>
root@MasterK8s:~/mec-lab/manifests# kubectl get svc -n mec -o wide | grep gateway-scheduler
gateway-scheduler   ClusterIP   10.100.39.112   <none>        8080/TCP   34s   app=gateway-scheduler
root@MasterK8s:~/mec-lab/manifests#

root@MasterK8s:~/mec-lab/manifests# kubectl exec -n mec -it trafficgen-test -- sh -lc \
'curl -sS -m 3 http://gateway-scheduler.mec.svc.cluster.local:8080/healthz'
{"ok":true,"pod":"gateway-scheduler-6b6b9d8dc-h94z4","alg":"rr","uptime_s":123.4}root@MasterK8s:~/mec-lab/manifests#

root@MasterK8s:~/mec-lab/manifests# kubectl exec -n mec -it trafficgen-test -- sh -lc \
'curl -sS -m 10 -X POST http://gateway-scheduler.mec.svc.cluster.local:8080/api/task \
 -H "Content-Type: application/json" \
 -d "{\"request_id\":\"t1\",\"task_size\":1024,\"cpu_ms\":50,\"deadline_ms\":120,\"priority\":2}"'
{"ok":true,"decision_site":"mec1","e2e_latency_ms":51,"sla_miss":false}root@MasterK8s:~/mec-lab/manifests#

T√¥i ƒë√£ ki·ªÉm tra to√†n b·ªô output anh g·ª≠i. K·∫øt lu·∫≠n: PASS 100%, v√† th·∫≠m ch√≠ c·∫•u h√¨nh hi·ªán t·∫°i ƒëang ·ªü tr·∫°ng th√°i r·∫•t ‚Äúƒë·∫πp‚Äù ƒë·ªÉ test scale.
1) Check output: PASS/FAIL chi ti·∫øt
‚úÖ A) X√≥a gateway c≈© ‚Äî PASS
Deployment + Service c≈© ƒë√£ deleted ƒë√∫ng.

‚úÖ B) T·∫°o ConfigMap code ‚Äî PASS
configmap/gateway-scheduler-code created ƒë√∫ng.

‚úÖ C) Deploy gateway m·ªõi replicas=3 ‚Äî PASS
Rollout ho√†n t·∫•t, ƒë·ªß 3 replicas available.

‚úÖ D) Scale test ƒë√∫ng nghƒ©a ‚Äúdistributed‚Äù ‚Äî PASS
3 pods gateway ƒëang ph√¢n b·ªë tr√™n 3 node kh√°c nhau:
open5gs
workerk8s
masterk8s
ƒê√¢y l√† ƒëi·ªÅu ki·ªán r·∫•t t·ªët ƒë·ªÉ test:
load balancing qua Service
kh√°c bi·ªát latency/CPU node (n·∫øu c√≥)
‚Äúscale-out effect‚Äù

‚úÖ E) Service ClusterIP ‚Äî PASS
ClusterIP: 10.100.39.112:8080 OK.

‚úÖ F) UE -> Service DNS -> Gateway ‚Äî PASS
/healthz tr·∫£ JSON OK.
POST /api/task tr·∫£ decision + latency + sla_miss OK.
pod tr·∫£ v·ªÅ l√† gateway-scheduler-...-h94z4 ch·ª©ng minh service ƒëang route ƒë√∫ng (v√† c√≥ th·ªÉ ƒë·ªïi pod theo request).

### L·ªãch s·ª≠ 3 ###
2) M·ª•c ti√™u ti·∫øp theo (ƒë√∫ng y√™u c·∫ßu c·ªßa anh)

Thay gateway ‚Äúhttp-echo‚Äù b·∫±ng gateway-scheduler th·∫≠t:

Nh·∫≠n request JSON: cpu_ms, deadline_ms, priority, task_size

Ch·∫°y 3 thu·∫≠t to√°n: rr | pd | latency

Ghi log JSON (1 d√≤ng/task) ƒë·ªÉ parse KPI

Deploy 2‚Äì3 pods ƒë·ªÉ test scale (replicas = 3)

Cho trafficgen t·∫°o task distribution (cpu_i, deadline_i, priority_i) v√† g·ª≠i theo rate (light/medium/heavy)

Xu·∫•t log -> CSV -> bi·ªÉu ƒë·ªì (p50/p95/p99, SLA miss, throughput)

3) TRI·ªÇN KHAI GATEWAY/SCHEDULER TH·∫¨T (FastAPI) + SCALE 3 PODS
3.1 D·ªçn gateway c≈© (http-echo)

Tr√™n masterk8s:

cd ~/mec-lab/manifests
kubectl delete -f 20-gateway-scheduler.yaml


Output k·ª≥ v·ªçng:

deployment.apps "gateway-scheduler" deleted

service "gateway-scheduler" deleted

üëâ Ch·∫°y xong, paste output n·∫øu c√≥ l·ªói; n·∫øu kh√¥ng c√≥ l·ªói ta ƒëi ti·∫øp.

3.2 T·∫°o ConfigMap ch·ª©a code gateway (kh√¥ng c·∫ßn build image)

T·∫°o file: 21-gateway-code-configmap.yaml

cat > 21-gateway-code-configmap.yaml <<'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: gateway-scheduler-code
  namespace: mec
data:
  app.py: |
    import os, time, json, itertools, random
    from fastapi import FastAPI, Request
    from pydantic import BaseModel, Field
    from typing import Optional

    APP_START = time.time()
    POD_NAME = os.getenv("HOSTNAME", "unknown")
    ALG = os.getenv("ALG", "rr")  # rr | pd | latency
    SITES = ["mec1", "mec2", "cloud"]
    rr_cycle = itertools.cycle(SITES)

    # simple moving estimate of per-site queue delay (ms)
    est_q = {s: 0.0 for s in SITES}

    class Task(BaseModel):
      request_id: str
      task_size: int = 1024
      cpu_ms: int = Field(ge=1, le=2000)
      deadline_ms: int = Field(ge=1, le=10000)
      priority: int = Field(ge=1, le=3)

    app = FastAPI()

    def decide_site(t: Task):
      if ALG == "rr":
        return next(rr_cycle)
      if ALG == "pd":
        # prioritize higher priority and tighter deadlines
        # score: higher is better
        scores = {}
        for s in SITES:
          scores[s] = (t.priority * 1000) - (t.deadline_ms) - est_q[s]
        return max(scores, key=scores.get)
      # latency-aware (queue estimate only; network latency can be added later)
      return min(est_q, key=est_q.get)

    @app.get("/healthz")
    def healthz():
      return {"ok": True, "pod": POD_NAME, "alg": ALG, "uptime_s": round(time.time()-APP_START, 1)}

    @app.post("/api/task")
    async def api_task(t: Task, req: Request):
      t1 = time.time()
      site = decide_site(t)

      # simulate queue wait and execution time (in ms)
      # queue grows with load; simple model so we can compare algorithms
      queue_wait_ms = max(0, int(est_q[site] + random.uniform(0, 5)))
      exec_ms = int(t.cpu_ms)

      # update queue estimate: decay + add workload
      est_q[site] = max(0.0, est_q[site] * 0.90 + exec_ms * 0.05)

      # simulate processing
      time.sleep((queue_wait_ms + exec_ms) / 1000.0)

      e2e_ms = int((time.time() - t1) * 1000)
      sla_miss = e2e_ms > t.deadline_ms

      log = {
        "ts": int(time.time()),
        "pod": POD_NAME,
        "alg": ALG,
        "request_id": t.request_id,
        "cpu_ms": t.cpu_ms,
        "deadline_ms": t.deadline_ms,
        "priority": t.priority,
        "task_size": t.task_size,
        "decision_site": site,
        "queue_wait_ms": queue_wait_ms,
        "exec_ms": exec_ms,
        "e2e_latency_ms": e2e_ms,
        "sla_miss": sla_miss,
      }
      print(json.dumps(log), flush=True)
      return {"ok": True, "decision_site": site, "e2e_latency_ms": e2e_ms, "sla_miss": sla_miss}
EOF


Apply:

kubectl apply -f 21-gateway-code-configmap.yaml


Output k·ª≥ v·ªçng: configmap/gateway-scheduler-code created

üëâ N·∫øu OK ta l√†m ti·∫øp lu√¥n.

3.3 T·∫°o Deployment + Service cho gateway-scheduler (replicas=3)

T·∫°o file: 22-gateway-scheduler-deploy.yaml

cat > 22-gateway-scheduler-deploy.yaml <<'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gateway-scheduler
  namespace: mec
spec:
  replicas: 3
  selector:
    matchLabels: { app: gateway-scheduler }
  template:
    metadata:
      labels: { app: gateway-scheduler }
    spec:
      containers:
      - name: gateway
        image: python:3.11-slim
        env:
        - name: ALG
          value: "rr"   # rr | pd | latency (s·∫Ω ƒë·ªïi khi ch·∫°y th√≠ nghi·ªám)
        ports:
        - containerPort: 8080
        command: ["sh","-lc"]
        args:
          - |
            pip install --no-cache-dir fastapi uvicorn pydantic &&
            uvicorn app:app --host 0.0.0.0 --port 8080
        volumeMounts:
        - name: code
          mountPath: /app
        workingDir: /app
        resources:
          requests: { cpu: "300m", memory: "256Mi" }
          limits:   { cpu: "1",    memory: "512Mi" }
      volumes:
      - name: code
        configMap:
          name: gateway-scheduler-code
---
apiVersion: v1
kind: Service
metadata:
  name: gateway-scheduler
  namespace: mec
spec:
  selector:
    app: gateway-scheduler
  ports:
  - name: http
    port: 8080
    targetPort: 8080
  type: ClusterIP
EOF


Apply + theo d√µi rollout:

kubectl apply -f 22-gateway-scheduler-deploy.yaml
kubectl rollout status deploy/gateway-scheduler -n mec
kubectl get pods -n mec -o wide | grep gateway-scheduler
kubectl get svc -n mec -o wide | grep gateway-scheduler

Output ƒë√∫ng k·ª≥ v·ªçng

rollout status b√°o successfully rolled out

C√≥ 3 pods gateway-scheduler-... Running

Service v·∫´n c√≥ ClusterIP

üëâ Anh paste 3 output:

kubectl rollout status ...

kubectl get pods ... | grep gateway-scheduler

kubectl get svc ... | grep gateway-scheduler

3.4 Test t·ª´ UE (trafficgen-test) v√†o endpoint th·∫≠t

Ch·∫°y:

kubectl exec -n mec -it trafficgen-test -- sh -lc \
'curl -sS -m 3 http://gateway-scheduler.mec.svc.cluster.local:8080/healthz'


Output ƒë√∫ng k·ª≥ v·ªçng: JSON c√≥ ok, pod, alg.

Ti·∫øp theo test POST 1 task:

kubectl exec -n mec -it trafficgen-test -- sh -lc \
'curl -sS -m 10 -X POST http://gateway-scheduler.mec.svc.cluster.local:8080/api/task \
 -H "Content-Type: application/json" \
 -d "{\"request_id\":\"t1\",\"task_size\":1024,\"cpu_ms\":50,\"deadline_ms\":120,\"priority\":2}"'


Output ƒë√∫ng: JSON c√≥ decision_site, e2e_latency_ms, sla_miss.

üëâ Paste output 2 l·ªánh curl.

4) TrafficGen th·∫≠t (t·∫°o cpu_i/deadline_i/priority_i theo distribution) ‚Äì b∆∞·ªõc k·∫ø ti·∫øp

Ngay khi gateway 3 pods ch·∫°y OK, t√¥i s·∫Ω h∆∞·ªõng d·∫´n:

T·∫°o trafficgen d·∫°ng Deployment (replicas=1) pin l√™n node ue

C√≥ c·∫•u h√¨nh ENV ƒë·ªÉ ch·ªçn:

LOAD=light/medium/heavy (rate)

DISTRIBUTION=uniform/weighted

T·ª± sinh request_id + g·ª≠i li√™n t·ª•c

Thu log gateway v√† parse ra CSV + v·∫Ω bi·ªÉu ƒë·ªì

### output 2 ###
1) Check output: PASS/FAIL
‚úÖ B∆∞·ªõc 4 (Compute sites) ‚Äî PASS
kubectl get pods -n mec -o wide cho th·∫•y:
mec1 ch·∫°y tr√™n open5gs ‚úÖ
mec2 ch·∫°y tr√™n workerk8s ‚úÖ
cloud ch·∫°y tr√™n masterk8s ‚úÖ
trafficgen-test ch·∫°y tr√™n ue ‚úÖ

‚úÖ B∆∞·ªõc 5 (Gateway + Service) ‚Äî PASS
gateway pod Running ‚úÖ (ƒëang ch·∫°y tr√™n open5gs ‚Äì ch·∫•p nh·∫≠n)
service ClusterIP: 10.100.46.245:8080 ‚úÖ

‚úÖ B∆∞·ªõc 6 (DNS + HTTP t·ª´ UE) ‚Äî PASS
nslookup resolve ƒë√∫ng ClusterIP ‚úÖ
curl tr·∫£ OK ‚úÖ
=> H·ªá th·ªëng ‚Äúplumbing‚Äù ƒë√£ th√¥ng.

### L·ªãch s·ª≠ 2 ###
B∆Ø·ªöC 4: Deploy Compute Services (mec1/mec2/cloud) ‚Äì ch·∫°y ƒë√∫ng node
4.0 Th·ª±c hi·ªán ·ªü ƒë√¢u?

Tr√™n masterk8s (10.10.33.45), trong th∆∞ m·ª•c manifests:

cd ~/mec-lab/manifests

4.1 T·∫°o file manifest

T·∫°o file: 10-compute-sites.yaml

cat > 10-compute-sites.yaml <<'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mec1
  namespace: mec
spec:
  replicas: 1
  selector:
    matchLabels: { app: mec1 }
  template:
    metadata:
      labels: { app: mec1 }
    spec:
      nodeSelector:
        mec-site: mec1
      containers:
      - name: mec1
        image: nginx:stable
        resources:
          requests: { cpu: "500m", memory: "500Mi" }
          limits:   { cpu: "1",    memory: "1Gi" }
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mec2
  namespace: mec
spec:
  replicas: 1
  selector:
    matchLabels: { app: mec2 }
  template:
    metadata:
      labels: { app: mec2 }
    spec:
      nodeSelector:
        mec-site: mec2
      containers:
      - name: mec2
        image: nginx:stable
        resources:
          requests: { cpu: "500m", memory: "500Mi" }
          limits:   { cpu: "1",    memory: "1Gi" }
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cloud
  namespace: mec
spec:
  replicas: 1
  selector:
    matchLabels: { app: cloud }
  template:
    metadata:
      labels: { app: cloud }
    spec:
      nodeSelector:
        mec-site: cloud
      containers:
      - name: cloud
        image: nginx:stable
        resources:
          requests: { cpu: "1", memory: "1Gi" }
          limits:   { cpu: "2", memory: "2Gi" }
EOF

4.2 Apply + ki·ªÉm tra
kubectl apply -f 10-compute-sites.yaml
kubectl get pods -n mec -o wide

Output ƒë√∫ng k·ª≥ v·ªçng (quan tr·ªçng)

Anh ph·∫£i th·∫•y 3 pod Running v√† c·ªôt NODE ƒë√∫ng:

mec1-... ch·∫°y tr√™n open5gs

mec2-... ch·∫°y tr√™n workerk8s

cloud-... ch·∫°y tr√™n masterk8s

üëâ Anh paste output kubectl get pods -n mec -o wide sau b∆∞·ªõc n√†y.

B∆Ø·ªöC 5: Deploy Gateway/Scheduler + Service (ƒë·ªÉ trafficgen g·ªçi)

·ªû b∆∞·ªõc n√†y, m·ª•c ti√™u tr∆∞·ªõc m·∫Øt: c√≥ 1 endpoint ·ªïn ƒë·ªãnh trong cluster (DNS + service).
V√¨ anh ch∆∞a g·ª≠i image gateway th·∫≠t, ta l√†m theo 2 t·∫ßng:

T·∫ßng A (test plumbing): deploy gateway ‚Äúecho server‚Äù ƒë·ªÉ test POST/JSON v√† log.

T·∫ßng B (production): thay b·∫±ng gateway th·∫≠t sau.

5.1 Gateway t·∫°m th·ªùi: d√πng http-echo (c·ª±c nh·∫π, test nhanh)

T·∫°o file: 20-gateway-scheduler.yaml

cat > 20-gateway-scheduler.yaml <<'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gateway-scheduler
  namespace: mec
spec:
  replicas: 1
  selector:
    matchLabels: { app: gateway-scheduler }
  template:
    metadata:
      labels: { app: gateway-scheduler }
    spec:
      containers:
      - name: gateway
        image: hashicorp/http-echo:1.0.0
        args:
          - "-listen=:8080"
          - "-text=OK"
        env:
          - name: ALG
            value: "rr"
        ports:
          - containerPort: 8080
        resources:
          requests: { cpu: "200m", memory: "128Mi" }
          limits:   { cpu: "500m", memory: "256Mi" }
---
apiVersion: v1
kind: Service
metadata:
  name: gateway-scheduler
  namespace: mec
spec:
  selector:
    app: gateway-scheduler
  ports:
  - name: http
    port: 8080
    targetPort: 8080
  type: ClusterIP
EOF


Apply + check:

kubectl apply -f 20-gateway-scheduler.yaml
kubectl get pods -n mec -o wide | grep gateway-scheduler
kubectl get svc -n mec -o wide | grep gateway-scheduler

Output ƒë√∫ng k·ª≥ v·ªçng

Pod gateway-scheduler Running

Service gateway-scheduler c√≥ ClusterIP

üëâ Paste 2 outputs tr√™n.

B∆Ø·ªöC 6: Test connectivity t·ª´ node UE (trafficgen-test) ‚Üí gateway service

Anh ƒë√£ c√≥ pod trafficgen-test ch·∫°y tr√™n node ue. Ta d√πng lu√¥n pod ƒë√≥ ƒë·ªÉ test DNS + HTTP.

6.1 Exec v√†o trafficgen-test v√† curl
kubectl exec -n mec -it trafficgen-test -- sh -lc \
'nslookup gateway-scheduler.mec.svc.cluster.local && \
 curl -sS -m 3 http://gateway-scheduler.mec.svc.cluster.local:8080'

Output ƒë√∫ng k·ª≥ v·ªçng

nslookup tr·∫£ v·ªÅ IP (ClusterIP)

curl tr·∫£ v·ªÅ OK

üëâ Paste output l·ªánh n√†y.

B∆Ø·ªöC 7: Chu·∫©n b·ªã TrafficGen th·∫≠t ƒë·ªÉ t·∫°o cpu_i, deadline_i, priority_i (nh∆∞ng l√†m theo ki·ªÉu an to√†n)

Hi·ªán gateway t·∫°m ch·ªâ tr·∫£ ‚ÄúOK‚Äù n√™n ch∆∞a x·ª≠ l√Ω JSON.
ƒê·ªÉ ƒë√∫ng m·ª•c ti√™u lu·∫≠n vƒÉn, ta c·∫ßn gateway c√≥ endpoint nh·∫≠n JSON + ghi log.

V√¨ v·∫≠y b∆∞·ªõc k·∫ø ti·∫øp t√¥i s·∫Ω d·ª±ng gateway-scheduler b·∫£n t·ªëi thi·ªÉu (Python FastAPI), ch·∫°y trong K8s, c√≥:

/api/task nh·∫≠n JSON (cpu_ms/deadline_ms/priority)

‚Äúdecision‚Äù gi·∫£ l·∫≠p (rr/pd/latency)

log JSON 1 d√≤ng/task

Nh∆∞ng tr∆∞·ªõc khi t√¥i ƒë∆∞a manifest, t√¥i c·∫ßn anh x√°c nh·∫≠n 1 ƒëi·ªÉm ƒë·ªÉ kh·ªèi l√†m sai:

Anh mu·ªën gateway ch·∫°y d∆∞·ªõi d·∫°ng:

Deployment 1 pod (ƒë·ªß cho th√≠ nghi·ªám ban ƒë·∫ßu), hay

Deployment 2-3 pod (ƒë·ªÉ test scale)?

N·∫øu anh kh√¥ng tr·∫£ l·ªùi, t√¥i s·∫Ω m·∫∑c ƒë·ªãnh 1 pod ƒë·ªÉ ƒëi nhanh.

Anh g·ª≠i output ƒë·ªÉ t√¥i x√°c nh·∫≠n (theo th·ª© t·ª±)

kubectl get pods -n mec -o wide (sau B∆∞·ªõc 4)

kubectl get pods -n mec -o wide | grep gateway-scheduler

kubectl get svc -n mec -o wide | grep gateway-scheduler

Output kubectl exec ... nslookup ... curl ...

Sau khi anh g·ª≠i, t√¥i s·∫Ω:

Ch·ªët PASS/FAIL

N·∫øu PASS: g·ª≠i ngay B∆∞·ªõc 8 (gateway FastAPI + logging + metrics + script parse/v·∫Ω bi·ªÉu ƒë·ªì).

### Output 1 ###
1) K·∫øt qu·∫£ ki·ªÉm tra: PASS/FAIL
‚úÖ A. kubectl get nodes -o wide ‚Äî PASS
ƒê·ªß 4 node v√† IP ƒë√∫ng nh∆∞ ki·∫øn tr√∫c:
masterk8s: 10.10.33.45
open5gs: 10.10.33.54
ue: 10.10.33.40
workerk8s: 10.10.33.60

‚úÖ B. Labels ‚Äî PASS
Anh ƒë√£ g√°n ƒë√∫ng:
masterk8s c√≥ mec-site=cloud
open5gs c√≥ mec-site=mec1
workerk8s c√≥ mec-site=mec2
ue c√≥ c·∫£ 2 label:
node-role.kubernetes.io/ue=true (label n·ªÅn t·∫£ng anh d√πng)
role=ue (label theo template file)
=> ƒê√¢y l√† t√¨nh hu·ªëng l√Ω t∆∞·ªüng, kh√¥ng xung ƒë·ªôt.

‚úÖ C. Taint UE ‚Äî PASS
ue c√≥ taint:
dedicated=ue:NoSchedule
Chu·∫©n ƒë·ªÉ ‚Äúpin workload‚Äù trafficgen/UE.

‚úÖ D. Trafficgen pinned ƒë√∫ng node ‚Äî PASS
trafficgen-test ƒëang:
Running
Node = ue
IP Pod: 192.168.131.206
=> NodeSelector + toleration c·ªßa anh ƒë√∫ng tuy·ªát ƒë·ªëi.

### L·ªãch s·ª≠ 1 ###
B∆Ø·ªöC 0 (B·∫ÆT BU·ªòC): Ch·∫°y ·ªü ƒë√¢u? ki·ªÉm tra context tr∆∞·ªõc

‚úÖ Th·ª±c hi·ªán c√°c l·ªánh kubectl ... tr√™n m√°y c√≥ kubeconfig (khuy·∫øn ngh·ªã: masterk8s 10.10.33.45).

Tr√™n masterk8s ch·∫°y:

kubectl cluster-info
kubectl get nodes -o wide

Output k·ª≥ v·ªçng

Th·∫•y ƒë·ªß 4 node: masterk8s open5gs workerk8s ue

C·ªôt INTERNAL-IP ƒë√∫ng: 10.10.33.45 / 54 / 60 / 40

üëâ Anh paste output kubectl get nodes -o wide (ch·ªâ c·∫ßn 1 l·∫ßn, ƒë·ªÉ t√¥i b√°m IP c·ªßa anh).

B∆Ø·ªöC 1: C·∫•u h√¨nh Label v√† Toleration
1.1 G√°n label ‚Äúmec-site‚Äù cho 3 node compute + label UE (theo file)

Ch·∫°y ƒë√∫ng c√°c l·ªánh sau (th√™m --overwrite ƒë·ªÉ tr√°nh l·ªói ƒë√£ t·ªìn t·∫°i):

kubectl label nodes open5gs mec-site=mec1 --overwrite
kubectl label nodes workerk8s mec-site=mec2 --overwrite
kubectl label nodes masterk8s mec-site=cloud --overwrite
kubectl label nodes ue role=ue --overwrite

Output k·ª≥ v·ªçng

M·ªói l·ªánh s·∫Ω tr·∫£ v·ªÅ d·∫°ng:

node/open5gs labeled

node/workerk8s labeled

node/masterk8s labeled

node/ue labeled

L∆∞u √Ω quan tr·ªçng:
Anh ƒë√£ c√≥ label node-role.kubernetes.io/ue=true. Vi·ªác th√™m role=ue kh√¥ng xung ƒë·ªôt, ch·ªâ l√† th√™m 1 label n·ªØa cho ƒë√∫ng ‚Äútemplate‚Äù trong file.

1.2 Ki·ªÉm tra label ƒë√£ apply ƒë√∫ng ch∆∞a

Ch·∫°y:

kubectl get nodes --show-labels

Output ƒë√∫ng (k·ª≥ v·ªçng)

Anh ph·∫£i nh√¨n th·∫•y:

Node open5gs c√≥ mec-site=mec1

Node workerk8s c√≥ mec-site=mec2

Node masterk8s c√≥ mec-site=cloud

Node ue c√≥ role=ue v√† node-role.kubernetes.io/ue=true

üëâ Anh paste output kubectl get nodes --show-labels.

1.3 Toleration/taint cho node UE (ƒë·ªÉ pin UE/TrafficGen)

V√¨ h·ªá anh ƒëang d√πng taint dedicated=ue:NoSchedule, ta c·∫ßn x√°c minh taint n√†y c√≤n t·ªìn t·∫°i.

Ch·∫°y:

kubectl describe node ue | sed -n '/Taints:/,/Conditions:/p'

Output ƒë√∫ng (k·ª≥ v·ªçng)

Trong ph·∫ßn Taints: ph·∫£i th·∫•y d√≤ng t∆∞∆°ng t·ª±:

dedicated=ue:NoSchedule

üëâ Anh paste output ph·∫ßn n√†y.

N·∫øu kh√¥ng th·∫•y taint, t√¥i s·∫Ω ƒë∆∞a l·ªánh taint l·∫°i chu·∫©n theo h·ªá anh.

B∆Ø·ªöC 2: T·∫°o Namespace ‚Äúmec‚Äù

Anh c√≥ 2 c√°ch: YAML (chu·∫©n lu·∫≠n vƒÉn/lab) ho·∫∑c kubectl create (nhanh). T√¥i khuy·∫øn ngh·ªã YAML ƒë·ªÉ qu·∫£n l√Ω phi√™n b·∫£n.

2.1 T·∫°o file YAML tr√™n masterk8s

Tr√™n masterk8s:

mkdir -p ~/mec-lab/manifests
cd ~/mec-lab/manifests


T·∫°o file 00-namespace-mec.yaml v·ªõi n·ªôi dung:

apiVersion: v1
kind: Namespace
metadata:
  name: mec


Apply:

kubectl apply -f 00-namespace-mec.yaml

Output k·ª≥ v·ªçng

namespace/mec created (ho·∫∑c configured n·∫øu ƒë√£ c√≥)

2.2 Ki·ªÉm tra namespace

Ch·∫°y:

kubectl get namespaces | grep -E '^mec\b'

Output ƒë√∫ng

mec Active ...

üëâ Anh paste output 2 l·ªánh:

kubectl apply -f 00-namespace-mec.yaml

kubectl get namespaces | grep -E '^mec\b'

B∆Ø·ªöC 3: C·∫•u h√¨nh Toleration + NodeSelector cho TrafficGen/UE

V√¨ h·ªá c·ªßa anh c√≥:

label UE node: node-role.kubernetes.io/ue=true (anh ƒë√£ set)

taint UE node: dedicated=ue:NoSchedule

=> Pod TrafficGen ph·∫£i c√≥:

nodeSelector: node-role.kubernetes.io/ue: "true"

toleration ƒë√∫ng key/value/effect c·ªßa taint

3.1 Manifest Pod TrafficGen (test pinning + DNS)

T·∫°o file 30-trafficgen-test.yaml:

apiVersion: v1
kind: Pod
metadata:
  name: trafficgen-test
  namespace: mec
spec:
  nodeSelector:
    node-role.kubernetes.io/ue: "true"
  tolerations:
  - key: "dedicated"
    operator: "Equal"
    value: "ue"
    effect: "NoSchedule"
  containers:
  - name: curl
    image: curlimages/curl:8.5.0
    command: ["sh","-lc"]
    args:
      - |
        echo "trafficgen-test running on node:" &&
        hostname &&
        sleep 3600


Apply v√† ki·ªÉm tra pod ch·∫°y ƒë√∫ng node ue:

kubectl apply -f 30-trafficgen-test.yaml
kubectl get pod -n mec -o wide | grep trafficgen-test

Output ƒë√∫ng (k·ª≥ v·ªçng)

Pod trafficgen-test tr·∫°ng th√°i Running

C·ªôt NODE = ue (IP node 10.10.33.40)

üëâ Anh paste output kubectl get pod -n mec -o wide | grep trafficgen-test.


### G·ª£i √Ω l√Ω thuy·∫øt ###
Sau khi ƒë√£ c·∫•u h√¨nh K8s + MEC + Scheduler nh∆∞ trong file, l√†m sao t·∫°o workload (cpu_i, deadline_i, priority_i), cho h·ªá th·ªëng ch·∫°y th·∫≠t, thu k·∫øt qu·∫£, v·∫Ω bi·ªÉu ƒë·ªì / dashboard?
T√¥i tr·∫£ l·ªùi theo ƒë√∫ng hi·ªán tr·∫°ng file anh g·ª≠i, kh√¥ng gi·∫£ ƒë·ªãnh th√™m 

T·ªîNG QUAN LU·ªíNG TH·ª∞C NGHI·ªÜM (END-TO-END)
TrafficGen (UE)
   ‚Üì  (JSON task)
Gateway / Scheduler
   ‚Üì  (decision)
MEC1 / MEC2 / Cloud
   ‚Üì  (process)
Logs + Metrics
   ‚Üì
CSV / Prometheus
   ‚Üì
Python / Grafana

1. T·∫†O cpu_i, deadline_i, priority_i T·ª™ ƒê√ÇU?

üëâ KH√îNG t·∫°o trong Kubernetes
üëâ T·∫†O T·ª™ TRAFFICGEN (UE side)

1.1 Task model (chu·∫©n cho lu·∫≠n vƒÉn)

M·ªói request = 1 task i

{
  "request_id": "uuid",
  "cpu_ms": 50,
  "deadline_ms": 120,
  "priority": 2,
  "task_size": 1024
}

Bi·∫øn	√ù nghƒ©a	Ghi ch√∫ h·ªçc thu·∫≠t
cpu_ms	cpu_i	workload
deadline_ms	d_i	SLA
priority	p_i	QoS class
task_size	input size	optional
2. VI·∫æT TRAFFICGEN (C√ÅCH ƒê∆†N GI·∫¢N ‚Äì HI·ªÜU QU·∫¢)
2.1 Logic TrafficGen (pseudo-code)
import random, time, requests, uuid

while True:
    task = {
        "request_id": str(uuid.uuid4()),
        "cpu_ms": random.choice([20, 50, 100, 200]),
        "deadline_ms": random.choice([80, 120, 200]),
        "priority": random.choice([1, 2, 3]),
        "task_size": random.choice([512, 1024])
    }

    requests.post("http://gateway-scheduler/api/task", json=task)
    time.sleep(0.05)  # load control

2.2 Mapping load level (R·∫§T QUAN TR·ªåNG)
Load	sleep	req/s
Light	0.2s	~5
Medium	0.05s	~20
Heavy	0.01s	~100

üìå Ch·ªâ ƒë·ªïi sleep l√† ƒë·ªïi k·ªãch b·∫£n th√≠ nghi·ªám ‚Üí h·ªôi ƒë·ªìng r·∫•t th√≠ch

3. SCHEDULER X·ª¨ L√ù TASK NH∆Ø TH·∫æ N√ÄO?
3.1 Round Robin (RR)
site = sites[next_index]

3.2 Priority‚ÄìDeadline
score = alpha * priority - beta * deadline
site = argmax(score)

3.3 Latency-aware
site = argmin(
    estimated_queue_delay + network_latency
)


üëâ Anh KH√îNG c·∫ßn t·ªëi ∆∞u to√°n h·ªçc cao si√™u
üëâ Quan tr·ªçng l√† so s√°nh k·∫øt qu·∫£

4. MEC SERVICE ‚ÄúGI·∫¢ L·∫¨P CPU‚Äù RA SAO?
4.1 C√°ch m√¥ ph·ªèng CPU (ƒë·ªß cho th·∫°c sƒ©)
import time

def process(cpu_ms):
    start = time.time()
    while (time.time() - start) * 1000 < cpu_ms:
        pass


üìå Kh√¥ng c·∫ßn ch·∫°y task th·∫≠t
üìå C√°i anh c·∫ßn l√† ƒë·ªô tr·ªÖ & tranh ch·∫•p t√†i nguy√™n

5. LOG PH·∫¢I GHI NH∆Ø TH·∫æ N√ÄO? (R·∫§T QUAN TR·ªåNG)
5.1 Log chu·∫©n (JSON ‚Äì 1 d√≤ng / task)
{
  "request_id": "...",
  "alg": "rr",
  "priority": 2,
  "deadline_ms": 120,
  "site": "mec1",
  "queue_wait_ms": 18,
  "exec_ms": 50,
  "e2e_latency_ms": 92,
  "sla_miss": false,
  "timestamp": 1700000000
}


üìå KH√îNG log text lung tung
üìå H·ªôi ƒë·ªìng c·ª±c k·ª≥ th√≠ch log JSON

6. THU KPI NH∆Ø TH·∫æ N√ÄO?
6.1 C√°ch 1 ‚Äì ƒê·ª¶ & NHANH (Python + CSV) ‚úÖ
kubectl logs gateway-scheduler > logs.json


Parse b·∫±ng Python:

import pandas as pd
df = pd.read_json("logs.json", lines=True)

df["e2e_latency_ms"].quantile([0.5, 0.95, 0.99])
df["sla_miss"].mean()


üëâ ƒê·ª¶ ƒë·ªÉ t·ªët nghi·ªáp

6.2 C√°ch 2 ‚Äì PRO (Prometheus + Grafana) ‚≠ê

N·∫øu mu·ªën dashboard ƒë·∫πp:

Gateway expose /metrics

D√πng:

latency_bucket

sla_miss_total

Grafana v·∫Ω:

p50 / p95 / p99

SLA miss %

CPU per MEC site

üìå Kh√¥ng b·∫Øt bu·ªôc, nh∆∞ng r·∫•t ‚ÄúƒÉn ƒëi·ªÉm‚Äù

7. BI·ªÇU ƒê·ªí N√ÄO N√äN V·∫º? (KH√îNG V·∫º TH·ª™A)
Bi·ªÉu ƒë·ªì	M·ª•c ƒë√≠ch
Latency p95 vs Load	So s√°nh thu·∫≠t to√°n
SLA miss vs Load	Ch·ª©ng minh priority
Throughput vs Load	ƒê√°nh ƒë·ªïi
Queue delay per site	Hi·ªáu qu·∫£ offloading
8. L·ªò TR√åNH TH·ª∞C NGHI·ªÜM CHU·∫®N (G·ª¢I √ù)

Fix RR ‚Üí ch·∫°y ·ªïn

Add Priority‚ÄìDeadline

Add Latency-aware

M·ªói thu·∫≠t to√°n:

Light / Medium / Heavy

C√≥ / kh√¥ng mobility

M·ªói case ch·∫°y ‚â• 5 ph√∫t

üëâ ƒê√¢y l√† Ch∆∞∆°ng 5 lu·∫≠n vƒÉn ho√†n ch·ªânh

9. T√ìM L·∫†I ‚Äì ANH ƒêANG ·ªû ƒê√öNG CH·ªñ

File hi·ªán t·∫°i = x∆∞∆°ng s·ªëng th·ª±c nghi·ªám

Vi·ªác anh c·∫ßn l√†m NGAY B√ÇY GI·ªú:

Vi·∫øt trafficgen

Chu·∫©n h√≥a log

Ch·∫°y load

Xu·∫•t CSV

V·∫Ω bi·ªÉu ƒë·ªì

üëâ N·∫øu anh mu·ªën, b∆∞·ªõc ti·∫øp theo t√¥i c√≥ th·ªÉ:

Vi·∫øt trafficgen ho√†n ch·ªânh

Vi·∫øt scheduler skeleton (RR / Priority / Latency)

Vi·∫øt script parse log + v·∫Ω bi·ªÉu ƒë·ªì (Python)


##############
# 13/01/2026 #
##############
B∆∞·ªõc 1: C·∫•u h√¨nh Label v√† Toleration
1.1 Label c√°c Nodes

Tr∆∞·ªõc ti√™n, b·∫°n s·∫Ω c·∫ßn g√°n label cho c√°c nodes ƒë·ªÉ ƒë·∫£m b·∫£o r·∫±ng c√°c d·ªãch v·ª• s·∫Ω ƒë∆∞·ª£c ph√¢n ph·ªëi ƒë√∫ng node.

kubectl label nodes open5gs mec-site=mec1
kubectl label nodes workerk8s mec-site=mec2
kubectl label nodes masterk8s mec-site=cloud
kubectl label nodes ue role=ue


Output Ki·ªÉm Tra:
ƒê·ªÉ ki·ªÉm tra xem c√°c label ƒë√£ ƒë∆∞·ª£c √°p d·ª•ng ch√≠nh x√°c hay ch∆∞a, b·∫°n c√≥ th·ªÉ d√πng l·ªánh sau:

kubectl get nodes --show-labels


M·ªói node s·∫Ω c√≥ label t∆∞∆°ng ·ª©ng nh∆∞ mec-site=mec1, mec-site=mec2, v.v.

B∆∞·ªõc 2: T·∫°o Namespace ‚Äúmec‚Äù

T·∫°o m·ªôt namespace ƒë·ªÉ ch·ª©a t·∫•t c·∫£ c√°c d·ªãch v·ª• li√™n quan ƒë·∫øn MEC.

2.1 T·∫°o Namespace
apiVersion: v1
kind: Namespace
metadata:
  name: mec


Output Ki·ªÉm Tra:

kubectl get namespaces


Namespace mec s·∫Ω xu·∫•t hi·ªán trong danh s√°ch.

B∆∞·ªõc 3: C·∫•u h√¨nh Toleration v√† NodeSelector cho TrafficGen/UE

T·∫°o c·∫•u h√¨nh ƒë·ªÉ ƒë·∫£m b·∫£o TrafficGen/UE ch·ªâ ƒë∆∞·ª£c ch·∫°y tr√™n node ue.

3.1 Pod TrafficGen v·ªõi Toleration v√† NodeSelector
apiVersion: v1
kind: Pod
metadata:
  name: trafficgen
  namespace: mec
spec:
  tolerations:
  - key: "role"
    operator: "Equal"
    value: "ue"
    effect: "NoSchedule"
  nodeSelector:
    role: "ue"
  containers:
    - name: trafficgen
      image: your-trafficgen-image
      resources:
        requests:
          memory: "500Mi"
          cpu: "500m"
        limits:
          memory: "1Gi"
          cpu: "1"


Output Ki·ªÉm Tra:
Ki·ªÉm tra xem pod ƒë√£ ƒë∆∞·ª£c schedule ƒë√∫ng v√†o node ue:

kubectl get pods -o wide


Pod trafficgen s·∫Ω ƒë∆∞·ª£c g√°n v√†o node ue.

B∆∞·ªõc 4: T·∫°o v√† C·∫•u h√¨nh Compute Services (mec1, mec2, cloud)
4.1 Manifest cho MEC1, MEC2, Cloud

T·∫°o c√°c Deployment v√† Service cho c√°c compute sites MEC1, MEC2 v√† Cloud.

MEC1 Deployment:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mec1
  namespace: mec
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mec1
  template:
    metadata:
      labels:
        app: mec1
    spec:
      nodeSelector:
        mec-site: mec1
      containers:
        - name: mec1
          image: your-mec1-image
          resources:
            requests:
              memory: "500Mi"
              cpu: "500m"
            limits:
              memory: "1Gi"
              cpu: "1"

MEC2 Deployment:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mec2
  namespace: mec
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mec2
  template:
    metadata:
      labels:
        app: mec2
    spec:
      nodeSelector:
        mec-site: mec2
      containers:
        - name: mec2
          image: your-mec2-image
          resources:
            requests:
              memory: "500Mi"
              cpu: "500m"
            limits:
              memory: "1Gi"
              cpu: "1"

Cloud Deployment:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cloud
  namespace: mec
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cloud
  template:
    metadata:
      labels:
        app: cloud
    spec:
      nodeSelector:
        mec-site: cloud
      containers:
        - name: cloud
          image: your-cloud-image
          resources:
            requests:
              memory: "1Gi"
              cpu: "1"
            limits:
              memory: "2Gi"
              cpu: "2"


Output Ki·ªÉm Tra:
Ki·ªÉm tra pod ƒë√£ ƒë∆∞·ª£c deploy v√† schedule v√†o ƒë√∫ng node:

kubectl get pods -o wide


C√°c pod c·ªßa mec1, mec2, v√† cloud s·∫Ω xu·∫•t hi·ªán tr√™n c√°c node open5gs, workerk8s v√† masterk8s t∆∞∆°ng ·ª©ng.

B∆∞·ªõc 5: C·∫•u h√¨nh Gateway/Scheduler

C·∫•u h√¨nh d·ªãch v·ª• Gateway/Scheduler ƒë·ªÉ quy·∫øt ƒë·ªãnh offloading v·ªõi 3 thu·∫≠t to√°n:

5.1 Gateway/Scheduler Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gateway-scheduler
  namespace: mec
spec:
  replicas: 1
  selector:
    matchLabels:
      app: gateway-scheduler
  template:
    metadata:
      labels:
        app: gateway-scheduler
    spec:
      containers:
        - name: gateway-scheduler
          image: your-gateway-image
          env:
            - name: ALG
              value: "rr"  # c√≥ th·ªÉ l√† rr | pq | latency
          resources:
            requests:
              memory: "500Mi"
              cpu: "500m"
            limits:
              memory: "1Gi"
              cpu: "1"


Output Ki·ªÉm Tra:
Ki·ªÉm tra pod c·ªßa Gateway/Scheduler ƒë√£ ch·∫°y ch∆∞a:

kubectl get pods -o wide


Pod gateway-scheduler s·∫Ω xu·∫•t hi·ªán.

B∆∞·ªõc 6: Ki·ªÉm tra API v√† Format Log

API c·ªßa Gateway/Scheduler c·∫ßn ph·∫£i nh·∫≠n y√™u c·∫ßu v·ªõi c√°c tham s·ªë nh∆∞ request_id, task_size, cpu_ms, deadline_ms, priority v√† t√≠nh to√°n vi·ªác offloading t·ªõi c√°c site MEC ho·∫∑c Cloud.

6.1 API Format (JSON)
{
  "request_id": "12345",
  "task_size": 1024,
  "cpu_ms": 100,
  "deadline_ms": 200,
  "priority": 1
}

6.2 Log Format (JSON)
{
  "request_id": "12345",
  "t1_recv": 1611243156,
  "decision_site": "mec1",
  "queue_wait_ms": 10,
  "t2_send": 1611243170
}


Output Ki·ªÉm Tra:
Khi API ƒë∆∞·ª£c g·ªçi, log s·∫Ω ƒë∆∞·ª£c ghi l·∫°i v√† c√≥ th·ªÉ ki·ªÉm tra th√¥ng qua kubectl logs.

kubectl logs <gateway-scheduler-pod>


Ki·ªÉm tra c√°c logs ƒë·ªÉ x√°c minh r·∫±ng qu√° tr√¨nh x·ª≠ l√Ω y√™u c·∫ßu v√† quy·∫øt ƒë·ªãnh offloading ƒëang di·ªÖn ra ƒë√∫ng c√°ch.

B∆∞·ªõc 7: KPI v√† Thu th·∫≠p D·ªØ li·ªáu

KPI c·∫ßn ƒë∆∞·ª£c thu th·∫≠p trong su·ªët c√°c th·ª≠ nghi·ªám. ƒê√¢y l√† m·ªôt s·ªë c√°ch ƒë·ªÉ ƒëo l∆∞·ªùng:

7.1 Thu th·∫≠p KPI b·∫±ng kubectl

Latency: B·∫°n c√≥ th·ªÉ ƒëo t0_send v√† t3_recv trong log ƒë·ªÉ t√≠nh to√°n latency E2E.

Throughput: ƒê·∫øm s·ªë y√™u c·∫ßu (req/s) g·ª≠i ƒëi t·ª´ UE ƒë·∫øn Gateway/Scheduler.

CPU/Memory: S·ª≠ d·ª•ng kubectl top ƒë·ªÉ l·∫•y d·ªØ li·ªáu t√†i nguy√™n.

kubectl top pod -n mec

7.2 Thu th·∫≠p SLA Miss Rate

ƒêo l∆∞·ªùng c√°c y√™u c·∫ßu kh√¥ng ho√†n th√†nh trong th·ªùi gian h·∫°n ch√≥t (deadline miss).

B∆∞·ªõc 8: K·ªãch b·∫£n Benchmark
8.1 Ch·∫°y Th·ª≠ Nghi·ªám

B·∫°n c√≥ th·ªÉ b·∫Øt ƒë·∫ßu ch·∫°y th·ª≠ nghi·ªám v·ªõi c√°c tham s·ªë:

Load: Light, Medium, Heavy.

UE Count: 1, 5, 20.

Mobility: Static, ho·∫∑c thay ƒë·ªïi TAC.

8.2 L·ªánh Ch·∫°y Benchmark

Gi·∫£ s·ª≠ b·∫°n ƒëang ch·∫°y th·ª≠ nghi·ªám v·ªõi trafficgen tr√™n node ue:

kubectl run trafficgen --image=your-trafficgen-image --namespace=mec


T√πy ch·ªânh s·ªë l∆∞·ª£ng v√† ƒë·ªô ph·ª©c t·∫°p c·ªßa traffic.

T·ªïng k·∫øt:

C√°c b∆∞·ªõc tr√™n cung c·∫•p c·∫•u h√¨nh chi ti·∫øt cho Label, NodeSelector, Tolerations, Compute Services, Gateway/Scheduler, v√† KPI.

Sau khi tri·ªÉn khai v√† ki·ªÉm tra, b·∫°n s·∫Ω c√≥ ƒë∆∞·ª£c c√°c logs, KPI nh∆∞ latency, throughput, SLA miss, v√† CPU/mem usage ƒë·ªÉ ƒë√°nh gi√° ho·∫°t ƒë·ªông c·ªßa h·ªá th·ªëng.

##############
# 10/01/2026 #
##############
B·∫°n l√† tr·ª£ l√Ω k·ªπ thu·∫≠t cho b√†i lab/lu·∫≠n vƒÉn 5G MEC offloading + scheduling tr√™n Kubernetes. H√£y lu√¥n b√°m ƒë√∫ng hi·ªán tr·∫°ng b√™n d∆∞·ªõi (KH√îNG gi·∫£ ƒë·ªãnh kh√°c), v√† ƒë∆∞a ra k·∫ø ho·∫°ch/manifest/thi·∫øt k·∫ø chi ti·∫øt ƒë·ªÉ tri·ªÉn khai b∆∞·ªõc ti·∫øp theo.

=== HI·ªÜN TR·∫†NG / C·∫§U H√åNH ƒê√É C√ì ===
- M√¥i tr∆∞·ªùng: Kubernetes cluster g·ªìm 4 node:
  1) masterk8s: control-plane + s·∫Ω d√πng l√†m ‚Äúcore/central/cloud compute‚Äù (n·∫øu c·∫ßn)
  2) open5gs: worker ch·∫°y Open5GS core + ƒë·ªìng th·ªùi l√† site MEC1
  3) workerk8s: worker l√† site MEC2
  4) ue: node d√†nh cho UERANSIM (UE/gNB), c√≥ taint ƒë·ªÉ ch·ªâ workload UE/trafficgen ƒë∆∞·ª£c schedule l√™n
- ƒê√£ deploy th√†nh c√¥ng Open5GS + UERANSIM tr√™n K8s:
  - Core network (AMF/SMF/UPF‚Ä¶) ch·∫°y OK
  - 2 gNB (mec1 v√† mec2) + 1 UE
- User-plane OK:
  - UE Registration OK
  - PDU session OK
  - Interface uesimtun0 c√≥ IP v√† ping Internet (8.8.8.8) OK
- K·∫øt n·ªëi N3 ƒë√£ fix b·∫±ng OVS bridge + VXLAN (ƒë·∫£m b·∫£o ƒë∆∞·ªùng N3/UE user-plane qua c√°c site ho·∫°t ƒë·ªông)
- Mobility test OK:
  - UE ƒë·ªïi TAC t·ª´ mec1 -> mec2 (handover/reattach theo k·ªãch b·∫£n), AMF/gNB2 c√≥ d·∫•u v·∫øt attach + PDU setup
- M·ª•c ti√™u b√†i to√°n: d·ª±ng 2 MEC service (mec1/mec2) + 1 scheduler/gateway microservice ƒë·ªÉ ch·∫°y 3 thu·∫≠t to√°n:
  1) Round-robin
  2) Priority-Deadline
  3) Latency-aware
  v√† ƒëo KPI: latency p50/p95/p99, throughput, queue delay/length, CPU/mem, SLA miss (deadline miss), theo c√°c m·ª©c t·∫£i light/medium/heavy, c√≥/kh√¥ng mobility.

=== KI·∫æN TR√öC MONG MU·ªêN (T·ªêI GI·∫¢N, THUY·∫æT PH·ª§C) ===
- UE/Traffic Generator -> g·ªçi 1 endpoint duy nh·∫•t: Gateway/Scheduler (L7)
- Gateway quy·∫øt ƒë·ªãnh route task t·ªõi:
  - MEC1 app (ch·∫°y tr√™n node open5gs)
  - MEC2 app (ch·∫°y tr√™n node workerk8s)
  - Cloud/Central app (ch·∫°y tr√™n node masterk8s) = m√¥ ph·ªèng offload ‚Äúl√™n core‚Äù
- L∆∞u √Ω: quy·∫øt ƒë·ªãnh offloading n·∫±m ·ªü Gateway/Scheduler (application-layer), kh√¥ng n·∫±m ·ªü Open5GS core. Core ch·ªâ ƒë·∫£m nhi·ªám k·∫øt n·ªëi 5G SA + user-plane.

=== KHUY·∫æN NGH·ªä / B∆Ø·ªöC TI·∫æP THEO PH·∫¢I L√ÄM (∆ØU TI√äN) ===
1) Ch·ªët placement b·∫±ng label/affinity:
   - Label node:
     - open5gs: mec-site=mec1
     - workerk8s: mec-site=mec2
     - masterk8s: mec-site=cloud (ho·∫∑c cloud=true)
     - ue: role=ue
   - T·∫°o namespace ‚Äúmec‚Äù
   - Thi·∫øt k·∫ø toleration/nodeSelector ƒë·ªÉ trafficgen/UE lu√¥n n·∫±m ·ªü node ue
2) D·ª±ng 3 compute service ƒë∆°n gi·∫£n (mec1/mec2/cloud):
   - M·ªói service c√≥ API nh·∫≠n request g·ªìm: request_id, task_size, cpu_ms(or mi), deadline_ms, priority
   - mec1 Deployment nodeSelector mec-site=mec1
   - mec2 Deployment nodeSelector mec-site=mec2
   - cloud Deployment nodeSelector mec-site=cloud (masterk8s)
   - Set resources requests/limits ƒë·ªÉ t·∫°o ngh·∫Ωn c√≥ ki·ªÉm so√°t
3) D·ª±ng Gateway/Scheduler:
   - Config ALG=rr|pq|latency
   - Log JSON 1 d√≤ng/req v√† ƒëo:
     - t1_recv, decision_site, queue_wait_ms, t2_send
   - Th·ª±c hi·ªán decision rule v√† 3 thu·∫≠t to√°n:
     - RR: lu√¢n phi√™n
     - PQ: ∆∞u ti√™n theo priority/deadline
     - Latency-aware: cost = a*RTT(UE->site) + b*queue_wait(site) + c*exec_estimate(site)
4) Traffic generator t·ª´ UE node:
   - Pod trafficgen pinned v√†o node ue
   - Ch·∫°y test light/medium/heavy; (tu·ª≥ ch·ªçn) m√¥ ph·ªèng kh√°c RTT b·∫±ng tc netem theo ƒë√≠ch mec1/mec2/cloud
5) KPI & thu th·∫≠p d·ªØ li·ªáu (t·ªëi thi·ªÉu tr∆∞·ªõc, Prometheus sau):
   - E2E latency p50/p95/p99 (UE ƒëo t0_send -> t3_recv)
   - Throughput req/s
   - Queue delay/length (gateway ƒëo)
   - CPU/mem: kubectl top / metrics
   - SLA miss rate: % request ho√†n th√†nh sau deadline
6) Ma tr·∫≠n th√≠ nghi·ªám t·ªëi thi·ªÉu:
   - Algorithm: RR | PQ | Latency-aware
   - Load: L/M/H
   - UE count: 1 / 5 / 20 (t√πy nƒÉng l·ª±c)
   - Mobility: static | ƒë·ªïi TAC mec1<->mec2
   - Deadline mix: tight vs loose, priority ratio (vd 20/80)

=== QUY T·∫ÆC OFFLOAD ƒê·ªÄ XU·∫§T (R√ï R√ÄNG ƒê·ªÇ CH·ª®NG MINH) ===
- predicted_finish(site) = now + queue_wait_est(site) + exec_est(site)
- N·∫øu predicted_finish(nearest_edge) > deadline => offload cloud
- N·∫øu queue_len_edge > Q ho·∫∑c cpu_edge > C => offload cloud
- Ng∆∞·ª£c l·∫°i => ch·ªçn nearest edge (mec1/mec2) theo RTT/TAC; trong latency-aware d√πng cost nh·ªè nh·∫•t.

=== Y√äU C·∫¶U ƒê·∫¶U RA T·ª™ B·∫†N (CHATGPT) ===
- ƒê·ª´ng h·ªèi l·∫°i c√°c th√¥ng tin ƒë√£ n√™u. N·∫øu thi·∫øu, h√£y ƒë∆∞a gi·∫£ ƒë·ªãnh r√µ r√†ng v√† h·ª£p l√Ω.
- H√£y t·∫°o:
  1) Danh s√°ch manifest Kubernetes (Deploy/Service/ConfigMap) cho mec1/mec2/cloud/gateway/trafficgen (c√≥ nodeSelector+tolerations)
  2) Thi·∫øt k·∫ø API + format log JSON chu·∫©n ho√° + c√°ch t√≠nh KPI
  3) K·ªãch b·∫£n ch·∫°y benchmark (l·ªánh ch·∫°y, tham s·ªë L/M/H, s·ªë req, concurrency)
  4) C√°ch ghi k·∫øt qu·∫£ ra CSV v√† g·ª£i √Ω plot 4 bi·ªÉu ƒë·ªì ch√≠nh: latency p95, throughput, SLA miss, CPU util theo t·∫£i
- ∆Øu ti√™n gi·∫£i ph√°p nhanh ra s·ªë li·ªáu ƒë·ªÉ vi·∫øt b√°o c√°o, sau ƒë√≥ m·ªõi t·ªëi ∆∞u/Prometheus/Grafana.

B·∫Øt ƒë·∫ßu t·ª´ b∆∞·ªõc 1: ƒë·ªÅ xu·∫•t c·ª• th·ªÉ label/toleration/nodeSelector v√† skeleton manifest cho 3 compute service + gateway.

##############
# 01/01/2026 #
##############

PROMPT (ƒê·ªÄ T√ÄI 1 ‚Äì MEC Offloading & Scheduling)

M√¨nh ƒëang m√¥ ph·ªèng 5G SA + MEC b·∫±ng Open5GS + UERANSIM tr√™n Kubernetes ƒë·ªÉ l√†m ƒë·ªÅ t√†i:
‚ÄúNghi√™n c·ª©u chuy·ªÉn t·∫£i v√† l·∫≠p l·ªãch t√†i nguy√™n trong m·∫°ng MEC‚Äù.

1) M√¥ h√¨nh h·ªá th·ªëng hi·ªán t·∫°i
K8s cluster c√≥ nhi·ªÅu node:
masterk8s (ƒëi·ªÅu ph·ªëi, c√≥ Open5GS control-plane)
open5gs (site MEC1)
workerk8s (site MEC2)
ue (node ch·∫°y UE pod)
Open5GS core: AMF ch·∫°y NGAP tr√™n 10.10.3.200:38412 (multus interface n3).
2 gNB UERANSIM (m√¥ ph·ªèng 2 v√πng MEC):
gNB mec1: ngapIp/gtpIp=10.10.3.231, tac=1, cellid=0x10
gNB mec2: ngapIp/gtpIp=10.10.3.232, tac=2, cellid=0x11
UE UERANSIM:
SUPI imsi-001010000000001, DNN internet, slice sst=1 sd=000001
Registration OK, PDU session OK
uesimtun0 ƒë∆∞·ª£c c·∫•p IP (v√≠ d·ª• 10.41.0.x)
Ping 8.8.8.8 OK t·ª´ UE pod ‚Üí user-plane ra Internet ho·∫°t ƒë·ªông.
K·∫øt n·ªëi N3 gi·ªØa c√°c node ƒë∆∞·ª£c fix b·∫±ng OVS bridge n3br + VXLAN (key=103, udp/4789) ƒë·ªÉ ƒë·∫£m b·∫£o gNB‚ÜîAMF ping ƒë∆∞·ª£c v√† SCTP NG Setup OK.
Mobility test ƒë√£ ch·ª©ng minh UE chuy·ªÉn TAC:
UE log: ‚ÄúSelected cell tac[1] ‚Ä¶‚Äù r·ªìi sau ƒë√≥ ‚ÄúSelected cell tac[2] ‚Ä¶‚Äù
AMF log: nh·∫≠n InitialUEMessage v·ªõi TAC[1] CellID[0x10] v√† sau ƒë√≥ TAC[2] CellID[0x11]
gNB mec2 log: nh·∫≠n Initial NAS + Initial Context Setup + PDU session resources setup cho UE.

2) M·ª•c ti√™u ƒë·ªÅ t√†i 1 (Offloading & Scheduling trong MEC)
M√¨nh mu·ªën x√¢y d·ª±ng m√¥ h√¨nh offloading: UE g·ª≠i task (request) t·ªõi MEC, MEC quy·∫øt ƒë·ªãnh:
x·ª≠ l√Ω ·ªü MEC1/MEC2 (edge) ho·∫∑c offload l√™n cloud (n·∫øu c√≥), v√†
l·∫≠p l·ªãch t√†i nguy√™n compute/network t·∫°i MEC.
M√¨nh c·∫ßn b·∫°n h∆∞·ªõng d·∫´n theo c√°c ph·∫ßn:

M√¥ h√¨nh ho√° b√†i to√°n:
Task model: k√≠ch th∆∞·ªõc input, s·ªë chu k·ª≥ CPU/MI, deadline/latency budget, ƒë·ªô ∆∞u ti√™n.
MEC resource model: CPU cores, queue, service rate, gi·ªõi h·∫°n ƒë·ªìng th·ªùi.
Network model: RTT UE‚ÜíMEC, bandwidth gi·∫£ l·∫≠p, latency theo site (mec1 vs mec2).
Objective: minimize latency/violation, maximize throughput, fairness, utilization, minimize drops.
Thi·∫øt k·∫ø & tri·ªÉn khai 3 thu·∫≠t to√°n l·∫≠p l·ªãch ƒë·ªÉ so s√°nh:
Round-robin
Priority queue (theo priority/deadline)
Latency-aware (ch·ªçn MEC site + th·ª© t·ª± x·ª≠ l√Ω ƒë·ªÉ gi·∫£m E2E latency; c√≥ th·ªÉ d√πng cost function)
Sinh l∆∞u l∆∞·ª£ng b·∫±ng UERANSIM:
T·ª´ UE pod t·∫°o traffic HTTP/UDP (nhi·ªÅu flow) ho·∫∑c nhi·ªÅu UE pod (ue1, ue2, ‚Ä¶).
Control bi·∫øn: request rate, packet size, burst, s·ªë UE, t·ªâ l·ªá task priority.
ƒêo KPI (ch∆∞a c√≥ Prometheus/Grafana):
End-to-end latency (client timestamp)
Task completion time / response time distribution (p50/p95/p99)
Throughput (req/s, Mbps)
Packet loss / retries
Queueing delay, queue length
CPU/memory utilization t·∫°i MEC pods (kubectl top / cgroup stats)
SLA violation rate (deadline miss)
Ph√¢n t√≠ch & ƒë√°nh gi√°:
So s√°nh 3 thu·∫≠t to√°n theo KPI ·ªü nhi·ªÅu m·ª©c t·∫£i (light/medium/heavy)
Nh·∫≠n x√©t trade-off: latency vs fairness vs utilization
K·∫øt lu·∫≠n thu·∫≠t to√°n n√†o ph√π h·ª£p ƒëi·ªÅu ki·ªán MEC (nhi·ªÅu UE, mobility, bursty traffic)
ƒê·ªÅ xu·∫•t c·∫£i ti·∫øn (hybrid, admission control, dynamic weights)

3) Hi·ªán tr·∫°ng / c·∫ßn l√†m ti·∫øp ngay
Hi·ªán core + gNB + UE ƒë√£ ho·∫°t ƒë·ªông v√† c√≥ Internet t·ª´ UE pod.
Ch∆∞a c√†i Prometheus/Grafana.
M√¨nh mu·ªën b·∫Øt ƒë·∫ßu theo h∆∞·ªõng d·ª±ng 2 MEC service (mec1/mec2) + m·ªôt scheduler/gateway (c√≥ th·ªÉ l√† 1 microservice) ƒë·ªÉ th·ª±c thi 3 thu·∫≠t to√°n v√† ƒëo KPI.
H√£y ƒë·ªÅ xu·∫•t ki·∫øn tr√∫c tri·ªÉn khai c·ª• th·ªÉ tr√™n K8s (pods/services, nodeSelector cho mec1/mec2), c√°ch sinh traffic t·ª´ UE, c√°ch log/thu th·∫≠p s·ªë li·ªáu, v√† k·∫ø ho·∫°ch th√≠ nghi·ªám (matrix c√°c test cases).

üëâ H√£y tr·∫£ l·ªùi nh∆∞ m·ªôt ‚Äúroadmap th·ª±c thi + checklist command-level‚Äù, chia r√µ t·ª´ng b∆∞·ªõc tri·ªÉn khai v√† t·ª´ng lo·∫°i output c·∫ßn thu ƒë·ªÉ vi·∫øt lu·∫≠n vƒÉn (h√¨nh/bi·ªÉu ƒë·ªì/b·∫£ng so s√°nh).
