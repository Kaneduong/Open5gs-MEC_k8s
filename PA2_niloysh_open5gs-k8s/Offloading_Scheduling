##############
# 20/01/2026 #
##############

### Lá»‹ch sá»­ 3 ###
2) Má»¥c tiÃªu tiáº¿p theo (Ä‘Ãºng yÃªu cáº§u cá»§a anh)

Thay gateway â€œhttp-echoâ€ báº±ng gateway-scheduler tháº­t:

Nháº­n request JSON: cpu_ms, deadline_ms, priority, task_size

Cháº¡y 3 thuáº­t toÃ¡n: rr | pd | latency

Ghi log JSON (1 dÃ²ng/task) Ä‘á»ƒ parse KPI

Deploy 2â€“3 pods Ä‘á»ƒ test scale (replicas = 3)

Cho trafficgen táº¡o task distribution (cpu_i, deadline_i, priority_i) vÃ  gá»­i theo rate (light/medium/heavy)

Xuáº¥t log -> CSV -> biá»ƒu Ä‘á»“ (p50/p95/p99, SLA miss, throughput)

3) TRIá»‚N KHAI GATEWAY/SCHEDULER THáº¬T (FastAPI) + SCALE 3 PODS
3.1 Dá»n gateway cÅ© (http-echo)

TrÃªn masterk8s:

cd ~/mec-lab/manifests
kubectl delete -f 20-gateway-scheduler.yaml


Output ká»³ vá»ng:

deployment.apps "gateway-scheduler" deleted

service "gateway-scheduler" deleted

ğŸ‘‰ Cháº¡y xong, paste output náº¿u cÃ³ lá»—i; náº¿u khÃ´ng cÃ³ lá»—i ta Ä‘i tiáº¿p.

3.2 Táº¡o ConfigMap chá»©a code gateway (khÃ´ng cáº§n build image)

Táº¡o file: 21-gateway-code-configmap.yaml

cat > 21-gateway-code-configmap.yaml <<'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: gateway-scheduler-code
  namespace: mec
data:
  app.py: |
    import os, time, json, itertools, random
    from fastapi import FastAPI, Request
    from pydantic import BaseModel, Field
    from typing import Optional

    APP_START = time.time()
    POD_NAME = os.getenv("HOSTNAME", "unknown")
    ALG = os.getenv("ALG", "rr")  # rr | pd | latency
    SITES = ["mec1", "mec2", "cloud"]
    rr_cycle = itertools.cycle(SITES)

    # simple moving estimate of per-site queue delay (ms)
    est_q = {s: 0.0 for s in SITES}

    class Task(BaseModel):
      request_id: str
      task_size: int = 1024
      cpu_ms: int = Field(ge=1, le=2000)
      deadline_ms: int = Field(ge=1, le=10000)
      priority: int = Field(ge=1, le=3)

    app = FastAPI()

    def decide_site(t: Task):
      if ALG == "rr":
        return next(rr_cycle)
      if ALG == "pd":
        # prioritize higher priority and tighter deadlines
        # score: higher is better
        scores = {}
        for s in SITES:
          scores[s] = (t.priority * 1000) - (t.deadline_ms) - est_q[s]
        return max(scores, key=scores.get)
      # latency-aware (queue estimate only; network latency can be added later)
      return min(est_q, key=est_q.get)

    @app.get("/healthz")
    def healthz():
      return {"ok": True, "pod": POD_NAME, "alg": ALG, "uptime_s": round(time.time()-APP_START, 1)}

    @app.post("/api/task")
    async def api_task(t: Task, req: Request):
      t1 = time.time()
      site = decide_site(t)

      # simulate queue wait and execution time (in ms)
      # queue grows with load; simple model so we can compare algorithms
      queue_wait_ms = max(0, int(est_q[site] + random.uniform(0, 5)))
      exec_ms = int(t.cpu_ms)

      # update queue estimate: decay + add workload
      est_q[site] = max(0.0, est_q[site] * 0.90 + exec_ms * 0.05)

      # simulate processing
      time.sleep((queue_wait_ms + exec_ms) / 1000.0)

      e2e_ms = int((time.time() - t1) * 1000)
      sla_miss = e2e_ms > t.deadline_ms

      log = {
        "ts": int(time.time()),
        "pod": POD_NAME,
        "alg": ALG,
        "request_id": t.request_id,
        "cpu_ms": t.cpu_ms,
        "deadline_ms": t.deadline_ms,
        "priority": t.priority,
        "task_size": t.task_size,
        "decision_site": site,
        "queue_wait_ms": queue_wait_ms,
        "exec_ms": exec_ms,
        "e2e_latency_ms": e2e_ms,
        "sla_miss": sla_miss,
      }
      print(json.dumps(log), flush=True)
      return {"ok": True, "decision_site": site, "e2e_latency_ms": e2e_ms, "sla_miss": sla_miss}
EOF


Apply:

kubectl apply -f 21-gateway-code-configmap.yaml


Output ká»³ vá»ng: configmap/gateway-scheduler-code created

ğŸ‘‰ Náº¿u OK ta lÃ m tiáº¿p luÃ´n.

3.3 Táº¡o Deployment + Service cho gateway-scheduler (replicas=3)

Táº¡o file: 22-gateway-scheduler-deploy.yaml

cat > 22-gateway-scheduler-deploy.yaml <<'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gateway-scheduler
  namespace: mec
spec:
  replicas: 3
  selector:
    matchLabels: { app: gateway-scheduler }
  template:
    metadata:
      labels: { app: gateway-scheduler }
    spec:
      containers:
      - name: gateway
        image: python:3.11-slim
        env:
        - name: ALG
          value: "rr"   # rr | pd | latency (sáº½ Ä‘á»•i khi cháº¡y thÃ­ nghiá»‡m)
        ports:
        - containerPort: 8080
        command: ["sh","-lc"]
        args:
          - |
            pip install --no-cache-dir fastapi uvicorn pydantic &&
            uvicorn app:app --host 0.0.0.0 --port 8080
        volumeMounts:
        - name: code
          mountPath: /app
        workingDir: /app
        resources:
          requests: { cpu: "300m", memory: "256Mi" }
          limits:   { cpu: "1",    memory: "512Mi" }
      volumes:
      - name: code
        configMap:
          name: gateway-scheduler-code
---
apiVersion: v1
kind: Service
metadata:
  name: gateway-scheduler
  namespace: mec
spec:
  selector:
    app: gateway-scheduler
  ports:
  - name: http
    port: 8080
    targetPort: 8080
  type: ClusterIP
EOF


Apply + theo dÃµi rollout:

kubectl apply -f 22-gateway-scheduler-deploy.yaml
kubectl rollout status deploy/gateway-scheduler -n mec
kubectl get pods -n mec -o wide | grep gateway-scheduler
kubectl get svc -n mec -o wide | grep gateway-scheduler

Output Ä‘Ãºng ká»³ vá»ng

rollout status bÃ¡o successfully rolled out

CÃ³ 3 pods gateway-scheduler-... Running

Service váº«n cÃ³ ClusterIP

ğŸ‘‰ Anh paste 3 output:

kubectl rollout status ...

kubectl get pods ... | grep gateway-scheduler

kubectl get svc ... | grep gateway-scheduler

3.4 Test tá»« UE (trafficgen-test) vÃ o endpoint tháº­t

Cháº¡y:

kubectl exec -n mec -it trafficgen-test -- sh -lc \
'curl -sS -m 3 http://gateway-scheduler.mec.svc.cluster.local:8080/healthz'


Output Ä‘Ãºng ká»³ vá»ng: JSON cÃ³ ok, pod, alg.

Tiáº¿p theo test POST 1 task:

kubectl exec -n mec -it trafficgen-test -- sh -lc \
'curl -sS -m 10 -X POST http://gateway-scheduler.mec.svc.cluster.local:8080/api/task \
 -H "Content-Type: application/json" \
 -d "{\"request_id\":\"t1\",\"task_size\":1024,\"cpu_ms\":50,\"deadline_ms\":120,\"priority\":2}"'


Output Ä‘Ãºng: JSON cÃ³ decision_site, e2e_latency_ms, sla_miss.

ğŸ‘‰ Paste output 2 lá»‡nh curl.

4) TrafficGen tháº­t (táº¡o cpu_i/deadline_i/priority_i theo distribution) â€“ bÆ°á»›c káº¿ tiáº¿p

Ngay khi gateway 3 pods cháº¡y OK, tÃ´i sáº½ hÆ°á»›ng dáº«n:

Táº¡o trafficgen dáº¡ng Deployment (replicas=1) pin lÃªn node ue

CÃ³ cáº¥u hÃ¬nh ENV Ä‘á»ƒ chá»n:

LOAD=light/medium/heavy (rate)

DISTRIBUTION=uniform/weighted

Tá»± sinh request_id + gá»­i liÃªn tá»¥c

Thu log gateway vÃ  parse ra CSV + váº½ biá»ƒu Ä‘á»“

### output 2 ###
1) Check output: PASS/FAIL
âœ… BÆ°á»›c 4 (Compute sites) â€” PASS
kubectl get pods -n mec -o wide cho tháº¥y:
mec1 cháº¡y trÃªn open5gs âœ…
mec2 cháº¡y trÃªn workerk8s âœ…
cloud cháº¡y trÃªn masterk8s âœ…
trafficgen-test cháº¡y trÃªn ue âœ…

âœ… BÆ°á»›c 5 (Gateway + Service) â€” PASS
gateway pod Running âœ… (Ä‘ang cháº¡y trÃªn open5gs â€“ cháº¥p nháº­n)
service ClusterIP: 10.100.46.245:8080 âœ…

âœ… BÆ°á»›c 6 (DNS + HTTP tá»« UE) â€” PASS
nslookup resolve Ä‘Ãºng ClusterIP âœ…
curl tráº£ OK âœ…
=> Há»‡ thá»‘ng â€œplumbingâ€ Ä‘Ã£ thÃ´ng.

### Lá»‹ch sá»­ 2 ###
BÆ¯á»šC 4: Deploy Compute Services (mec1/mec2/cloud) â€“ cháº¡y Ä‘Ãºng node
4.0 Thá»±c hiá»‡n á»Ÿ Ä‘Ã¢u?

TrÃªn masterk8s (10.10.33.45), trong thÆ° má»¥c manifests:

cd ~/mec-lab/manifests

4.1 Táº¡o file manifest

Táº¡o file: 10-compute-sites.yaml

cat > 10-compute-sites.yaml <<'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mec1
  namespace: mec
spec:
  replicas: 1
  selector:
    matchLabels: { app: mec1 }
  template:
    metadata:
      labels: { app: mec1 }
    spec:
      nodeSelector:
        mec-site: mec1
      containers:
      - name: mec1
        image: nginx:stable
        resources:
          requests: { cpu: "500m", memory: "500Mi" }
          limits:   { cpu: "1",    memory: "1Gi" }
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mec2
  namespace: mec
spec:
  replicas: 1
  selector:
    matchLabels: { app: mec2 }
  template:
    metadata:
      labels: { app: mec2 }
    spec:
      nodeSelector:
        mec-site: mec2
      containers:
      - name: mec2
        image: nginx:stable
        resources:
          requests: { cpu: "500m", memory: "500Mi" }
          limits:   { cpu: "1",    memory: "1Gi" }
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cloud
  namespace: mec
spec:
  replicas: 1
  selector:
    matchLabels: { app: cloud }
  template:
    metadata:
      labels: { app: cloud }
    spec:
      nodeSelector:
        mec-site: cloud
      containers:
      - name: cloud
        image: nginx:stable
        resources:
          requests: { cpu: "1", memory: "1Gi" }
          limits:   { cpu: "2", memory: "2Gi" }
EOF

4.2 Apply + kiá»ƒm tra
kubectl apply -f 10-compute-sites.yaml
kubectl get pods -n mec -o wide

Output Ä‘Ãºng ká»³ vá»ng (quan trá»ng)

Anh pháº£i tháº¥y 3 pod Running vÃ  cá»™t NODE Ä‘Ãºng:

mec1-... cháº¡y trÃªn open5gs

mec2-... cháº¡y trÃªn workerk8s

cloud-... cháº¡y trÃªn masterk8s

ğŸ‘‰ Anh paste output kubectl get pods -n mec -o wide sau bÆ°á»›c nÃ y.

BÆ¯á»šC 5: Deploy Gateway/Scheduler + Service (Ä‘á»ƒ trafficgen gá»i)

á» bÆ°á»›c nÃ y, má»¥c tiÃªu trÆ°á»›c máº¯t: cÃ³ 1 endpoint á»•n Ä‘á»‹nh trong cluster (DNS + service).
VÃ¬ anh chÆ°a gá»­i image gateway tháº­t, ta lÃ m theo 2 táº§ng:

Táº§ng A (test plumbing): deploy gateway â€œecho serverâ€ Ä‘á»ƒ test POST/JSON vÃ  log.

Táº§ng B (production): thay báº±ng gateway tháº­t sau.

5.1 Gateway táº¡m thá»i: dÃ¹ng http-echo (cá»±c nháº¹, test nhanh)

Táº¡o file: 20-gateway-scheduler.yaml

cat > 20-gateway-scheduler.yaml <<'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gateway-scheduler
  namespace: mec
spec:
  replicas: 1
  selector:
    matchLabels: { app: gateway-scheduler }
  template:
    metadata:
      labels: { app: gateway-scheduler }
    spec:
      containers:
      - name: gateway
        image: hashicorp/http-echo:1.0.0
        args:
          - "-listen=:8080"
          - "-text=OK"
        env:
          - name: ALG
            value: "rr"
        ports:
          - containerPort: 8080
        resources:
          requests: { cpu: "200m", memory: "128Mi" }
          limits:   { cpu: "500m", memory: "256Mi" }
---
apiVersion: v1
kind: Service
metadata:
  name: gateway-scheduler
  namespace: mec
spec:
  selector:
    app: gateway-scheduler
  ports:
  - name: http
    port: 8080
    targetPort: 8080
  type: ClusterIP
EOF


Apply + check:

kubectl apply -f 20-gateway-scheduler.yaml
kubectl get pods -n mec -o wide | grep gateway-scheduler
kubectl get svc -n mec -o wide | grep gateway-scheduler

Output Ä‘Ãºng ká»³ vá»ng

Pod gateway-scheduler Running

Service gateway-scheduler cÃ³ ClusterIP

ğŸ‘‰ Paste 2 outputs trÃªn.

BÆ¯á»šC 6: Test connectivity tá»« node UE (trafficgen-test) â†’ gateway service

Anh Ä‘Ã£ cÃ³ pod trafficgen-test cháº¡y trÃªn node ue. Ta dÃ¹ng luÃ´n pod Ä‘Ã³ Ä‘á»ƒ test DNS + HTTP.

6.1 Exec vÃ o trafficgen-test vÃ  curl
kubectl exec -n mec -it trafficgen-test -- sh -lc \
'nslookup gateway-scheduler.mec.svc.cluster.local && \
 curl -sS -m 3 http://gateway-scheduler.mec.svc.cluster.local:8080'

Output Ä‘Ãºng ká»³ vá»ng

nslookup tráº£ vá» IP (ClusterIP)

curl tráº£ vá» OK

ğŸ‘‰ Paste output lá»‡nh nÃ y.

BÆ¯á»šC 7: Chuáº©n bá»‹ TrafficGen tháº­t Ä‘á»ƒ táº¡o cpu_i, deadline_i, priority_i (nhÆ°ng lÃ m theo kiá»ƒu an toÃ n)

Hiá»‡n gateway táº¡m chá»‰ tráº£ â€œOKâ€ nÃªn chÆ°a xá»­ lÃ½ JSON.
Äá»ƒ Ä‘Ãºng má»¥c tiÃªu luáº­n vÄƒn, ta cáº§n gateway cÃ³ endpoint nháº­n JSON + ghi log.

VÃ¬ váº­y bÆ°á»›c káº¿ tiáº¿p tÃ´i sáº½ dá»±ng gateway-scheduler báº£n tá»‘i thiá»ƒu (Python FastAPI), cháº¡y trong K8s, cÃ³:

/api/task nháº­n JSON (cpu_ms/deadline_ms/priority)

â€œdecisionâ€ giáº£ láº­p (rr/pd/latency)

log JSON 1 dÃ²ng/task

NhÆ°ng trÆ°á»›c khi tÃ´i Ä‘Æ°a manifest, tÃ´i cáº§n anh xÃ¡c nháº­n 1 Ä‘iá»ƒm Ä‘á»ƒ khá»i lÃ m sai:

Anh muá»‘n gateway cháº¡y dÆ°á»›i dáº¡ng:

Deployment 1 pod (Ä‘á»§ cho thÃ­ nghiá»‡m ban Ä‘áº§u), hay

Deployment 2-3 pod (Ä‘á»ƒ test scale)?

Náº¿u anh khÃ´ng tráº£ lá»i, tÃ´i sáº½ máº·c Ä‘á»‹nh 1 pod Ä‘á»ƒ Ä‘i nhanh.

Anh gá»­i output Ä‘á»ƒ tÃ´i xÃ¡c nháº­n (theo thá»© tá»±)

kubectl get pods -n mec -o wide (sau BÆ°á»›c 4)

kubectl get pods -n mec -o wide | grep gateway-scheduler

kubectl get svc -n mec -o wide | grep gateway-scheduler

Output kubectl exec ... nslookup ... curl ...

Sau khi anh gá»­i, tÃ´i sáº½:

Chá»‘t PASS/FAIL

Náº¿u PASS: gá»­i ngay BÆ°á»›c 8 (gateway FastAPI + logging + metrics + script parse/váº½ biá»ƒu Ä‘á»“).

### Output 1 ###
1) Káº¿t quáº£ kiá»ƒm tra: PASS/FAIL
âœ… A. kubectl get nodes -o wide â€” PASS
Äá»§ 4 node vÃ  IP Ä‘Ãºng nhÆ° kiáº¿n trÃºc:
masterk8s: 10.10.33.45
open5gs: 10.10.33.54
ue: 10.10.33.40
workerk8s: 10.10.33.60

âœ… B. Labels â€” PASS
Anh Ä‘Ã£ gÃ¡n Ä‘Ãºng:
masterk8s cÃ³ mec-site=cloud
open5gs cÃ³ mec-site=mec1
workerk8s cÃ³ mec-site=mec2
ue cÃ³ cáº£ 2 label:
node-role.kubernetes.io/ue=true (label ná»n táº£ng anh dÃ¹ng)
role=ue (label theo template file)
=> ÄÃ¢y lÃ  tÃ¬nh huá»‘ng lÃ½ tÆ°á»Ÿng, khÃ´ng xung Ä‘á»™t.

âœ… C. Taint UE â€” PASS
ue cÃ³ taint:
dedicated=ue:NoSchedule
Chuáº©n Ä‘á»ƒ â€œpin workloadâ€ trafficgen/UE.

âœ… D. Trafficgen pinned Ä‘Ãºng node â€” PASS
trafficgen-test Ä‘ang:
Running
Node = ue
IP Pod: 192.168.131.206
=> NodeSelector + toleration cá»§a anh Ä‘Ãºng tuyá»‡t Ä‘á»‘i.

### Lá»‹ch sá»­ 1 ###
BÆ¯á»šC 0 (Báº®T BUá»˜C): Cháº¡y á»Ÿ Ä‘Ã¢u? kiá»ƒm tra context trÆ°á»›c

âœ… Thá»±c hiá»‡n cÃ¡c lá»‡nh kubectl ... trÃªn mÃ¡y cÃ³ kubeconfig (khuyáº¿n nghá»‹: masterk8s 10.10.33.45).

TrÃªn masterk8s cháº¡y:

kubectl cluster-info
kubectl get nodes -o wide

Output ká»³ vá»ng

Tháº¥y Ä‘á»§ 4 node: masterk8s open5gs workerk8s ue

Cá»™t INTERNAL-IP Ä‘Ãºng: 10.10.33.45 / 54 / 60 / 40

ğŸ‘‰ Anh paste output kubectl get nodes -o wide (chá»‰ cáº§n 1 láº§n, Ä‘á»ƒ tÃ´i bÃ¡m IP cá»§a anh).

BÆ¯á»šC 1: Cáº¥u hÃ¬nh Label vÃ  Toleration
1.1 GÃ¡n label â€œmec-siteâ€ cho 3 node compute + label UE (theo file)

Cháº¡y Ä‘Ãºng cÃ¡c lá»‡nh sau (thÃªm --overwrite Ä‘á»ƒ trÃ¡nh lá»—i Ä‘Ã£ tá»“n táº¡i):

kubectl label nodes open5gs mec-site=mec1 --overwrite
kubectl label nodes workerk8s mec-site=mec2 --overwrite
kubectl label nodes masterk8s mec-site=cloud --overwrite
kubectl label nodes ue role=ue --overwrite

Output ká»³ vá»ng

Má»—i lá»‡nh sáº½ tráº£ vá» dáº¡ng:

node/open5gs labeled

node/workerk8s labeled

node/masterk8s labeled

node/ue labeled

LÆ°u Ã½ quan trá»ng:
Anh Ä‘Ã£ cÃ³ label node-role.kubernetes.io/ue=true. Viá»‡c thÃªm role=ue khÃ´ng xung Ä‘á»™t, chá»‰ lÃ  thÃªm 1 label ná»¯a cho Ä‘Ãºng â€œtemplateâ€ trong file.

1.2 Kiá»ƒm tra label Ä‘Ã£ apply Ä‘Ãºng chÆ°a

Cháº¡y:

kubectl get nodes --show-labels

Output Ä‘Ãºng (ká»³ vá»ng)

Anh pháº£i nhÃ¬n tháº¥y:

Node open5gs cÃ³ mec-site=mec1

Node workerk8s cÃ³ mec-site=mec2

Node masterk8s cÃ³ mec-site=cloud

Node ue cÃ³ role=ue vÃ  node-role.kubernetes.io/ue=true

ğŸ‘‰ Anh paste output kubectl get nodes --show-labels.

1.3 Toleration/taint cho node UE (Ä‘á»ƒ pin UE/TrafficGen)

VÃ¬ há»‡ anh Ä‘ang dÃ¹ng taint dedicated=ue:NoSchedule, ta cáº§n xÃ¡c minh taint nÃ y cÃ²n tá»“n táº¡i.

Cháº¡y:

kubectl describe node ue | sed -n '/Taints:/,/Conditions:/p'

Output Ä‘Ãºng (ká»³ vá»ng)

Trong pháº§n Taints: pháº£i tháº¥y dÃ²ng tÆ°Æ¡ng tá»±:

dedicated=ue:NoSchedule

ğŸ‘‰ Anh paste output pháº§n nÃ y.

Náº¿u khÃ´ng tháº¥y taint, tÃ´i sáº½ Ä‘Æ°a lá»‡nh taint láº¡i chuáº©n theo há»‡ anh.

BÆ¯á»šC 2: Táº¡o Namespace â€œmecâ€

Anh cÃ³ 2 cÃ¡ch: YAML (chuáº©n luáº­n vÄƒn/lab) hoáº·c kubectl create (nhanh). TÃ´i khuyáº¿n nghá»‹ YAML Ä‘á»ƒ quáº£n lÃ½ phiÃªn báº£n.

2.1 Táº¡o file YAML trÃªn masterk8s

TrÃªn masterk8s:

mkdir -p ~/mec-lab/manifests
cd ~/mec-lab/manifests


Táº¡o file 00-namespace-mec.yaml vá»›i ná»™i dung:

apiVersion: v1
kind: Namespace
metadata:
  name: mec


Apply:

kubectl apply -f 00-namespace-mec.yaml

Output ká»³ vá»ng

namespace/mec created (hoáº·c configured náº¿u Ä‘Ã£ cÃ³)

2.2 Kiá»ƒm tra namespace

Cháº¡y:

kubectl get namespaces | grep -E '^mec\b'

Output Ä‘Ãºng

mec Active ...

ğŸ‘‰ Anh paste output 2 lá»‡nh:

kubectl apply -f 00-namespace-mec.yaml

kubectl get namespaces | grep -E '^mec\b'

BÆ¯á»šC 3: Cáº¥u hÃ¬nh Toleration + NodeSelector cho TrafficGen/UE

VÃ¬ há»‡ cá»§a anh cÃ³:

label UE node: node-role.kubernetes.io/ue=true (anh Ä‘Ã£ set)

taint UE node: dedicated=ue:NoSchedule

=> Pod TrafficGen pháº£i cÃ³:

nodeSelector: node-role.kubernetes.io/ue: "true"

toleration Ä‘Ãºng key/value/effect cá»§a taint

3.1 Manifest Pod TrafficGen (test pinning + DNS)

Táº¡o file 30-trafficgen-test.yaml:

apiVersion: v1
kind: Pod
metadata:
  name: trafficgen-test
  namespace: mec
spec:
  nodeSelector:
    node-role.kubernetes.io/ue: "true"
  tolerations:
  - key: "dedicated"
    operator: "Equal"
    value: "ue"
    effect: "NoSchedule"
  containers:
  - name: curl
    image: curlimages/curl:8.5.0
    command: ["sh","-lc"]
    args:
      - |
        echo "trafficgen-test running on node:" &&
        hostname &&
        sleep 3600


Apply vÃ  kiá»ƒm tra pod cháº¡y Ä‘Ãºng node ue:

kubectl apply -f 30-trafficgen-test.yaml
kubectl get pod -n mec -o wide | grep trafficgen-test

Output Ä‘Ãºng (ká»³ vá»ng)

Pod trafficgen-test tráº¡ng thÃ¡i Running

Cá»™t NODE = ue (IP node 10.10.33.40)

ğŸ‘‰ Anh paste output kubectl get pod -n mec -o wide | grep trafficgen-test.


### Gá»£i Ã½ lÃ½ thuyáº¿t ###
Sau khi Ä‘Ã£ cáº¥u hÃ¬nh K8s + MEC + Scheduler nhÆ° trong file, lÃ m sao táº¡o workload (cpu_i, deadline_i, priority_i), cho há»‡ thá»‘ng cháº¡y tháº­t, thu káº¿t quáº£, váº½ biá»ƒu Ä‘á»“ / dashboard?
TÃ´i tráº£ lá»i theo Ä‘Ãºng hiá»‡n tráº¡ng file anh gá»­i, khÃ´ng giáº£ Ä‘á»‹nh thÃªm 

Tá»”NG QUAN LUá»’NG THá»°C NGHIá»†M (END-TO-END)
TrafficGen (UE)
   â†“  (JSON task)
Gateway / Scheduler
   â†“  (decision)
MEC1 / MEC2 / Cloud
   â†“  (process)
Logs + Metrics
   â†“
CSV / Prometheus
   â†“
Python / Grafana

1. Táº O cpu_i, deadline_i, priority_i Tá»ª ÄÃ‚U?

ğŸ‘‰ KHÃ”NG táº¡o trong Kubernetes
ğŸ‘‰ Táº O Tá»ª TRAFFICGEN (UE side)

1.1 Task model (chuáº©n cho luáº­n vÄƒn)

Má»—i request = 1 task i

{
  "request_id": "uuid",
  "cpu_ms": 50,
  "deadline_ms": 120,
  "priority": 2,
  "task_size": 1024
}

Biáº¿n	Ã nghÄ©a	Ghi chÃº há»c thuáº­t
cpu_ms	cpu_i	workload
deadline_ms	d_i	SLA
priority	p_i	QoS class
task_size	input size	optional
2. VIáº¾T TRAFFICGEN (CÃCH ÄÆ N GIáº¢N â€“ HIá»†U QUáº¢)
2.1 Logic TrafficGen (pseudo-code)
import random, time, requests, uuid

while True:
    task = {
        "request_id": str(uuid.uuid4()),
        "cpu_ms": random.choice([20, 50, 100, 200]),
        "deadline_ms": random.choice([80, 120, 200]),
        "priority": random.choice([1, 2, 3]),
        "task_size": random.choice([512, 1024])
    }

    requests.post("http://gateway-scheduler/api/task", json=task)
    time.sleep(0.05)  # load control

2.2 Mapping load level (Ráº¤T QUAN TRá»ŒNG)
Load	sleep	req/s
Light	0.2s	~5
Medium	0.05s	~20
Heavy	0.01s	~100

ğŸ“Œ Chá»‰ Ä‘á»•i sleep lÃ  Ä‘á»•i ká»‹ch báº£n thÃ­ nghiá»‡m â†’ há»™i Ä‘á»“ng ráº¥t thÃ­ch

3. SCHEDULER Xá»¬ LÃ TASK NHÆ¯ THáº¾ NÃ€O?
3.1 Round Robin (RR)
site = sites[next_index]

3.2 Priorityâ€“Deadline
score = alpha * priority - beta * deadline
site = argmax(score)

3.3 Latency-aware
site = argmin(
    estimated_queue_delay + network_latency
)


ğŸ‘‰ Anh KHÃ”NG cáº§n tá»‘i Æ°u toÃ¡n há»c cao siÃªu
ğŸ‘‰ Quan trá»ng lÃ  so sÃ¡nh káº¿t quáº£

4. MEC SERVICE â€œGIáº¢ Láº¬P CPUâ€ RA SAO?
4.1 CÃ¡ch mÃ´ phá»ng CPU (Ä‘á»§ cho tháº¡c sÄ©)
import time

def process(cpu_ms):
    start = time.time()
    while (time.time() - start) * 1000 < cpu_ms:
        pass


ğŸ“Œ KhÃ´ng cáº§n cháº¡y task tháº­t
ğŸ“Œ CÃ¡i anh cáº§n lÃ  Ä‘á»™ trá»… & tranh cháº¥p tÃ i nguyÃªn

5. LOG PHáº¢I GHI NHÆ¯ THáº¾ NÃ€O? (Ráº¤T QUAN TRá»ŒNG)
5.1 Log chuáº©n (JSON â€“ 1 dÃ²ng / task)
{
  "request_id": "...",
  "alg": "rr",
  "priority": 2,
  "deadline_ms": 120,
  "site": "mec1",
  "queue_wait_ms": 18,
  "exec_ms": 50,
  "e2e_latency_ms": 92,
  "sla_miss": false,
  "timestamp": 1700000000
}


ğŸ“Œ KHÃ”NG log text lung tung
ğŸ“Œ Há»™i Ä‘á»“ng cá»±c ká»³ thÃ­ch log JSON

6. THU KPI NHÆ¯ THáº¾ NÃ€O?
6.1 CÃ¡ch 1 â€“ Äá»¦ & NHANH (Python + CSV) âœ…
kubectl logs gateway-scheduler > logs.json


Parse báº±ng Python:

import pandas as pd
df = pd.read_json("logs.json", lines=True)

df["e2e_latency_ms"].quantile([0.5, 0.95, 0.99])
df["sla_miss"].mean()


ğŸ‘‰ Äá»¦ Ä‘á»ƒ tá»‘t nghiá»‡p

6.2 CÃ¡ch 2 â€“ PRO (Prometheus + Grafana) â­

Náº¿u muá»‘n dashboard Ä‘áº¹p:

Gateway expose /metrics

DÃ¹ng:

latency_bucket

sla_miss_total

Grafana váº½:

p50 / p95 / p99

SLA miss %

CPU per MEC site

ğŸ“Œ KhÃ´ng báº¯t buá»™c, nhÆ°ng ráº¥t â€œÄƒn Ä‘iá»ƒmâ€

7. BIá»‚U Äá»’ NÃ€O NÃŠN Váº¼? (KHÃ”NG Váº¼ THá»ªA)
Biá»ƒu Ä‘á»“	Má»¥c Ä‘Ã­ch
Latency p95 vs Load	So sÃ¡nh thuáº­t toÃ¡n
SLA miss vs Load	Chá»©ng minh priority
Throughput vs Load	ÄÃ¡nh Ä‘á»•i
Queue delay per site	Hiá»‡u quáº£ offloading
8. Lá»˜ TRÃŒNH THá»°C NGHIá»†M CHUáº¨N (Gá»¢I Ã)

Fix RR â†’ cháº¡y á»•n

Add Priorityâ€“Deadline

Add Latency-aware

Má»—i thuáº­t toÃ¡n:

Light / Medium / Heavy

CÃ³ / khÃ´ng mobility

Má»—i case cháº¡y â‰¥ 5 phÃºt

ğŸ‘‰ ÄÃ¢y lÃ  ChÆ°Æ¡ng 5 luáº­n vÄƒn hoÃ n chá»‰nh

9. TÃ“M Láº I â€“ ANH ÄANG á» ÄÃšNG CHá»–

File hiá»‡n táº¡i = xÆ°Æ¡ng sá»‘ng thá»±c nghiá»‡m

Viá»‡c anh cáº§n lÃ m NGAY BÃ‚Y GIá»œ:

Viáº¿t trafficgen

Chuáº©n hÃ³a log

Cháº¡y load

Xuáº¥t CSV

Váº½ biá»ƒu Ä‘á»“

ğŸ‘‰ Náº¿u anh muá»‘n, bÆ°á»›c tiáº¿p theo tÃ´i cÃ³ thá»ƒ:

Viáº¿t trafficgen hoÃ n chá»‰nh

Viáº¿t scheduler skeleton (RR / Priority / Latency)

Viáº¿t script parse log + váº½ biá»ƒu Ä‘á»“ (Python)


##############
# 13/01/2026 #
##############
BÆ°á»›c 1: Cáº¥u hÃ¬nh Label vÃ  Toleration
1.1 Label cÃ¡c Nodes

TrÆ°á»›c tiÃªn, báº¡n sáº½ cáº§n gÃ¡n label cho cÃ¡c nodes Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng cÃ¡c dá»‹ch vá»¥ sáº½ Ä‘Æ°á»£c phÃ¢n phá»‘i Ä‘Ãºng node.

kubectl label nodes open5gs mec-site=mec1
kubectl label nodes workerk8s mec-site=mec2
kubectl label nodes masterk8s mec-site=cloud
kubectl label nodes ue role=ue


Output Kiá»ƒm Tra:
Äá»ƒ kiá»ƒm tra xem cÃ¡c label Ä‘Ã£ Ä‘Æ°á»£c Ã¡p dá»¥ng chÃ­nh xÃ¡c hay chÆ°a, báº¡n cÃ³ thá»ƒ dÃ¹ng lá»‡nh sau:

kubectl get nodes --show-labels


Má»—i node sáº½ cÃ³ label tÆ°Æ¡ng á»©ng nhÆ° mec-site=mec1, mec-site=mec2, v.v.

BÆ°á»›c 2: Táº¡o Namespace â€œmecâ€

Táº¡o má»™t namespace Ä‘á»ƒ chá»©a táº¥t cáº£ cÃ¡c dá»‹ch vá»¥ liÃªn quan Ä‘áº¿n MEC.

2.1 Táº¡o Namespace
apiVersion: v1
kind: Namespace
metadata:
  name: mec


Output Kiá»ƒm Tra:

kubectl get namespaces


Namespace mec sáº½ xuáº¥t hiá»‡n trong danh sÃ¡ch.

BÆ°á»›c 3: Cáº¥u hÃ¬nh Toleration vÃ  NodeSelector cho TrafficGen/UE

Táº¡o cáº¥u hÃ¬nh Ä‘á»ƒ Ä‘áº£m báº£o TrafficGen/UE chá»‰ Ä‘Æ°á»£c cháº¡y trÃªn node ue.

3.1 Pod TrafficGen vá»›i Toleration vÃ  NodeSelector
apiVersion: v1
kind: Pod
metadata:
  name: trafficgen
  namespace: mec
spec:
  tolerations:
  - key: "role"
    operator: "Equal"
    value: "ue"
    effect: "NoSchedule"
  nodeSelector:
    role: "ue"
  containers:
    - name: trafficgen
      image: your-trafficgen-image
      resources:
        requests:
          memory: "500Mi"
          cpu: "500m"
        limits:
          memory: "1Gi"
          cpu: "1"


Output Kiá»ƒm Tra:
Kiá»ƒm tra xem pod Ä‘Ã£ Ä‘Æ°á»£c schedule Ä‘Ãºng vÃ o node ue:

kubectl get pods -o wide


Pod trafficgen sáº½ Ä‘Æ°á»£c gÃ¡n vÃ o node ue.

BÆ°á»›c 4: Táº¡o vÃ  Cáº¥u hÃ¬nh Compute Services (mec1, mec2, cloud)
4.1 Manifest cho MEC1, MEC2, Cloud

Táº¡o cÃ¡c Deployment vÃ  Service cho cÃ¡c compute sites MEC1, MEC2 vÃ  Cloud.

MEC1 Deployment:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mec1
  namespace: mec
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mec1
  template:
    metadata:
      labels:
        app: mec1
    spec:
      nodeSelector:
        mec-site: mec1
      containers:
        - name: mec1
          image: your-mec1-image
          resources:
            requests:
              memory: "500Mi"
              cpu: "500m"
            limits:
              memory: "1Gi"
              cpu: "1"

MEC2 Deployment:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mec2
  namespace: mec
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mec2
  template:
    metadata:
      labels:
        app: mec2
    spec:
      nodeSelector:
        mec-site: mec2
      containers:
        - name: mec2
          image: your-mec2-image
          resources:
            requests:
              memory: "500Mi"
              cpu: "500m"
            limits:
              memory: "1Gi"
              cpu: "1"

Cloud Deployment:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cloud
  namespace: mec
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cloud
  template:
    metadata:
      labels:
        app: cloud
    spec:
      nodeSelector:
        mec-site: cloud
      containers:
        - name: cloud
          image: your-cloud-image
          resources:
            requests:
              memory: "1Gi"
              cpu: "1"
            limits:
              memory: "2Gi"
              cpu: "2"


Output Kiá»ƒm Tra:
Kiá»ƒm tra pod Ä‘Ã£ Ä‘Æ°á»£c deploy vÃ  schedule vÃ o Ä‘Ãºng node:

kubectl get pods -o wide


CÃ¡c pod cá»§a mec1, mec2, vÃ  cloud sáº½ xuáº¥t hiá»‡n trÃªn cÃ¡c node open5gs, workerk8s vÃ  masterk8s tÆ°Æ¡ng á»©ng.

BÆ°á»›c 5: Cáº¥u hÃ¬nh Gateway/Scheduler

Cáº¥u hÃ¬nh dá»‹ch vá»¥ Gateway/Scheduler Ä‘á»ƒ quyáº¿t Ä‘á»‹nh offloading vá»›i 3 thuáº­t toÃ¡n:

5.1 Gateway/Scheduler Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gateway-scheduler
  namespace: mec
spec:
  replicas: 1
  selector:
    matchLabels:
      app: gateway-scheduler
  template:
    metadata:
      labels:
        app: gateway-scheduler
    spec:
      containers:
        - name: gateway-scheduler
          image: your-gateway-image
          env:
            - name: ALG
              value: "rr"  # cÃ³ thá»ƒ lÃ  rr | pq | latency
          resources:
            requests:
              memory: "500Mi"
              cpu: "500m"
            limits:
              memory: "1Gi"
              cpu: "1"


Output Kiá»ƒm Tra:
Kiá»ƒm tra pod cá»§a Gateway/Scheduler Ä‘Ã£ cháº¡y chÆ°a:

kubectl get pods -o wide


Pod gateway-scheduler sáº½ xuáº¥t hiá»‡n.

BÆ°á»›c 6: Kiá»ƒm tra API vÃ  Format Log

API cá»§a Gateway/Scheduler cáº§n pháº£i nháº­n yÃªu cáº§u vá»›i cÃ¡c tham sá»‘ nhÆ° request_id, task_size, cpu_ms, deadline_ms, priority vÃ  tÃ­nh toÃ¡n viá»‡c offloading tá»›i cÃ¡c site MEC hoáº·c Cloud.

6.1 API Format (JSON)
{
  "request_id": "12345",
  "task_size": 1024,
  "cpu_ms": 100,
  "deadline_ms": 200,
  "priority": 1
}

6.2 Log Format (JSON)
{
  "request_id": "12345",
  "t1_recv": 1611243156,
  "decision_site": "mec1",
  "queue_wait_ms": 10,
  "t2_send": 1611243170
}


Output Kiá»ƒm Tra:
Khi API Ä‘Æ°á»£c gá»i, log sáº½ Ä‘Æ°á»£c ghi láº¡i vÃ  cÃ³ thá»ƒ kiá»ƒm tra thÃ´ng qua kubectl logs.

kubectl logs <gateway-scheduler-pod>


Kiá»ƒm tra cÃ¡c logs Ä‘á»ƒ xÃ¡c minh ráº±ng quÃ¡ trÃ¬nh xá»­ lÃ½ yÃªu cáº§u vÃ  quyáº¿t Ä‘á»‹nh offloading Ä‘ang diá»…n ra Ä‘Ãºng cÃ¡ch.

BÆ°á»›c 7: KPI vÃ  Thu tháº­p Dá»¯ liá»‡u

KPI cáº§n Ä‘Æ°á»£c thu tháº­p trong suá»‘t cÃ¡c thá»­ nghiá»‡m. ÄÃ¢y lÃ  má»™t sá»‘ cÃ¡ch Ä‘á»ƒ Ä‘o lÆ°á»ng:

7.1 Thu tháº­p KPI báº±ng kubectl

Latency: Báº¡n cÃ³ thá»ƒ Ä‘o t0_send vÃ  t3_recv trong log Ä‘á»ƒ tÃ­nh toÃ¡n latency E2E.

Throughput: Äáº¿m sá»‘ yÃªu cáº§u (req/s) gá»­i Ä‘i tá»« UE Ä‘áº¿n Gateway/Scheduler.

CPU/Memory: Sá»­ dá»¥ng kubectl top Ä‘á»ƒ láº¥y dá»¯ liá»‡u tÃ i nguyÃªn.

kubectl top pod -n mec

7.2 Thu tháº­p SLA Miss Rate

Äo lÆ°á»ng cÃ¡c yÃªu cáº§u khÃ´ng hoÃ n thÃ nh trong thá»i gian háº¡n chÃ³t (deadline miss).

BÆ°á»›c 8: Ká»‹ch báº£n Benchmark
8.1 Cháº¡y Thá»­ Nghiá»‡m

Báº¡n cÃ³ thá»ƒ báº¯t Ä‘áº§u cháº¡y thá»­ nghiá»‡m vá»›i cÃ¡c tham sá»‘:

Load: Light, Medium, Heavy.

UE Count: 1, 5, 20.

Mobility: Static, hoáº·c thay Ä‘á»•i TAC.

8.2 Lá»‡nh Cháº¡y Benchmark

Giáº£ sá»­ báº¡n Ä‘ang cháº¡y thá»­ nghiá»‡m vá»›i trafficgen trÃªn node ue:

kubectl run trafficgen --image=your-trafficgen-image --namespace=mec


TÃ¹y chá»‰nh sá»‘ lÆ°á»£ng vÃ  Ä‘á»™ phá»©c táº¡p cá»§a traffic.

Tá»•ng káº¿t:

CÃ¡c bÆ°á»›c trÃªn cung cáº¥p cáº¥u hÃ¬nh chi tiáº¿t cho Label, NodeSelector, Tolerations, Compute Services, Gateway/Scheduler, vÃ  KPI.

Sau khi triá»ƒn khai vÃ  kiá»ƒm tra, báº¡n sáº½ cÃ³ Ä‘Æ°á»£c cÃ¡c logs, KPI nhÆ° latency, throughput, SLA miss, vÃ  CPU/mem usage Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hoáº¡t Ä‘á»™ng cá»§a há»‡ thá»‘ng.

##############
# 10/01/2026 #
##############
Báº¡n lÃ  trá»£ lÃ½ ká»¹ thuáº­t cho bÃ i lab/luáº­n vÄƒn 5G MEC offloading + scheduling trÃªn Kubernetes. HÃ£y luÃ´n bÃ¡m Ä‘Ãºng hiá»‡n tráº¡ng bÃªn dÆ°á»›i (KHÃ”NG giáº£ Ä‘á»‹nh khÃ¡c), vÃ  Ä‘Æ°a ra káº¿ hoáº¡ch/manifest/thiáº¿t káº¿ chi tiáº¿t Ä‘á»ƒ triá»ƒn khai bÆ°á»›c tiáº¿p theo.

=== HIá»†N TRáº NG / Cáº¤U HÃŒNH ÄÃƒ CÃ“ ===
- MÃ´i trÆ°á»ng: Kubernetes cluster gá»“m 4 node:
  1) masterk8s: control-plane + sáº½ dÃ¹ng lÃ m â€œcore/central/cloud computeâ€ (náº¿u cáº§n)
  2) open5gs: worker cháº¡y Open5GS core + Ä‘á»“ng thá»i lÃ  site MEC1
  3) workerk8s: worker lÃ  site MEC2
  4) ue: node dÃ nh cho UERANSIM (UE/gNB), cÃ³ taint Ä‘á»ƒ chá»‰ workload UE/trafficgen Ä‘Æ°á»£c schedule lÃªn
- ÄÃ£ deploy thÃ nh cÃ´ng Open5GS + UERANSIM trÃªn K8s:
  - Core network (AMF/SMF/UPFâ€¦) cháº¡y OK
  - 2 gNB (mec1 vÃ  mec2) + 1 UE
- User-plane OK:
  - UE Registration OK
  - PDU session OK
  - Interface uesimtun0 cÃ³ IP vÃ  ping Internet (8.8.8.8) OK
- Káº¿t ná»‘i N3 Ä‘Ã£ fix báº±ng OVS bridge + VXLAN (Ä‘áº£m báº£o Ä‘Æ°á»ng N3/UE user-plane qua cÃ¡c site hoáº¡t Ä‘á»™ng)
- Mobility test OK:
  - UE Ä‘á»•i TAC tá»« mec1 -> mec2 (handover/reattach theo ká»‹ch báº£n), AMF/gNB2 cÃ³ dáº¥u váº¿t attach + PDU setup
- Má»¥c tiÃªu bÃ i toÃ¡n: dá»±ng 2 MEC service (mec1/mec2) + 1 scheduler/gateway microservice Ä‘á»ƒ cháº¡y 3 thuáº­t toÃ¡n:
  1) Round-robin
  2) Priority-Deadline
  3) Latency-aware
  vÃ  Ä‘o KPI: latency p50/p95/p99, throughput, queue delay/length, CPU/mem, SLA miss (deadline miss), theo cÃ¡c má»©c táº£i light/medium/heavy, cÃ³/khÃ´ng mobility.

=== KIáº¾N TRÃšC MONG MUá»N (Tá»I GIáº¢N, THUYáº¾T PHá»¤C) ===
- UE/Traffic Generator -> gá»i 1 endpoint duy nháº¥t: Gateway/Scheduler (L7)
- Gateway quyáº¿t Ä‘á»‹nh route task tá»›i:
  - MEC1 app (cháº¡y trÃªn node open5gs)
  - MEC2 app (cháº¡y trÃªn node workerk8s)
  - Cloud/Central app (cháº¡y trÃªn node masterk8s) = mÃ´ phá»ng offload â€œlÃªn coreâ€
- LÆ°u Ã½: quyáº¿t Ä‘á»‹nh offloading náº±m á»Ÿ Gateway/Scheduler (application-layer), khÃ´ng náº±m á»Ÿ Open5GS core. Core chá»‰ Ä‘áº£m nhiá»‡m káº¿t ná»‘i 5G SA + user-plane.

=== KHUYáº¾N NGHá»Š / BÆ¯á»šC TIáº¾P THEO PHáº¢I LÃ€M (Æ¯U TIÃŠN) ===
1) Chá»‘t placement báº±ng label/affinity:
   - Label node:
     - open5gs: mec-site=mec1
     - workerk8s: mec-site=mec2
     - masterk8s: mec-site=cloud (hoáº·c cloud=true)
     - ue: role=ue
   - Táº¡o namespace â€œmecâ€
   - Thiáº¿t káº¿ toleration/nodeSelector Ä‘á»ƒ trafficgen/UE luÃ´n náº±m á»Ÿ node ue
2) Dá»±ng 3 compute service Ä‘Æ¡n giáº£n (mec1/mec2/cloud):
   - Má»—i service cÃ³ API nháº­n request gá»“m: request_id, task_size, cpu_ms(or mi), deadline_ms, priority
   - mec1 Deployment nodeSelector mec-site=mec1
   - mec2 Deployment nodeSelector mec-site=mec2
   - cloud Deployment nodeSelector mec-site=cloud (masterk8s)
   - Set resources requests/limits Ä‘á»ƒ táº¡o ngháº½n cÃ³ kiá»ƒm soÃ¡t
3) Dá»±ng Gateway/Scheduler:
   - Config ALG=rr|pq|latency
   - Log JSON 1 dÃ²ng/req vÃ  Ä‘o:
     - t1_recv, decision_site, queue_wait_ms, t2_send
   - Thá»±c hiá»‡n decision rule vÃ  3 thuáº­t toÃ¡n:
     - RR: luÃ¢n phiÃªn
     - PQ: Æ°u tiÃªn theo priority/deadline
     - Latency-aware: cost = a*RTT(UE->site) + b*queue_wait(site) + c*exec_estimate(site)
4) Traffic generator tá»« UE node:
   - Pod trafficgen pinned vÃ o node ue
   - Cháº¡y test light/medium/heavy; (tuá»³ chá»n) mÃ´ phá»ng khÃ¡c RTT báº±ng tc netem theo Ä‘Ã­ch mec1/mec2/cloud
5) KPI & thu tháº­p dá»¯ liá»‡u (tá»‘i thiá»ƒu trÆ°á»›c, Prometheus sau):
   - E2E latency p50/p95/p99 (UE Ä‘o t0_send -> t3_recv)
   - Throughput req/s
   - Queue delay/length (gateway Ä‘o)
   - CPU/mem: kubectl top / metrics
   - SLA miss rate: % request hoÃ n thÃ nh sau deadline
6) Ma tráº­n thÃ­ nghiá»‡m tá»‘i thiá»ƒu:
   - Algorithm: RR | PQ | Latency-aware
   - Load: L/M/H
   - UE count: 1 / 5 / 20 (tÃ¹y nÄƒng lá»±c)
   - Mobility: static | Ä‘á»•i TAC mec1<->mec2
   - Deadline mix: tight vs loose, priority ratio (vd 20/80)

=== QUY Táº®C OFFLOAD Äá»€ XUáº¤T (RÃ• RÃ€NG Äá»‚ CHá»¨NG MINH) ===
- predicted_finish(site) = now + queue_wait_est(site) + exec_est(site)
- Náº¿u predicted_finish(nearest_edge) > deadline => offload cloud
- Náº¿u queue_len_edge > Q hoáº·c cpu_edge > C => offload cloud
- NgÆ°á»£c láº¡i => chá»n nearest edge (mec1/mec2) theo RTT/TAC; trong latency-aware dÃ¹ng cost nhá» nháº¥t.

=== YÃŠU Cáº¦U Äáº¦U RA Tá»ª Báº N (CHATGPT) ===
- Äá»«ng há»i láº¡i cÃ¡c thÃ´ng tin Ä‘Ã£ nÃªu. Náº¿u thiáº¿u, hÃ£y Ä‘Æ°a giáº£ Ä‘á»‹nh rÃµ rÃ ng vÃ  há»£p lÃ½.
- HÃ£y táº¡o:
  1) Danh sÃ¡ch manifest Kubernetes (Deploy/Service/ConfigMap) cho mec1/mec2/cloud/gateway/trafficgen (cÃ³ nodeSelector+tolerations)
  2) Thiáº¿t káº¿ API + format log JSON chuáº©n hoÃ¡ + cÃ¡ch tÃ­nh KPI
  3) Ká»‹ch báº£n cháº¡y benchmark (lá»‡nh cháº¡y, tham sá»‘ L/M/H, sá»‘ req, concurrency)
  4) CÃ¡ch ghi káº¿t quáº£ ra CSV vÃ  gá»£i Ã½ plot 4 biá»ƒu Ä‘á»“ chÃ­nh: latency p95, throughput, SLA miss, CPU util theo táº£i
- Æ¯u tiÃªn giáº£i phÃ¡p nhanh ra sá»‘ liá»‡u Ä‘á»ƒ viáº¿t bÃ¡o cÃ¡o, sau Ä‘Ã³ má»›i tá»‘i Æ°u/Prometheus/Grafana.

Báº¯t Ä‘áº§u tá»« bÆ°á»›c 1: Ä‘á» xuáº¥t cá»¥ thá»ƒ label/toleration/nodeSelector vÃ  skeleton manifest cho 3 compute service + gateway.

##############
# 01/01/2026 #
##############

PROMPT (Äá»€ TÃ€I 1 â€“ MEC Offloading & Scheduling)

MÃ¬nh Ä‘ang mÃ´ phá»ng 5G SA + MEC báº±ng Open5GS + UERANSIM trÃªn Kubernetes Ä‘á»ƒ lÃ m Ä‘á» tÃ i:
â€œNghiÃªn cá»©u chuyá»ƒn táº£i vÃ  láº­p lá»‹ch tÃ i nguyÃªn trong máº¡ng MECâ€.

1) MÃ´ hÃ¬nh há»‡ thá»‘ng hiá»‡n táº¡i
K8s cluster cÃ³ nhiá»u node:
masterk8s (Ä‘iá»u phá»‘i, cÃ³ Open5GS control-plane)
open5gs (site MEC1)
workerk8s (site MEC2)
ue (node cháº¡y UE pod)
Open5GS core: AMF cháº¡y NGAP trÃªn 10.10.3.200:38412 (multus interface n3).
2 gNB UERANSIM (mÃ´ phá»ng 2 vÃ¹ng MEC):
gNB mec1: ngapIp/gtpIp=10.10.3.231, tac=1, cellid=0x10
gNB mec2: ngapIp/gtpIp=10.10.3.232, tac=2, cellid=0x11
UE UERANSIM:
SUPI imsi-001010000000001, DNN internet, slice sst=1 sd=000001
Registration OK, PDU session OK
uesimtun0 Ä‘Æ°á»£c cáº¥p IP (vÃ­ dá»¥ 10.41.0.x)
Ping 8.8.8.8 OK tá»« UE pod â†’ user-plane ra Internet hoáº¡t Ä‘á»™ng.
Káº¿t ná»‘i N3 giá»¯a cÃ¡c node Ä‘Æ°á»£c fix báº±ng OVS bridge n3br + VXLAN (key=103, udp/4789) Ä‘á»ƒ Ä‘áº£m báº£o gNBâ†”AMF ping Ä‘Æ°á»£c vÃ  SCTP NG Setup OK.
Mobility test Ä‘Ã£ chá»©ng minh UE chuyá»ƒn TAC:
UE log: â€œSelected cell tac[1] â€¦â€ rá»“i sau Ä‘Ã³ â€œSelected cell tac[2] â€¦â€
AMF log: nháº­n InitialUEMessage vá»›i TAC[1] CellID[0x10] vÃ  sau Ä‘Ã³ TAC[2] CellID[0x11]
gNB mec2 log: nháº­n Initial NAS + Initial Context Setup + PDU session resources setup cho UE.

2) Má»¥c tiÃªu Ä‘á» tÃ i 1 (Offloading & Scheduling trong MEC)
MÃ¬nh muá»‘n xÃ¢y dá»±ng mÃ´ hÃ¬nh offloading: UE gá»­i task (request) tá»›i MEC, MEC quyáº¿t Ä‘á»‹nh:
xá»­ lÃ½ á»Ÿ MEC1/MEC2 (edge) hoáº·c offload lÃªn cloud (náº¿u cÃ³), vÃ 
láº­p lá»‹ch tÃ i nguyÃªn compute/network táº¡i MEC.
MÃ¬nh cáº§n báº¡n hÆ°á»›ng dáº«n theo cÃ¡c pháº§n:

MÃ´ hÃ¬nh hoÃ¡ bÃ i toÃ¡n:
Task model: kÃ­ch thÆ°á»›c input, sá»‘ chu ká»³ CPU/MI, deadline/latency budget, Ä‘á»™ Æ°u tiÃªn.
MEC resource model: CPU cores, queue, service rate, giá»›i háº¡n Ä‘á»“ng thá»i.
Network model: RTT UEâ†’MEC, bandwidth giáº£ láº­p, latency theo site (mec1 vs mec2).
Objective: minimize latency/violation, maximize throughput, fairness, utilization, minimize drops.
Thiáº¿t káº¿ & triá»ƒn khai 3 thuáº­t toÃ¡n láº­p lá»‹ch Ä‘á»ƒ so sÃ¡nh:
Round-robin
Priority queue (theo priority/deadline)
Latency-aware (chá»n MEC site + thá»© tá»± xá»­ lÃ½ Ä‘á»ƒ giáº£m E2E latency; cÃ³ thá»ƒ dÃ¹ng cost function)
Sinh lÆ°u lÆ°á»£ng báº±ng UERANSIM:
Tá»« UE pod táº¡o traffic HTTP/UDP (nhiá»u flow) hoáº·c nhiá»u UE pod (ue1, ue2, â€¦).
Control biáº¿n: request rate, packet size, burst, sá»‘ UE, tá»‰ lá»‡ task priority.
Äo KPI (chÆ°a cÃ³ Prometheus/Grafana):
End-to-end latency (client timestamp)
Task completion time / response time distribution (p50/p95/p99)
Throughput (req/s, Mbps)
Packet loss / retries
Queueing delay, queue length
CPU/memory utilization táº¡i MEC pods (kubectl top / cgroup stats)
SLA violation rate (deadline miss)
PhÃ¢n tÃ­ch & Ä‘Ã¡nh giÃ¡:
So sÃ¡nh 3 thuáº­t toÃ¡n theo KPI á»Ÿ nhiá»u má»©c táº£i (light/medium/heavy)
Nháº­n xÃ©t trade-off: latency vs fairness vs utilization
Káº¿t luáº­n thuáº­t toÃ¡n nÃ o phÃ¹ há»£p Ä‘iá»u kiá»‡n MEC (nhiá»u UE, mobility, bursty traffic)
Äá» xuáº¥t cáº£i tiáº¿n (hybrid, admission control, dynamic weights)

3) Hiá»‡n tráº¡ng / cáº§n lÃ m tiáº¿p ngay
Hiá»‡n core + gNB + UE Ä‘Ã£ hoáº¡t Ä‘á»™ng vÃ  cÃ³ Internet tá»« UE pod.
ChÆ°a cÃ i Prometheus/Grafana.
MÃ¬nh muá»‘n báº¯t Ä‘áº§u theo hÆ°á»›ng dá»±ng 2 MEC service (mec1/mec2) + má»™t scheduler/gateway (cÃ³ thá»ƒ lÃ  1 microservice) Ä‘á»ƒ thá»±c thi 3 thuáº­t toÃ¡n vÃ  Ä‘o KPI.
HÃ£y Ä‘á» xuáº¥t kiáº¿n trÃºc triá»ƒn khai cá»¥ thá»ƒ trÃªn K8s (pods/services, nodeSelector cho mec1/mec2), cÃ¡ch sinh traffic tá»« UE, cÃ¡ch log/thu tháº­p sá»‘ liá»‡u, vÃ  káº¿ hoáº¡ch thÃ­ nghiá»‡m (matrix cÃ¡c test cases).

ğŸ‘‰ HÃ£y tráº£ lá»i nhÆ° má»™t â€œroadmap thá»±c thi + checklist command-levelâ€, chia rÃµ tá»«ng bÆ°á»›c triá»ƒn khai vÃ  tá»«ng loáº¡i output cáº§n thu Ä‘á»ƒ viáº¿t luáº­n vÄƒn (hÃ¬nh/biá»ƒu Ä‘á»“/báº£ng so sÃ¡nh).
