##############
# 20/01/2026 #
##############
Sau khi Ä‘Ã£ cáº¥u hÃ¬nh K8s + MEC + Scheduler nhÆ° trong file, lÃ m sao táº¡o workload (cpu_i, deadline_i, priority_i), cho há»‡ thá»‘ng cháº¡y tháº­t, thu káº¿t quáº£, váº½ biá»ƒu Ä‘á»“ / dashboard?
TÃ´i tráº£ lá»i theo Ä‘Ãºng hiá»‡n tráº¡ng file anh gá»­i, khÃ´ng giáº£ Ä‘á»‹nh thÃªm 

Tá»”NG QUAN LUá»’NG THá»°C NGHIá»†M (END-TO-END)
TrafficGen (UE)
   â†“  (JSON task)
Gateway / Scheduler
   â†“  (decision)
MEC1 / MEC2 / Cloud
   â†“  (process)
Logs + Metrics
   â†“
CSV / Prometheus
   â†“
Python / Grafana

1. Táº O cpu_i, deadline_i, priority_i Tá»ª ÄÃ‚U?

ğŸ‘‰ KHÃ”NG táº¡o trong Kubernetes
ğŸ‘‰ Táº O Tá»ª TRAFFICGEN (UE side)

1.1 Task model (chuáº©n cho luáº­n vÄƒn)

Má»—i request = 1 task i

{
  "request_id": "uuid",
  "cpu_ms": 50,
  "deadline_ms": 120,
  "priority": 2,
  "task_size": 1024
}

Biáº¿n	Ã nghÄ©a	Ghi chÃº há»c thuáº­t
cpu_ms	cpu_i	workload
deadline_ms	d_i	SLA
priority	p_i	QoS class
task_size	input size	optional
2. VIáº¾T TRAFFICGEN (CÃCH ÄÆ N GIáº¢N â€“ HIá»†U QUáº¢)
2.1 Logic TrafficGen (pseudo-code)
import random, time, requests, uuid

while True:
    task = {
        "request_id": str(uuid.uuid4()),
        "cpu_ms": random.choice([20, 50, 100, 200]),
        "deadline_ms": random.choice([80, 120, 200]),
        "priority": random.choice([1, 2, 3]),
        "task_size": random.choice([512, 1024])
    }

    requests.post("http://gateway-scheduler/api/task", json=task)
    time.sleep(0.05)  # load control

2.2 Mapping load level (Ráº¤T QUAN TRá»ŒNG)
Load	sleep	req/s
Light	0.2s	~5
Medium	0.05s	~20
Heavy	0.01s	~100

ğŸ“Œ Chá»‰ Ä‘á»•i sleep lÃ  Ä‘á»•i ká»‹ch báº£n thÃ­ nghiá»‡m â†’ há»™i Ä‘á»“ng ráº¥t thÃ­ch

3. SCHEDULER Xá»¬ LÃ TASK NHÆ¯ THáº¾ NÃ€O?
3.1 Round Robin (RR)
site = sites[next_index]

3.2 Priorityâ€“Deadline
score = alpha * priority - beta * deadline
site = argmax(score)

3.3 Latency-aware
site = argmin(
    estimated_queue_delay + network_latency
)


ğŸ‘‰ Anh KHÃ”NG cáº§n tá»‘i Æ°u toÃ¡n há»c cao siÃªu
ğŸ‘‰ Quan trá»ng lÃ  so sÃ¡nh káº¿t quáº£

4. MEC SERVICE â€œGIáº¢ Láº¬P CPUâ€ RA SAO?
4.1 CÃ¡ch mÃ´ phá»ng CPU (Ä‘á»§ cho tháº¡c sÄ©)
import time

def process(cpu_ms):
    start = time.time()
    while (time.time() - start) * 1000 < cpu_ms:
        pass


ğŸ“Œ KhÃ´ng cáº§n cháº¡y task tháº­t
ğŸ“Œ CÃ¡i anh cáº§n lÃ  Ä‘á»™ trá»… & tranh cháº¥p tÃ i nguyÃªn

5. LOG PHáº¢I GHI NHÆ¯ THáº¾ NÃ€O? (Ráº¤T QUAN TRá»ŒNG)
5.1 Log chuáº©n (JSON â€“ 1 dÃ²ng / task)
{
  "request_id": "...",
  "alg": "rr",
  "priority": 2,
  "deadline_ms": 120,
  "site": "mec1",
  "queue_wait_ms": 18,
  "exec_ms": 50,
  "e2e_latency_ms": 92,
  "sla_miss": false,
  "timestamp": 1700000000
}


ğŸ“Œ KHÃ”NG log text lung tung
ğŸ“Œ Há»™i Ä‘á»“ng cá»±c ká»³ thÃ­ch log JSON

6. THU KPI NHÆ¯ THáº¾ NÃ€O?
6.1 CÃ¡ch 1 â€“ Äá»¦ & NHANH (Python + CSV) âœ…
kubectl logs gateway-scheduler > logs.json


Parse báº±ng Python:

import pandas as pd
df = pd.read_json("logs.json", lines=True)

df["e2e_latency_ms"].quantile([0.5, 0.95, 0.99])
df["sla_miss"].mean()


ğŸ‘‰ Äá»¦ Ä‘á»ƒ tá»‘t nghiá»‡p

6.2 CÃ¡ch 2 â€“ PRO (Prometheus + Grafana) â­

Náº¿u muá»‘n dashboard Ä‘áº¹p:

Gateway expose /metrics

DÃ¹ng:

latency_bucket

sla_miss_total

Grafana váº½:

p50 / p95 / p99

SLA miss %

CPU per MEC site

ğŸ“Œ KhÃ´ng báº¯t buá»™c, nhÆ°ng ráº¥t â€œÄƒn Ä‘iá»ƒmâ€

7. BIá»‚U Äá»’ NÃ€O NÃŠN Váº¼? (KHÃ”NG Váº¼ THá»ªA)
Biá»ƒu Ä‘á»“	Má»¥c Ä‘Ã­ch
Latency p95 vs Load	So sÃ¡nh thuáº­t toÃ¡n
SLA miss vs Load	Chá»©ng minh priority
Throughput vs Load	ÄÃ¡nh Ä‘á»•i
Queue delay per site	Hiá»‡u quáº£ offloading
8. Lá»˜ TRÃŒNH THá»°C NGHIá»†M CHUáº¨N (Gá»¢I Ã)

Fix RR â†’ cháº¡y á»•n

Add Priorityâ€“Deadline

Add Latency-aware

Má»—i thuáº­t toÃ¡n:

Light / Medium / Heavy

CÃ³ / khÃ´ng mobility

Má»—i case cháº¡y â‰¥ 5 phÃºt

ğŸ‘‰ ÄÃ¢y lÃ  ChÆ°Æ¡ng 5 luáº­n vÄƒn hoÃ n chá»‰nh

9. TÃ“M Láº I â€“ ANH ÄANG á» ÄÃšNG CHá»–

File hiá»‡n táº¡i = xÆ°Æ¡ng sá»‘ng thá»±c nghiá»‡m

Viá»‡c anh cáº§n lÃ m NGAY BÃ‚Y GIá»œ:

Viáº¿t trafficgen

Chuáº©n hÃ³a log

Cháº¡y load

Xuáº¥t CSV

Váº½ biá»ƒu Ä‘á»“

ğŸ‘‰ Náº¿u anh muá»‘n, bÆ°á»›c tiáº¿p theo tÃ´i cÃ³ thá»ƒ:

Viáº¿t trafficgen hoÃ n chá»‰nh

Viáº¿t scheduler skeleton (RR / Priority / Latency)

Viáº¿t script parse log + váº½ biá»ƒu Ä‘á»“ (Python)


##############
# 13/01/2026 #
##############
BÆ°á»›c 1: Cáº¥u hÃ¬nh Label vÃ  Toleration
1.1 Label cÃ¡c Nodes

TrÆ°á»›c tiÃªn, báº¡n sáº½ cáº§n gÃ¡n label cho cÃ¡c nodes Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng cÃ¡c dá»‹ch vá»¥ sáº½ Ä‘Æ°á»£c phÃ¢n phá»‘i Ä‘Ãºng node.

kubectl label nodes open5gs mec-site=mec1
kubectl label nodes workerk8s mec-site=mec2
kubectl label nodes masterk8s mec-site=cloud
kubectl label nodes ue role=ue


Output Kiá»ƒm Tra:
Äá»ƒ kiá»ƒm tra xem cÃ¡c label Ä‘Ã£ Ä‘Æ°á»£c Ã¡p dá»¥ng chÃ­nh xÃ¡c hay chÆ°a, báº¡n cÃ³ thá»ƒ dÃ¹ng lá»‡nh sau:

kubectl get nodes --show-labels


Má»—i node sáº½ cÃ³ label tÆ°Æ¡ng á»©ng nhÆ° mec-site=mec1, mec-site=mec2, v.v.

BÆ°á»›c 2: Táº¡o Namespace â€œmecâ€

Táº¡o má»™t namespace Ä‘á»ƒ chá»©a táº¥t cáº£ cÃ¡c dá»‹ch vá»¥ liÃªn quan Ä‘áº¿n MEC.

2.1 Táº¡o Namespace
apiVersion: v1
kind: Namespace
metadata:
  name: mec


Output Kiá»ƒm Tra:

kubectl get namespaces


Namespace mec sáº½ xuáº¥t hiá»‡n trong danh sÃ¡ch.

BÆ°á»›c 3: Cáº¥u hÃ¬nh Toleration vÃ  NodeSelector cho TrafficGen/UE

Táº¡o cáº¥u hÃ¬nh Ä‘á»ƒ Ä‘áº£m báº£o TrafficGen/UE chá»‰ Ä‘Æ°á»£c cháº¡y trÃªn node ue.

3.1 Pod TrafficGen vá»›i Toleration vÃ  NodeSelector
apiVersion: v1
kind: Pod
metadata:
  name: trafficgen
  namespace: mec
spec:
  tolerations:
  - key: "role"
    operator: "Equal"
    value: "ue"
    effect: "NoSchedule"
  nodeSelector:
    role: "ue"
  containers:
    - name: trafficgen
      image: your-trafficgen-image
      resources:
        requests:
          memory: "500Mi"
          cpu: "500m"
        limits:
          memory: "1Gi"
          cpu: "1"


Output Kiá»ƒm Tra:
Kiá»ƒm tra xem pod Ä‘Ã£ Ä‘Æ°á»£c schedule Ä‘Ãºng vÃ o node ue:

kubectl get pods -o wide


Pod trafficgen sáº½ Ä‘Æ°á»£c gÃ¡n vÃ o node ue.

BÆ°á»›c 4: Táº¡o vÃ  Cáº¥u hÃ¬nh Compute Services (mec1, mec2, cloud)
4.1 Manifest cho MEC1, MEC2, Cloud

Táº¡o cÃ¡c Deployment vÃ  Service cho cÃ¡c compute sites MEC1, MEC2 vÃ  Cloud.

MEC1 Deployment:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mec1
  namespace: mec
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mec1
  template:
    metadata:
      labels:
        app: mec1
    spec:
      nodeSelector:
        mec-site: mec1
      containers:
        - name: mec1
          image: your-mec1-image
          resources:
            requests:
              memory: "500Mi"
              cpu: "500m"
            limits:
              memory: "1Gi"
              cpu: "1"

MEC2 Deployment:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mec2
  namespace: mec
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mec2
  template:
    metadata:
      labels:
        app: mec2
    spec:
      nodeSelector:
        mec-site: mec2
      containers:
        - name: mec2
          image: your-mec2-image
          resources:
            requests:
              memory: "500Mi"
              cpu: "500m"
            limits:
              memory: "1Gi"
              cpu: "1"

Cloud Deployment:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cloud
  namespace: mec
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cloud
  template:
    metadata:
      labels:
        app: cloud
    spec:
      nodeSelector:
        mec-site: cloud
      containers:
        - name: cloud
          image: your-cloud-image
          resources:
            requests:
              memory: "1Gi"
              cpu: "1"
            limits:
              memory: "2Gi"
              cpu: "2"


Output Kiá»ƒm Tra:
Kiá»ƒm tra pod Ä‘Ã£ Ä‘Æ°á»£c deploy vÃ  schedule vÃ o Ä‘Ãºng node:

kubectl get pods -o wide


CÃ¡c pod cá»§a mec1, mec2, vÃ  cloud sáº½ xuáº¥t hiá»‡n trÃªn cÃ¡c node open5gs, workerk8s vÃ  masterk8s tÆ°Æ¡ng á»©ng.

BÆ°á»›c 5: Cáº¥u hÃ¬nh Gateway/Scheduler

Cáº¥u hÃ¬nh dá»‹ch vá»¥ Gateway/Scheduler Ä‘á»ƒ quyáº¿t Ä‘á»‹nh offloading vá»›i 3 thuáº­t toÃ¡n:

5.1 Gateway/Scheduler Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gateway-scheduler
  namespace: mec
spec:
  replicas: 1
  selector:
    matchLabels:
      app: gateway-scheduler
  template:
    metadata:
      labels:
        app: gateway-scheduler
    spec:
      containers:
        - name: gateway-scheduler
          image: your-gateway-image
          env:
            - name: ALG
              value: "rr"  # cÃ³ thá»ƒ lÃ  rr | pq | latency
          resources:
            requests:
              memory: "500Mi"
              cpu: "500m"
            limits:
              memory: "1Gi"
              cpu: "1"


Output Kiá»ƒm Tra:
Kiá»ƒm tra pod cá»§a Gateway/Scheduler Ä‘Ã£ cháº¡y chÆ°a:

kubectl get pods -o wide


Pod gateway-scheduler sáº½ xuáº¥t hiá»‡n.

BÆ°á»›c 6: Kiá»ƒm tra API vÃ  Format Log

API cá»§a Gateway/Scheduler cáº§n pháº£i nháº­n yÃªu cáº§u vá»›i cÃ¡c tham sá»‘ nhÆ° request_id, task_size, cpu_ms, deadline_ms, priority vÃ  tÃ­nh toÃ¡n viá»‡c offloading tá»›i cÃ¡c site MEC hoáº·c Cloud.

6.1 API Format (JSON)
{
  "request_id": "12345",
  "task_size": 1024,
  "cpu_ms": 100,
  "deadline_ms": 200,
  "priority": 1
}

6.2 Log Format (JSON)
{
  "request_id": "12345",
  "t1_recv": 1611243156,
  "decision_site": "mec1",
  "queue_wait_ms": 10,
  "t2_send": 1611243170
}


Output Kiá»ƒm Tra:
Khi API Ä‘Æ°á»£c gá»i, log sáº½ Ä‘Æ°á»£c ghi láº¡i vÃ  cÃ³ thá»ƒ kiá»ƒm tra thÃ´ng qua kubectl logs.

kubectl logs <gateway-scheduler-pod>


Kiá»ƒm tra cÃ¡c logs Ä‘á»ƒ xÃ¡c minh ráº±ng quÃ¡ trÃ¬nh xá»­ lÃ½ yÃªu cáº§u vÃ  quyáº¿t Ä‘á»‹nh offloading Ä‘ang diá»…n ra Ä‘Ãºng cÃ¡ch.

BÆ°á»›c 7: KPI vÃ  Thu tháº­p Dá»¯ liá»‡u

KPI cáº§n Ä‘Æ°á»£c thu tháº­p trong suá»‘t cÃ¡c thá»­ nghiá»‡m. ÄÃ¢y lÃ  má»™t sá»‘ cÃ¡ch Ä‘á»ƒ Ä‘o lÆ°á»ng:

7.1 Thu tháº­p KPI báº±ng kubectl

Latency: Báº¡n cÃ³ thá»ƒ Ä‘o t0_send vÃ  t3_recv trong log Ä‘á»ƒ tÃ­nh toÃ¡n latency E2E.

Throughput: Äáº¿m sá»‘ yÃªu cáº§u (req/s) gá»­i Ä‘i tá»« UE Ä‘áº¿n Gateway/Scheduler.

CPU/Memory: Sá»­ dá»¥ng kubectl top Ä‘á»ƒ láº¥y dá»¯ liá»‡u tÃ i nguyÃªn.

kubectl top pod -n mec

7.2 Thu tháº­p SLA Miss Rate

Äo lÆ°á»ng cÃ¡c yÃªu cáº§u khÃ´ng hoÃ n thÃ nh trong thá»i gian háº¡n chÃ³t (deadline miss).

BÆ°á»›c 8: Ká»‹ch báº£n Benchmark
8.1 Cháº¡y Thá»­ Nghiá»‡m

Báº¡n cÃ³ thá»ƒ báº¯t Ä‘áº§u cháº¡y thá»­ nghiá»‡m vá»›i cÃ¡c tham sá»‘:

Load: Light, Medium, Heavy.

UE Count: 1, 5, 20.

Mobility: Static, hoáº·c thay Ä‘á»•i TAC.

8.2 Lá»‡nh Cháº¡y Benchmark

Giáº£ sá»­ báº¡n Ä‘ang cháº¡y thá»­ nghiá»‡m vá»›i trafficgen trÃªn node ue:

kubectl run trafficgen --image=your-trafficgen-image --namespace=mec


TÃ¹y chá»‰nh sá»‘ lÆ°á»£ng vÃ  Ä‘á»™ phá»©c táº¡p cá»§a traffic.

Tá»•ng káº¿t:

CÃ¡c bÆ°á»›c trÃªn cung cáº¥p cáº¥u hÃ¬nh chi tiáº¿t cho Label, NodeSelector, Tolerations, Compute Services, Gateway/Scheduler, vÃ  KPI.

Sau khi triá»ƒn khai vÃ  kiá»ƒm tra, báº¡n sáº½ cÃ³ Ä‘Æ°á»£c cÃ¡c logs, KPI nhÆ° latency, throughput, SLA miss, vÃ  CPU/mem usage Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hoáº¡t Ä‘á»™ng cá»§a há»‡ thá»‘ng.

##############
# 10/01/2026 #
##############
Báº¡n lÃ  trá»£ lÃ½ ká»¹ thuáº­t cho bÃ i lab/luáº­n vÄƒn 5G MEC offloading + scheduling trÃªn Kubernetes. HÃ£y luÃ´n bÃ¡m Ä‘Ãºng hiá»‡n tráº¡ng bÃªn dÆ°á»›i (KHÃ”NG giáº£ Ä‘á»‹nh khÃ¡c), vÃ  Ä‘Æ°a ra káº¿ hoáº¡ch/manifest/thiáº¿t káº¿ chi tiáº¿t Ä‘á»ƒ triá»ƒn khai bÆ°á»›c tiáº¿p theo.

=== HIá»†N TRáº NG / Cáº¤U HÃŒNH ÄÃƒ CÃ“ ===
- MÃ´i trÆ°á»ng: Kubernetes cluster gá»“m 4 node:
  1) masterk8s: control-plane + sáº½ dÃ¹ng lÃ m â€œcore/central/cloud computeâ€ (náº¿u cáº§n)
  2) open5gs: worker cháº¡y Open5GS core + Ä‘á»“ng thá»i lÃ  site MEC1
  3) workerk8s: worker lÃ  site MEC2
  4) ue: node dÃ nh cho UERANSIM (UE/gNB), cÃ³ taint Ä‘á»ƒ chá»‰ workload UE/trafficgen Ä‘Æ°á»£c schedule lÃªn
- ÄÃ£ deploy thÃ nh cÃ´ng Open5GS + UERANSIM trÃªn K8s:
  - Core network (AMF/SMF/UPFâ€¦) cháº¡y OK
  - 2 gNB (mec1 vÃ  mec2) + 1 UE
- User-plane OK:
  - UE Registration OK
  - PDU session OK
  - Interface uesimtun0 cÃ³ IP vÃ  ping Internet (8.8.8.8) OK
- Káº¿t ná»‘i N3 Ä‘Ã£ fix báº±ng OVS bridge + VXLAN (Ä‘áº£m báº£o Ä‘Æ°á»ng N3/UE user-plane qua cÃ¡c site hoáº¡t Ä‘á»™ng)
- Mobility test OK:
  - UE Ä‘á»•i TAC tá»« mec1 -> mec2 (handover/reattach theo ká»‹ch báº£n), AMF/gNB2 cÃ³ dáº¥u váº¿t attach + PDU setup
- Má»¥c tiÃªu bÃ i toÃ¡n: dá»±ng 2 MEC service (mec1/mec2) + 1 scheduler/gateway microservice Ä‘á»ƒ cháº¡y 3 thuáº­t toÃ¡n:
  1) Round-robin
  2) Priority-Deadline
  3) Latency-aware
  vÃ  Ä‘o KPI: latency p50/p95/p99, throughput, queue delay/length, CPU/mem, SLA miss (deadline miss), theo cÃ¡c má»©c táº£i light/medium/heavy, cÃ³/khÃ´ng mobility.

=== KIáº¾N TRÃšC MONG MUá»N (Tá»I GIáº¢N, THUYáº¾T PHá»¤C) ===
- UE/Traffic Generator -> gá»i 1 endpoint duy nháº¥t: Gateway/Scheduler (L7)
- Gateway quyáº¿t Ä‘á»‹nh route task tá»›i:
  - MEC1 app (cháº¡y trÃªn node open5gs)
  - MEC2 app (cháº¡y trÃªn node workerk8s)
  - Cloud/Central app (cháº¡y trÃªn node masterk8s) = mÃ´ phá»ng offload â€œlÃªn coreâ€
- LÆ°u Ã½: quyáº¿t Ä‘á»‹nh offloading náº±m á»Ÿ Gateway/Scheduler (application-layer), khÃ´ng náº±m á»Ÿ Open5GS core. Core chá»‰ Ä‘áº£m nhiá»‡m káº¿t ná»‘i 5G SA + user-plane.

=== KHUYáº¾N NGHá»Š / BÆ¯á»šC TIáº¾P THEO PHáº¢I LÃ€M (Æ¯U TIÃŠN) ===
1) Chá»‘t placement báº±ng label/affinity:
   - Label node:
     - open5gs: mec-site=mec1
     - workerk8s: mec-site=mec2
     - masterk8s: mec-site=cloud (hoáº·c cloud=true)
     - ue: role=ue
   - Táº¡o namespace â€œmecâ€
   - Thiáº¿t káº¿ toleration/nodeSelector Ä‘á»ƒ trafficgen/UE luÃ´n náº±m á»Ÿ node ue
2) Dá»±ng 3 compute service Ä‘Æ¡n giáº£n (mec1/mec2/cloud):
   - Má»—i service cÃ³ API nháº­n request gá»“m: request_id, task_size, cpu_ms(or mi), deadline_ms, priority
   - mec1 Deployment nodeSelector mec-site=mec1
   - mec2 Deployment nodeSelector mec-site=mec2
   - cloud Deployment nodeSelector mec-site=cloud (masterk8s)
   - Set resources requests/limits Ä‘á»ƒ táº¡o ngháº½n cÃ³ kiá»ƒm soÃ¡t
3) Dá»±ng Gateway/Scheduler:
   - Config ALG=rr|pq|latency
   - Log JSON 1 dÃ²ng/req vÃ  Ä‘o:
     - t1_recv, decision_site, queue_wait_ms, t2_send
   - Thá»±c hiá»‡n decision rule vÃ  3 thuáº­t toÃ¡n:
     - RR: luÃ¢n phiÃªn
     - PQ: Æ°u tiÃªn theo priority/deadline
     - Latency-aware: cost = a*RTT(UE->site) + b*queue_wait(site) + c*exec_estimate(site)
4) Traffic generator tá»« UE node:
   - Pod trafficgen pinned vÃ o node ue
   - Cháº¡y test light/medium/heavy; (tuá»³ chá»n) mÃ´ phá»ng khÃ¡c RTT báº±ng tc netem theo Ä‘Ã­ch mec1/mec2/cloud
5) KPI & thu tháº­p dá»¯ liá»‡u (tá»‘i thiá»ƒu trÆ°á»›c, Prometheus sau):
   - E2E latency p50/p95/p99 (UE Ä‘o t0_send -> t3_recv)
   - Throughput req/s
   - Queue delay/length (gateway Ä‘o)
   - CPU/mem: kubectl top / metrics
   - SLA miss rate: % request hoÃ n thÃ nh sau deadline
6) Ma tráº­n thÃ­ nghiá»‡m tá»‘i thiá»ƒu:
   - Algorithm: RR | PQ | Latency-aware
   - Load: L/M/H
   - UE count: 1 / 5 / 20 (tÃ¹y nÄƒng lá»±c)
   - Mobility: static | Ä‘á»•i TAC mec1<->mec2
   - Deadline mix: tight vs loose, priority ratio (vd 20/80)

=== QUY Táº®C OFFLOAD Äá»€ XUáº¤T (RÃ• RÃ€NG Äá»‚ CHá»¨NG MINH) ===
- predicted_finish(site) = now + queue_wait_est(site) + exec_est(site)
- Náº¿u predicted_finish(nearest_edge) > deadline => offload cloud
- Náº¿u queue_len_edge > Q hoáº·c cpu_edge > C => offload cloud
- NgÆ°á»£c láº¡i => chá»n nearest edge (mec1/mec2) theo RTT/TAC; trong latency-aware dÃ¹ng cost nhá» nháº¥t.

=== YÃŠU Cáº¦U Äáº¦U RA Tá»ª Báº N (CHATGPT) ===
- Äá»«ng há»i láº¡i cÃ¡c thÃ´ng tin Ä‘Ã£ nÃªu. Náº¿u thiáº¿u, hÃ£y Ä‘Æ°a giáº£ Ä‘á»‹nh rÃµ rÃ ng vÃ  há»£p lÃ½.
- HÃ£y táº¡o:
  1) Danh sÃ¡ch manifest Kubernetes (Deploy/Service/ConfigMap) cho mec1/mec2/cloud/gateway/trafficgen (cÃ³ nodeSelector+tolerations)
  2) Thiáº¿t káº¿ API + format log JSON chuáº©n hoÃ¡ + cÃ¡ch tÃ­nh KPI
  3) Ká»‹ch báº£n cháº¡y benchmark (lá»‡nh cháº¡y, tham sá»‘ L/M/H, sá»‘ req, concurrency)
  4) CÃ¡ch ghi káº¿t quáº£ ra CSV vÃ  gá»£i Ã½ plot 4 biá»ƒu Ä‘á»“ chÃ­nh: latency p95, throughput, SLA miss, CPU util theo táº£i
- Æ¯u tiÃªn giáº£i phÃ¡p nhanh ra sá»‘ liá»‡u Ä‘á»ƒ viáº¿t bÃ¡o cÃ¡o, sau Ä‘Ã³ má»›i tá»‘i Æ°u/Prometheus/Grafana.

Báº¯t Ä‘áº§u tá»« bÆ°á»›c 1: Ä‘á» xuáº¥t cá»¥ thá»ƒ label/toleration/nodeSelector vÃ  skeleton manifest cho 3 compute service + gateway.

##############
# 01/01/2026 #
##############

PROMPT (Äá»€ TÃ€I 1 â€“ MEC Offloading & Scheduling)

MÃ¬nh Ä‘ang mÃ´ phá»ng 5G SA + MEC báº±ng Open5GS + UERANSIM trÃªn Kubernetes Ä‘á»ƒ lÃ m Ä‘á» tÃ i:
â€œNghiÃªn cá»©u chuyá»ƒn táº£i vÃ  láº­p lá»‹ch tÃ i nguyÃªn trong máº¡ng MECâ€.

1) MÃ´ hÃ¬nh há»‡ thá»‘ng hiá»‡n táº¡i
K8s cluster cÃ³ nhiá»u node:
masterk8s (Ä‘iá»u phá»‘i, cÃ³ Open5GS control-plane)
open5gs (site MEC1)
workerk8s (site MEC2)
ue (node cháº¡y UE pod)
Open5GS core: AMF cháº¡y NGAP trÃªn 10.10.3.200:38412 (multus interface n3).
2 gNB UERANSIM (mÃ´ phá»ng 2 vÃ¹ng MEC):
gNB mec1: ngapIp/gtpIp=10.10.3.231, tac=1, cellid=0x10
gNB mec2: ngapIp/gtpIp=10.10.3.232, tac=2, cellid=0x11
UE UERANSIM:
SUPI imsi-001010000000001, DNN internet, slice sst=1 sd=000001
Registration OK, PDU session OK
uesimtun0 Ä‘Æ°á»£c cáº¥p IP (vÃ­ dá»¥ 10.41.0.x)
Ping 8.8.8.8 OK tá»« UE pod â†’ user-plane ra Internet hoáº¡t Ä‘á»™ng.
Káº¿t ná»‘i N3 giá»¯a cÃ¡c node Ä‘Æ°á»£c fix báº±ng OVS bridge n3br + VXLAN (key=103, udp/4789) Ä‘á»ƒ Ä‘áº£m báº£o gNBâ†”AMF ping Ä‘Æ°á»£c vÃ  SCTP NG Setup OK.
Mobility test Ä‘Ã£ chá»©ng minh UE chuyá»ƒn TAC:
UE log: â€œSelected cell tac[1] â€¦â€ rá»“i sau Ä‘Ã³ â€œSelected cell tac[2] â€¦â€
AMF log: nháº­n InitialUEMessage vá»›i TAC[1] CellID[0x10] vÃ  sau Ä‘Ã³ TAC[2] CellID[0x11]
gNB mec2 log: nháº­n Initial NAS + Initial Context Setup + PDU session resources setup cho UE.

2) Má»¥c tiÃªu Ä‘á» tÃ i 1 (Offloading & Scheduling trong MEC)
MÃ¬nh muá»‘n xÃ¢y dá»±ng mÃ´ hÃ¬nh offloading: UE gá»­i task (request) tá»›i MEC, MEC quyáº¿t Ä‘á»‹nh:
xá»­ lÃ½ á»Ÿ MEC1/MEC2 (edge) hoáº·c offload lÃªn cloud (náº¿u cÃ³), vÃ 
láº­p lá»‹ch tÃ i nguyÃªn compute/network táº¡i MEC.
MÃ¬nh cáº§n báº¡n hÆ°á»›ng dáº«n theo cÃ¡c pháº§n:

MÃ´ hÃ¬nh hoÃ¡ bÃ i toÃ¡n:
Task model: kÃ­ch thÆ°á»›c input, sá»‘ chu ká»³ CPU/MI, deadline/latency budget, Ä‘á»™ Æ°u tiÃªn.
MEC resource model: CPU cores, queue, service rate, giá»›i háº¡n Ä‘á»“ng thá»i.
Network model: RTT UEâ†’MEC, bandwidth giáº£ láº­p, latency theo site (mec1 vs mec2).
Objective: minimize latency/violation, maximize throughput, fairness, utilization, minimize drops.
Thiáº¿t káº¿ & triá»ƒn khai 3 thuáº­t toÃ¡n láº­p lá»‹ch Ä‘á»ƒ so sÃ¡nh:
Round-robin
Priority queue (theo priority/deadline)
Latency-aware (chá»n MEC site + thá»© tá»± xá»­ lÃ½ Ä‘á»ƒ giáº£m E2E latency; cÃ³ thá»ƒ dÃ¹ng cost function)
Sinh lÆ°u lÆ°á»£ng báº±ng UERANSIM:
Tá»« UE pod táº¡o traffic HTTP/UDP (nhiá»u flow) hoáº·c nhiá»u UE pod (ue1, ue2, â€¦).
Control biáº¿n: request rate, packet size, burst, sá»‘ UE, tá»‰ lá»‡ task priority.
Äo KPI (chÆ°a cÃ³ Prometheus/Grafana):
End-to-end latency (client timestamp)
Task completion time / response time distribution (p50/p95/p99)
Throughput (req/s, Mbps)
Packet loss / retries
Queueing delay, queue length
CPU/memory utilization táº¡i MEC pods (kubectl top / cgroup stats)
SLA violation rate (deadline miss)
PhÃ¢n tÃ­ch & Ä‘Ã¡nh giÃ¡:
So sÃ¡nh 3 thuáº­t toÃ¡n theo KPI á»Ÿ nhiá»u má»©c táº£i (light/medium/heavy)
Nháº­n xÃ©t trade-off: latency vs fairness vs utilization
Káº¿t luáº­n thuáº­t toÃ¡n nÃ o phÃ¹ há»£p Ä‘iá»u kiá»‡n MEC (nhiá»u UE, mobility, bursty traffic)
Äá» xuáº¥t cáº£i tiáº¿n (hybrid, admission control, dynamic weights)

3) Hiá»‡n tráº¡ng / cáº§n lÃ m tiáº¿p ngay
Hiá»‡n core + gNB + UE Ä‘Ã£ hoáº¡t Ä‘á»™ng vÃ  cÃ³ Internet tá»« UE pod.
ChÆ°a cÃ i Prometheus/Grafana.
MÃ¬nh muá»‘n báº¯t Ä‘áº§u theo hÆ°á»›ng dá»±ng 2 MEC service (mec1/mec2) + má»™t scheduler/gateway (cÃ³ thá»ƒ lÃ  1 microservice) Ä‘á»ƒ thá»±c thi 3 thuáº­t toÃ¡n vÃ  Ä‘o KPI.
HÃ£y Ä‘á» xuáº¥t kiáº¿n trÃºc triá»ƒn khai cá»¥ thá»ƒ trÃªn K8s (pods/services, nodeSelector cho mec1/mec2), cÃ¡ch sinh traffic tá»« UE, cÃ¡ch log/thu tháº­p sá»‘ liá»‡u, vÃ  káº¿ hoáº¡ch thÃ­ nghiá»‡m (matrix cÃ¡c test cases).

ğŸ‘‰ HÃ£y tráº£ lá»i nhÆ° má»™t â€œroadmap thá»±c thi + checklist command-levelâ€, chia rÃµ tá»«ng bÆ°á»›c triá»ƒn khai vÃ  tá»«ng loáº¡i output cáº§n thu Ä‘á»ƒ viáº¿t luáº­n vÄƒn (hÃ¬nh/biá»ƒu Ä‘á»“/báº£ng so sÃ¡nh).
