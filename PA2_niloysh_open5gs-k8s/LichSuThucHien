###########################################################################################################################
### 02/01/2026
###Lỗi: root@Open5GS:~# keadm join --cloudcore-ipport=10.10.33.45:10000 --edgenode-name=                                                                                                                                                             edge2
I0102 21:46:03.469924 2441205 join.go:73] 1. Check KubeEdge edgecore process sta                                                                                                                                                             tus
I0102 21:46:03.483825 2441205 join.go:82] 2. Check if the management directory i                                                                                                                                                             s clean
I0102 21:46:03.483908 2441205 join.go:97] 3. Check if the node name is valid
Error: error making request: Get "https://10.10.33.45:10002/node/edge2": dial tc                                                                                                                                                             p 10.10.33.45:10002: connect: connection refused
### Nguyên nhân Cloudcore đang chạy trên OPEN5GS
root@MasterK8s:~# kubectl -n kubeedge get pod -o wide
NAME                        READY   STATUS    RESTARTS   AGE    IP            NODE      NOMINATED NODE   READINESS GATES
cloudcore-fbdb89495-s9wkg   1/1     Running   0          3d7h   10.10.33.54   open5gs   <none>           <none>
### Khắc phục
root@MasterK8s:~# kubectl -n kubeedge patch deploy cloudcore --type='merge' -p '{
  "spec": {
    "template": {
      "spec": {
        "nodeSelector": {
          "kubernetes.io/hostname": "masterk8s"
        },
        "tolerations": [
          { "key": "node-role.kubernetes.io/control-plane", "operator": "Exists", "effect": "NoSchedule" },
          { "key": "node-role.kubernetes.io/master", "operator": "Exists", "effect": "NoSchedule" }
        ]
      }
    }
  }
}'
kubectl -n kubeedge rollout restart deploy cloudcore
kubectl -n kubeedge get pod -o wide -w
deployment.apps/cloudcore patched
deployment.apps/cloudcore restarted
NAME                         READY   STATUS        RESTARTS   AGE    IP            NODE        NOMINATED NODE   READINESS GATES
cloudcore-6f4c49fb47-r74p9   0/1     Terminating   0          0s     10.10.33.45   masterk8s   <none>           <none>
cloudcore-fbdb89495-s9wkg    1/1     Running       0          3d7h   10.10.33.54   open5gs     <none>           <none>
cloudcore-6dffbd7696-2jt4k   0/1     Pending       0          0s     <none>        <none>      <none>           <none>
cloudcore-6dffbd7696-2jt4k   0/1     Pending       0          0s     <none>        <none>      <none>           <none>
cloudcore-6f4c49fb47-r74p9   1/1     Terminating   0          14s    10.10.33.45   masterk8s   <none>           <none>
cloudcore-6f4c49fb47-r74p9   0/1     Terminating   0          15s    10.10.33.45   masterk8s   <none>           <none>
cloudcore-6dffbd7696-2jt4k   0/1     Pending       0          15s    <none>        masterk8s   <none>           <none>
cloudcore-6dffbd7696-2jt4k   0/1     ContainerCreating   0          15s    10.10.33.45   masterk8s   <none>           <none>
cloudcore-6f4c49fb47-r74p9   0/1     Terminating         0          15s    10.10.33.45   masterk8s   <none>           <none>
cloudcore-6f4c49fb47-r74p9   0/1     Terminating         0          15s    10.10.33.45   masterk8s   <none>           <none>
cloudcore-6dffbd7696-2jt4k   1/1     Running             0          17s    10.10.33.45   masterk8s   <none>           <none>
cloudcore-fbdb89495-s9wkg    1/1     Terminating         0          3d7h   10.10.33.54   open5gs     <none>           <none>
cloudcore-fbdb89495-s9wkg    0/1     Terminating         0          3d7h   10.10.33.54   open5gs     <none>           <none>
cloudcore-fbdb89495-s9wkg    0/1     Terminating         0          3d7h   10.10.33.54   open5gs     <none>           <none>
cloudcore-fbdb89495-s9wkg    0/1     Terminating         0          3d7h   10.10.33.54   open5gs     <none>           <none>
^Croot@MasterK8s:~# kubectl -n kubeedge get pod -o wide -w
NAME                         READY   STATUS    RESTARTS   AGE     IP            NODE        NOMINATED NODE   READINESS GATES
cloudcore-6dffbd7696-2jt4k   1/1     Running   0          2m28s   10.10.33.45   masterk8s   <none>           <none>
sudo ss -lntp | egrep ':(10000|10002)\b' || true
^Croot@MasterK8s:~sudo ss -lntp | egrep ':(10000|10002)\b' || trueue
LISTEN 0      4096               *:10002            *:*    users:(("cloudcore",pid=2151739,fd=11))
LISTEN 0      4096               *:10000            *:*    users:(("cloudcore",pid=2151739,fd=10))

root@Open5GS:~# sudo keadm join --cloudcore-ipport=10.10.33.45:10000 --edgenode-name=edge2 --cgroupdriver=systemd
### Lỗi: Error: edge node join failed: timed out waiting for the condition
### Nguyên nhân: vừa join cluster k8s, vừa join cloudcore (kubeedge). Đây là 2 phương pháp khác nhau

### Làm tiếp
1) VM2: Deploy Open5GS bằng niloysh/open5gs-k8s (core + webui)

Repo có quickstart và các script deploy-core.sh / deploy-all.sh, và hướng dẫn deploy MongoDB + NAD (Multus) + Open5GS + WebUI. 
GitHub

1.1 Clone repo
git clone https://github.com/niloysh/open5gs-k8s.git
cd open5gs-k8s

1.2 Deploy core theo kiểu “guided” (dễ kiểm soát)
kubectl create namespace open5gs

kubectl apply -k mongodb -n open5gs
kubectl apply -k networks5g -n open5gs
kubectl apply -k open5gs -n open5gs
kubectl apply -k open5gs-webui -n open5gs


(Đúng theo README: MongoDB + NADs + Open5GS + WebUI). 
GitHub

1.3 Check nhanh
kubectl -n open5gs get pod -o wide
kubectl -n open5gs get svc -o wide


WebUI trong repo thường expose NodePort 30300 (README có nhắc). 
GitHub

Bạn truy cập: http://<NODE_IP>:30300 (NODE_IP có thể là 10.10.33.45 hoặc worker tùy service).

2) VM1 & VM3: Join làm worker K8s (kubeadm join)
2.1 Trên VM2: lấy lệnh join
kubeadm token create --print-join-command

2.2 Trên VM1 và VM3: chạy đúng lệnh join đó

Ví dụ dạng:

sudo kubeadm join 10.10.33.45:6443 --token ... --discovery-token-ca-cert-hash sha256:...

2.3 Check nodes

Trên VM2:

kubectl get nodes -o wide

3) Deploy 2 MEC app (pin vào VM1 và VM3)

Mục tiêu “đơn giản, thấy kết quả”: mỗi edge node chạy 1 app HTTP trả về tên node + thời gian, expose NodePort để VM4 gọi đo latency/downtime.

3.1 Tạo namespace
kubectl create ns mec

3.2 MEC app trên VM1 (node open5gs)
cat <<'EOF' | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mec-edge1
  namespace: mec
spec:
  replicas: 1
  selector:
    matchLabels: {app: mec-edge1}
  template:
    metadata:
      labels: {app: mec-edge1}
    spec:
      nodeSelector:
        kubernetes.io/hostname: open5gs
      containers:
      - name: app
        image: hashicorp/http-echo:1.0
        args: ["-text=HELLO_FROM_EDGE1_open5gs"]
        ports:
        - containerPort: 5678
---
apiVersion: v1
kind: Service
metadata:
  name: mec-edge1
  namespace: mec
spec:
  type: NodePort
  selector: {app: mec-edge1}
  ports:
  - name: http
    port: 80
    targetPort: 5678
    nodePort: 30080
EOF

3.3 MEC app trên VM3 (node workerk8s)
cat <<'EOF' | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mec-edge2
  namespace: mec
spec:
  replicas: 1
  selector:
    matchLabels: {app: mec-edge2}
  template:
    metadata:
      labels: {app: mec-edge2}
    spec:
      nodeSelector:
        kubernetes.io/hostname: workerk8s
      containers:
      - name: app
        image: hashicorp/http-echo:1.0
        args: ["-text=HELLO_FROM_EDGE2_workerk8s"]
        ports:
        - containerPort: 5678
---
apiVersion: v1
kind: Service
metadata:
  name: mec-edge2
  namespace: mec
spec:
  type: NodePort
  selector: {app: mec-edge2}
  ports:
  - name: http
    port: 80
    targetPort: 5678
    nodePort: 30081
EOF

3.4 Check
kubectl -n mec get pod -o wide
kubectl -n mec get svc -o wide

4) VM4 làm UE (traffic generator + đo MEC)
4.1 Test gọi MEC app (từ VM4)
curl -s http://10.10.33.54:30080
curl -s http://10.10.33.60:30081

4.2 Đề tài 1: “chuyển tải & lập lịch tài nguyên”

Tăng tải giả trên edge1 → dùng HPA hoặc scale replica

Hoặc cordon node để scheduler đẩy app sang node khác (dễ thấy)

Ví dụ “ép di chuyển” service:

kubectl -n mec patch deploy mec-edge1 -p '{"spec":{"template":{"spec":{"nodeSelector":{"kubernetes.io/hostname":"workerk8s"}}}}}'
kubectl -n mec rollout status deploy mec-edge1

4.3 Đề tài 2: “di chuyển dịch vụ”

Đo downtime khi “migrate” pod (rollout đổi nodeSelector) hoặc khi cordon/drain node.

Prometheus + Grafana cài ở đâu?
Khuyến nghị

Cài trong cluster K8s trên VM2, dùng kube-prometheus-stack (Prometheus Operator + Prometheus + Grafana…). Helm chart này là chuẩn phổ biến. 
GitHub
+1

Cài nhanh trên VM2
# cài helm nếu chưa có (Ubuntu)
sudo apt-get update -y
sudo apt-get install -y helm

helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

kubectl create ns monitoring

# pin về masterk8s (đỡ tốn tài nguyên MEC ở VM1/VM3)
cat <<'EOF' > /root/monitoring-values.yaml
grafana:
  service:
    type: NodePort
    nodePort: 30310
  tolerations:
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Exists"
    effect: "NoSchedule"
  nodeSelector:
    kubernetes.io/hostname: masterk8s

prometheus:
  prometheusSpec:
    tolerations:
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Exists"
      effect: "NoSchedule"
    nodeSelector:
      kubernetes.io/hostname: masterk8s
EOF

helm install mon prometheus-community/kube-prometheus-stack -n monitoring -f /root/monitoring-values.yaml


Truy cập Grafana:

http://10.10.33.45:30310

Lấy admin password:

kubectl -n monitoring get secret mon-grafana -o jsonpath='{.data.admin-password}' | base64 -d; echo

Checklist cuối (đảm bảo “hoạt động”)

Trên VM2:

kubectl get nodes -o wide
kubectl -n open5gs get pod
kubectl -n mec get pod -o wide
kubectl -n monitoring get pod


Trên VM4:

curl -s http://10.10.33.54:30080
curl -s http://10.10.33.60:30081

Gợi ý quan trọng về UERANSIM trên VM4

Với open5gs-k8s, phần UERANSIM “đơn giản nhất” thường là chạy UERANSIM trong K8s theo repo (có mục deploy-ran/UE). 
GitHub

Nếu bạn bắt buộc “VM4 chạy UERANSIM ngoài cluster”, sẽ phát sinh thêm việc expose SCTP/Multus routing (không còn tối giản). Nếu bạn vẫn muốn hướng “VM4 chạy UERANSIM thật”, nói mình biết: bạn muốn UE traffic đi qua core để ra MEC app hay chỉ cần UE mô phỏng + MEC app độc lập — mình sẽ chọn cách ít đau nhất và đưa script cấu hình tương ứng.



###########################################################################################################################
### 30/12/2025
- chuyển đổi mô hình (thực hiện phương án 2)
- xóa cấu hình cũ trên VM open5gs
- Join open5gs vào cluster k8s
- Cài Cloudcore trên masterk8
- Đang kết nối edgecore2 trên open5gs vào cloudcore (masterk8s) - Line 313


###
source https://github.com/niloysh/open5gs-k8s. để thực hiện 2 luận văn 
- nghiên cứu sự di chuyển của UE tại MEC
- nghiên cứu chuyển tải và lập lịch tài nguyên tại MEC
Hiện tại đã có 4 VM như sau:
VM1	Open5GS
VM2	MasterK8s (đã cài cluster k8s)
VM3	WokerK8s (đã join cluster k8s)
VM4	UE

cài source trên theo mô hình mới như sau:
VM2: K8s master + Open5GS core + CloudCore.
VM3: Edge node 1 (MEC app).
VM1: Edge node 2 (MEC app).
VM4: UERANSIM (RAN/UE).
###
Mô hình mới:
VM2 (MasterK8s):
Kubernetes control-plane.
Triển khai Open5GS Core (AMF, SMF, NRF, UDM).
Triển khai CloudCore (KubeEdge) để quản lý các edge node.

VM3 (WorkerK8s):
Edge node 1.
Chạy EdgeCore (KubeEdge).
Deploy MEC App (ví dụ inference service).

VM1 (Open5GS cũ):
Edge node 2.
Chạy EdgeCore (KubeEdge).
Deploy MEC App.

VM4 (UE):
UERANSIM (gNB + UE).
Sinh traffic, di chuyển UE.

Riêng VM: UE sẽ cài test lần lượt 2 cách:
- Cách 1: remove kiểm tra xem các source cũ và remove sạch, cài https://github.com/aligungr/UERANSIM

- Cách 2: remove kiểm tra xem các source cũ và remove sạch, cài: Trên VM4, clone repo open5gs-k8s:

git clone https://github.com/niloysh/open5gs-k8s
cd open5gs-k8s/ueransim
Trong thư mục này có Dockerfile và manifest → bạn có thể build image hoặc chạy binary.
Nếu muốn chạy trực tiếp trên VM4 (không join cluster):

Lấy Dockerfile hoặc script build từ đó.

Build UERANSIM binary:
make
./build/nr-gnb -c config/gnb.yaml
./build/nr-ue -c config/ue.yaml
Như vậy bạn vẫn dùng được “UERANSIM của open5gs-k8s” nhưng chạy ngoài cluster trên VM4.
###
root@MasterK8s:~# kubectl get nodes -o wide
NAME        STATUS   ROLES           AGE   VERSION    INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION       CONTAINER-RUNTIME
masterk8s   Ready    control-plane   17d   v1.30.14   10.10.33.45     <none>        Ubuntu 22.04.5 LTS   5.15.0-164-generic   containerd://1.7.28
workerk8s   Ready    <none>          16d   v1.30.14   103.121.91.87   <none>        Ubuntu 22.04.5 LTS   5.15.0-164-generic   containerd://1.7.28
root@MasterK8s:~# kubeadm token create --print-join-command
kubeadm join 10.10.33.45:6443 --token h2y2iy.omdtnf02hb6hl7t5 --discovery-token-ca-cert-hash sha256:d5f0eb55725fe43442f54be565eb3b58d43ace993a5c4fae59668271ff132c4b
###
Cách gỡ sạch Open5GS cài từ source
Xóa thư mục source code (nếu không cần giữ lại):

bash
sudo rm -rf ~/open5gs
Xóa các binary đã cài trong hệ thống (thường nằm ở /usr/local/bin):

bash
sudo rm -f /usr/local/bin/open5gs-*
Xóa thư mục config:

bash
sudo rm -rf /usr/local/etc/open5gs
Xóa thư viện và share data:

bash
sudo rm -rf /usr/local/lib/x86_64-linux-gnu/open5gs
sudo rm -rf /usr/local/share/open5gs
Xóa log và runtime data (nếu có):

bash
sudo rm -rf /usr/local/var/log/open5gs
sudo rm -rf /usr/local/var/run/open5gs
Kiểm tra service systemd (nếu bạn đã tạo):

bash
sudo systemctl stop open5gs*
sudo systemctl disable open5gs*
sudo rm -f /etc/systemd/system/open5gs*
sudo systemctl daemon-reload

root@Open5GS:~# sudo rm -rf ~/open5gs
root@Open5GS:~# sudo rm -f /usr/local/bin/open5gs-*
root@Open5GS:~# sudo rm -rf /usr/local/etc/open5gs
root@Open5GS:~# sudo rm -rf /usr/local/lib/x86_64-linux-gnu/open5gs
sudo rm -rf /usr/local/share/open5gs
root@Open5GS:~# sudo rm -rf /usr/local/var/log/open5gs
sudo rm -rf /usr/local/var/run/open5gs
root@Open5GS:~# sudo systemctl stop open5gs*
sudo systemctl disable open5gs*
sudo rm -f /etc/systemd/system/open5gs*
sudo systemctl daemon-reload
Failed to stop open5gs.pcap.service: Unit open5gs.pcap.service not loaded.
Failed to disable unit: Unit file open5gs.pcap.service does not exist.
root@Open5GS:~# ls
open5gs.pcap  sbi.pcap  snap
root@Open5GS:~# which open5gs-amfd
root@Open5GS:~# which open5gs-smfd
root@Open5GS:~# ps aux | grep open5gs
root      284750  0.0  0.3 291716 31708 ?        Sl   Dec23   0:00 /root/open5gs                                     /install/bin/open5gs-upfd
root      295273  0.0  0.7 1730820 57144 ?       Sl   Dec26   0:24 /root/open5gs                                     /install/bin/open5gs-amfd
root      295275  0.0  0.7 469752 63772 ?        Sl   Dec26   0:38 /root/open5gs                                     /install/bin/open5gs-smfd
root      295278  0.0  0.3 140940 27588 ?        Sl   Dec26   0:25 /root/open5gs                                     /install/bin/open5gs-udmd
root      295281  0.0  0.3 134088 28124 ?        Sl   Dec26   0:23 /root/open5gs                                     /install/bin/open5gs-ausfd
root      295283  0.0  0.4 170580 32844 ?        Sl   Dec26   0:23 /root/open5gs                                     /install/bin/open5gs-udrd
root      295286  0.0  0.4 190440 33356 ?        Sl   Dec26   0:25 /root/open5gs                                     /install/bin/open5gs-pcfd
root      300436  0.0  0.0   6480  2420 pts/1    S+   10:13   0:00 grep --color=                                     auto open5gs
root@Open5GS:~#
root@Open5GS:~# sudo pkill -f open5gs
root@Open5GS:~# ps aux | grep open5gs
root      300443  0.0  0.0   6480  2356 pts/1    S+   10:14   0:00 grep --color=auto open5gs
root@Open5GS:~# sudo kill -9 300443
kill: (300443): No such process
root@Open5GS:~# sudo kill -9 6480
kill: (6480): No such process
root@Open5GS:~# sudo kill -9 2356
kill: (2356): No such process
root@Open5GS:~# ps aux | grep open5gs
root      300454  0.0  0.0   6480  2416 pts/1    S+   10:15   0:00 grep --color=auto open5gs
root@Open5GS:~# sudo rm -rf /root/open5gs/install
root@Open5GS:~# which open5gs-amfd
ps aux | grep open5gs
root      300460  0.0  0.0   6480  2168 pts/1    S+   10:16   0:00 grep --color=auto open5gs
root@Open5GS:~#

###
apt update
apt install -y containerd

mkdir -p /etc/containerd
containerd config default | tee /etc/containerd/config.toml

nano /etc/containerd/config.toml
Tìm đoạn:
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
  SystemdCgroup = false
→ Sửa thành:
  SystemdCgroup = true

systemctl restart containerd
systemctl enable containerd

cat <<EOF | tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

modprobe overlay
modprobe br_netfilter

cat <<EOF | tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward = 1
EOF

sysctl --system

root@Open5GS:~# sudo swapoff -a
sudo sed -i '/ swap / s/^/#/' /etc/fstab

apt update
apt install -y apt-transport-https ca-certificates curl

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | tee /etc/apt/sources.list.d/kubernetes.list

apt update
apt install -y kubelet kubeadm kubectl

apt-mark hold kubelet kubeadm kubectl

root@Open5GS:~# kubeadm version
kubectl version --client
kubelet --version
kubeadm version: &version.Info{Major:"1", Minor:"30", GitVersion:"v1.30.14", GitCommit:"9e18483918821121abdf9aa82bc14d66df5d68cd", GitTreeState:"clean", BuildDate:"2025-06-17T18:34:53Z", GoVersion:"go1.23.10", Compiler:"gc", Platform:"linux/amd64"}
Client Version: v1.30.14
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Kubernetes v1.30.14
root@Open5GS:~#

root@MasterK8s:~# kubectl get nodes -o wide
NAME        STATUS   ROLES           AGE   VERSION    INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION       CONTAINER-RUNTIME
masterk8s   Ready    control-plane   17d   v1.30.14   10.10.33.45     <none>        Ubuntu 22.04.5 LTS   5.15.0-164-generic   containerd://1.7.28
open5gs     Ready    <none>          16m   v1.30.14   10.10.33.54     <none>        Ubuntu 22.04.5 LTS   5.15.0-164-generic   containerd://1.7.28
workerk8s   Ready    <none>          16d   v1.30.14   103.121.91.87   <none>        Ubuntu 22.04.5 LTS   5.15.0-164-generic   containerd://1.7.28

###
cài đặt keadm (KubeEdge)
1. Cài đặt keadm trên MasterK8s (VM2)
bash
# Tải bản mới nhất (ví dụ v1.15.0, bạn có thể đổi theo version mới)
wget https://github.com/kubeedge/kubeedge/releases/download/v1.15.0/keadm-v1.15.0-linux-amd64.tar.gz

# Giải nén
tar -xvf keadm-v1.15.0-linux-amd64.tar.gz

# Copy binary vào /usr/local/bin
sudo cp keadm-v1.15.0-linux-amd64/keadm/keadm /usr/local/bin/
Kiểm tra:

bash
keadm version
root@MasterK8s:~# keadm version
version: version.Info{Major:"1", Minor:"15", GitVersion:"v1.15.0", GitCommit:"3a95909a1b318add3f75fb5422f6e210ee1eaa51", GitTreeState:"clean", BuildDate:"2023-10-13T09:48:33Z", GoVersion:"go1.19.12", Compiler:"gc", Platform:"linux/amd64"}

2. Khởi tạo CloudCore trên MasterK8s
bash
keadm init --advertise-address 10.10.33.45
# Sau khi chạy, CloudCore sẽ lắng nghe kết nối từ EdgeCore.
root@MasterK8s:~# keadm init --advertise-address 10.10.33.45
Kubernetes version verification passed, KubeEdge installation will start...
CLOUDCORE started
=========CHART DETAILS=======
NAME: cloudcore
LAST DEPLOYED: Tue Dec 30 10:48:23 2025
NAMESPACE: kubeedge
STATUS: deployed
REVISION: 1
root@MasterK8s:~#

nano cloudcore-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: cloudcore
  namespace: kubeedge
spec:
  type: NodePort
  selector:
    app: cloudcore
  ports:
    - name: cloudcore-ws
      port: 10000
      targetPort: 10000
      nodePort: 30000
    - name: cloudcore-rest
      port: 10002
      targetPort: 10002
      nodePort: 30002


kubectl apply -f cloudcore-svc.yaml

3. Cài keadm trên Edge nodes (VM1, VM3)
Lặp lại bước tải và copy keadm trên VM1 và VM3.

4. Join EdgeCore từ VM1 và VM3
Trên VM1:
nano /etc/containerd/config.toml
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
  SystemdCgroup = true
Khởi động lại containerd:

bash
sudo systemctl restart containerd
Xóa dữ liệu cũ của KubeEdge (nếu có):

bash
sudo rm -rf /var/lib/kubeedge
sudo rm -rf /etc/kubeedge

keadm join --cloudcore-ipport=10.10.33.45:10000 --edgenode-name=edge2

LINE 313 - Đang kết nối edgecore2 trên open5gs vào cloudcore (masterk8s) - 
############### Lỗi
#I1230 11:17:55.120442  323952 command.go:901] 1. Check KubeEdge edgecore process status
#I1230 11:17:55.134426  323952 command.go:901] 2. Check if the management directory is clean
#Error: the management directory /etc/kubeedge/ is not clean, please remove it first
#execute keadm command failed:  the management directory /etc/kubeedge/ is not clean, please remove it first
#root@Open5GS:~# sudo systemctl restart containerd
#root@Open5GS:~# sudo rm -rf /var/lib/kubeedge
#root@Open5GS:~# keadm join --cloudcore-ipport=10.10.33.45:10000 --edgenode-name=edge2
#I1230 11:18:24.005455  324226 command.go:901] 1. Check KubeEdge edgecore process status
#I1230 11:18:24.028034  324226 command.go:901] 2. Check if the management directory is clean
#I1230 11:18:25.531904  324226 common.go:307] remote version is much newer: v1.22.0; falling back to: v1.15.0
#I1230 11:18:25.532011  324226 join.go:107] 3. Create the necessary directories
#I1230 11:18:25.540472  324226 join.go:184] 4. Pull Images
#Pulling docker.io/kubeedge/installation-package:v1.15.0 ...
#Successfully pulled docker.io/kubeedge/installation-package:v1.15.0
#Pulling docker.io/library/eclipse-mosquitto:1.6.15 ...
#Successfully pulled docker.io/library/eclipse-mosquitto:1.6.15
#Pulling docker.io/kubeedge/pause:3.6 ...
#Successfully pulled docker.io/kubeedge/pause:3.6
#I1230 11:18:25.547193  324226 join.go:184] 5. Copy resources from the image to the management directory
#E1230 11:18:28.516612  324226 remote_runtime.go:176] "RunPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: expected cgroupsPath to be of format \"slice:prefix:name\" for systemd cgroups, got \"/k8s.io/2ef78eff64653f2e0eec7334ceeb70f1ae782a17e6331a745d81c5b62205436e\" instead: unknown"
#Error: edge node join failed: copy resources failed: rpc error: code = Unknown desc = failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: expected cgroupsPath to be of format "slice:prefix:name" for systemd cgroups, got "/k8s.io/2ef78eff64653f2e0eec7334ceeb70f1ae782a17e6331a745d81c5b62205436e" instead: unknown
#execute keadm command failed:  edge node join failed: copy resources failed: rpc error: code = Unknown desc = failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: expected cgroupsPath to be of format "slice:prefix:name" for systemd cgroups, got "/k8s.io/2ef78eff64653f2e0eec7334ceeb70f1ae782a17e6331a745d81c5b62205436e" instead: unknown
#root@Open5GS:~#
############### Khắc phục cách 1: chuyển SystemdCgroup = false
############### Khắc phục cách 2: updated như Line 234 CloudCore cần một Service kiểu NodePort hoặc LoadBalancer để EdgeCore có thể kết nối từ bên ngoài cluster. Bạn có thể tạo thủ công như sau:


Vẫn lỗi 

kubectl edit cm -n kubeedge cloudcore
### Cấu hình mặc định
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
data:
  cloudcore.yaml: "apiVersion: cloudcore.config.kubeedge.io/v1alpha2\nkind: CloudCore\nkubeAPIConfig:\n
    \ kubeConfig: \"\"\n  master: \"\"\nmodules:\n  cloudHub:\n    advertiseAddress:\n
    \   - 10.10.33.45\n    dnsNames:\n    - \n    nodeLimit: 1000\n    tlsCAFile:
    /etc/kubeedge/ca/rootCA.crt\n    tlsCertFile: /etc/kubeedge/certs/edge.crt\n    tlsPrivateKeyFile:
    /etc/kubeedge/certs/edge.key\n    unixsocket:\n      address: unix:///var/lib/kubeedge/kubeedge.sock\n
    \     enable: true\n    websocket:\n      address: 0.0.0.0\n      enable: true\n
    \     port: 10000\n    quic:\n      address: 0.0.0.0\n      enable: false\n      maxIncomingStreams:
    10000\n      port: 10001\n    https:\n      address: 0.0.0.0\n      enable: true\n
    \     port: 10002\n  cloudStream:\n    enable: true\n    streamPort: 10003\n    tunnelPort:
    10004\n  dynamicController:\n    enable: false\n  router:\n    enable: false\n
    \ iptablesManager:\n    enable: true\n    mode: internal\n  nodeUpgradeJobController:\n
    \   enable: false\n"
kind: ConfigMap
metadata:
  annotations:
    meta.helm.sh/release-name: cloudcore
    meta.helm.sh/release-namespace: kubeedge
  creationTimestamp: "2025-12-30T03:48:27Z"
  labels:
    app.kubernetes.io/managed-by: Helm
    k8s-app: kubeedge
    kubeedge: cloudcore
  name: cloudcore
  namespace: kubeedge
  resourceVersion: "2153618"
  uid: 9b1ddfd4-f3fb-4390-a5ed-b9507e80fcd1

### Thay thành cấu hình mới:
apiVersion: v1
kind: ConfigMap
metadata:
  name: cloudcore
  namespace: kubeedge
data:
  cloudcore.yaml: |
    apiVersion: cloudcore.config.kubeedge.io/v1alpha2
    kind: CloudCore
    kubeAPIConfig:
      kubeConfig: ""
      master: ""
    modules:
      cloudHub:
        advertiseAddress:
          - 10.10.33.45
        dnsNames:
          - ""
        nodeLimit: 1000
        tlsCAFile: /etc/kubeedge/ca/rootCA.crt
        tlsCertFile: /etc/kubeedge/certs/edge.crt
        tlsPrivateKeyFile: /etc/kubeedge/certs/edge.key
        unixsocket:
          address: unix:///var/lib/kubeedge/kubeedge.sock
          enable: true
        websocket:
          address: 0.0.0.0
          enable: true
          port: 10000
        quic:
          address: 0.0.0.0
          enable: false
          maxIncomingStreams: 10000
          port: 10001
        https:
          address: 0.0.0.0
          enable: true
          port: 10002
        # nếu muốn disable TLS, có thể bỏ certfile/keyfile
      cloudStream:
        enable: true
        streamPort: 10003
        tunnelPort: 10004
      dynamicController:
        enable: false
      router:
        enable: false
      iptablesManager:
        enable: true
        mode: internal
      nodeUpgradeJobController:
        enable: false



Trên VM3:
bash
keadm join --cloudcore-ipport=10.10.33.45:10000 --edgenode-name=edge1
5. Kiểm tra node trong cluster
Trên MasterK8s:

bash
kubectl get nodes


###########################################################################################################################
### 29/12/2025
### Kiểm tra thông tin để xóa

root@MasterK8s:~# kubectl get pods -A
NAMESPACE     NAME                                                    READY   STATUS    RESTARTS        AGE
default       multus-test-pod                                         1/1     Running   387 (36m ago)   16d
kube-system   calico-kube-controllers-78b7fdb74b-554nk                1/1     Running   0               16d
kube-system   calico-node-5jw9g                                       0/1     Running   0               5d1h
kube-system   calico-node-m8vg8                                       0/1     Running   0               16d
kube-system   coredns-55cb58b774-2gpc8                                1/1     Running   0               17d
kube-system   coredns-55cb58b774-hqzcw                                1/1     Running   0               17d
kube-system   etcd-masterk8s                                          1/1     Running   0               17d
kube-system   kube-apiserver-masterk8s                                1/1     Running   0               17d
kube-system   kube-controller-manager-masterk8s                       1/1     Running   0               17d
kube-system   kube-multus-ds-47bmx                                    1/1     Running   0               5d1h
kube-system   kube-multus-ds-v2wx2                                    1/1     Running   0               5d1h
kube-system   kube-proxy-4bdf7                                        1/1     Running   0               16d
kube-system   kube-proxy-xrpdq                                        1/1     Running   0               17d
kube-system   kube-scheduler-masterk8s                                1/1     Running   0               17d
mec           loadgen-65cc5749b8-5ht57                                1/1     Running   0               5d2h
mec           loadgen-65cc5749b8-g4qdr                                1/1     Running   0               5d2h
mec           loadgen-65cc5749b8-hf6f7                                1/1     Running   0               5d2h
mec           loadgen-65cc5749b8-jgtks                                1/1     Running   0               5d2h
mec           loadgen-65cc5749b8-jq9zf                                1/1     Running   0               5d2h
mec           loadgen-65cc5749b8-km2zt                                1/1     Running   0               5d2h
mec           loadgen-65cc5749b8-kxzgp                                1/1     Running   0               5d2h
mec           loadgen-65cc5749b8-mqzhj                                1/1     Running   0               5d2h
mec           loadgen-65cc5749b8-nj6sh                                1/1     Running   0               5d2h
mec           loadgen-65cc5749b8-nzpmd                                1/1     Running   0               5d2h
mec           loadgen-65cc5749b8-qx6c5                                1/1     Running   0               5d2h
mec           loadgen-65cc5749b8-xzl8s                                1/1     Running   0               5d2h
mec           priority-proxy-5cbd4d9bb7-x5xhf                         1/1     Running   0               5d
mec           web-7c978b85dd-nmljr                                    1/1     Running   0               5d2h
mec           web-7c978b85dd-tdg5l                                    1/1     Running   0               5d1h
mec           web-7c978b85dd-xtwtg                                    1/1     Running   0               5d1h
monitoring    alertmanager-kps-kube-prometheus-stack-alertmanager-0   2/2     Running   0               5d
monitoring    kps-grafana-656db6b9cb-95rp2                            3/3     Running   0               4d6h
monitoring    kps-kube-prometheus-stack-operator-86865d4d87-qjj65     1/1     Running   0               5d
monitoring    kps-kube-state-metrics-5c9bb4b8c-nlb54                  1/1     Running   0               5d
monitoring    kps-prometheus-node-exporter-mphzd                      1/1     Running   0               5d
monitoring    kps-prometheus-node-exporter-sngkx                      1/1     Running   0               5d
monitoring    prometheus-kps-kube-prometheus-stack-prometheus-0       2/2     Running   0               4d6h
open5gs       upf-7dd5d45cfd-wgl6t                                    1/1     Running   5 (6d ago)      6d
root@MasterK8s:~# kubectl get deployments -A
NAMESPACE     NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE
kube-system   calico-kube-controllers              1/1     1            1           16d
kube-system   coredns                              2/2     2            2           17d
mec           loadgen                              12/12   12           12          5d2h
mec           priority-proxy                       1/1     1            1           5d
mec           web                                  3/3     3            3           5d2h
monitoring    kps-grafana                          1/1     1            1           5d
monitoring    kps-kube-prometheus-stack-operator   1/1     1            1           5d
monitoring    kps-kube-state-metrics               1/1     1            1           5d
open5gs       upf                                  1/1     1            1           6d19h
root@MasterK8s:~# kubectl get svc -A
NAMESPACE     NAME                                                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                        AGE
default       kubernetes                                          ClusterIP   10.96.0.1        <none>        443/TCP                        17d
kube-system   kps-kube-prometheus-stack-coredns                   ClusterIP   None             <none>        9153/TCP                       5d
kube-system   kps-kube-prometheus-stack-kube-controller-manager   ClusterIP   None             <none>        10257/TCP                      5d
kube-system   kps-kube-prometheus-stack-kube-etcd                 ClusterIP   None             <none>        2381/TCP                       5d
kube-system   kps-kube-prometheus-stack-kube-proxy                ClusterIP   None             <none>        10249/TCP                      5d
kube-system   kps-kube-prometheus-stack-kube-scheduler            ClusterIP   None             <none>        10259/TCP                      5d
kube-system   kps-kube-prometheus-stack-kubelet                   ClusterIP   None             <none>        10250/TCP,10255/TCP,4194/TCP   5d
kube-system   kube-dns                                            ClusterIP   10.96.0.10       <none>        53/UDP,53/TCP,9153/TCP         17d
mec           priority-proxy-svc                                  ClusterIP   10.96.41.55      <none>        80/TCP                         5d
mec           web-nodeport                                        NodePort    10.100.124.203   <none>        80:30080/TCP                   5d2h
mec           web-svc                                             ClusterIP   10.98.103.115    <none>        80/TCP                         5d2h
monitoring    alertmanager-operated                               ClusterIP   None             <none>        9093/TCP,9094/TCP,9094/UDP     5d
monitoring    kps-grafana                                         NodePort    10.97.193.198    <none>        80:30712/TCP                   5d
monitoring    kps-kube-prometheus-stack-alertmanager              ClusterIP   10.100.138.254   <none>        9093/TCP,8080/TCP              5d
monitoring    kps-kube-prometheus-stack-operator                  ClusterIP   10.102.185.168   <none>        443/TCP                        5d
monitoring    kps-kube-prometheus-stack-prometheus                ClusterIP   10.104.131.109   <none>        9090/TCP,8080/TCP              5d
monitoring    kps-kube-state-metrics                              ClusterIP   10.104.226.117   <none>        8080/TCP                       5d
monitoring    kps-prometheus-node-exporter                        ClusterIP   10.97.50.2       <none>        9100/TCP                       5d
monitoring    prometheus-nodeport                                 NodePort    10.100.246.112   <none>        9090:30390/TCP                 4d4h
monitoring    prometheus-operated                                 ClusterIP   None             <none>        9090/TCP                       5d
root@MasterK8s:~# kubectl get configmap -A
kubectl get secrets -A
NAMESPACE         NAME                                                          DATA   AGE
default           kube-root-ca.crt                                              1      17d
kube-node-lease   kube-root-ca.crt                                              1      17d
kube-public       cluster-info                                                  2      17d
kube-public       kube-root-ca.crt                                              1      17d
kube-system       calico-config                                                 4      16d
kube-system       coredns                                                       1      17d
kube-system       extension-apiserver-authentication                            6      17d
kube-system       kube-apiserver-legacy-service-account-token-tracking          1      17d
kube-system       kube-proxy                                                    2      17d
kube-system       kube-root-ca.crt                                              1      17d
kube-system       kubeadm-config                                                1      17d
kube-system       kubelet-config                                                1      17d
kube-system       multus-daemon-config                                          1      16d
mec               kube-root-ca.crt                                              1      5d2h
mec               proxy-config                                                  1      5d
monitoring        kps-grafana                                                   1      5d
monitoring        kps-grafana-config-dashboards                                 1      5d
monitoring        kps-kube-prometheus-stack-alertmanager-overview               1      5d
monitoring        kps-kube-prometheus-stack-apiserver                           1      5d
monitoring        kps-kube-prometheus-stack-cluster-total                       1      5d
monitoring        kps-kube-prometheus-stack-controller-manager                  1      5d
monitoring        kps-kube-prometheus-stack-etcd                                1      5d
monitoring        kps-kube-prometheus-stack-grafana-datasource                  1      5d
monitoring        kps-kube-prometheus-stack-grafana-overview                    1      5d
monitoring        kps-kube-prometheus-stack-k8s-coredns                         1      5d
monitoring        kps-kube-prometheus-stack-k8s-resources-cluster               1      5d
monitoring        kps-kube-prometheus-stack-k8s-resources-multicluster          1      5d
monitoring        kps-kube-prometheus-stack-k8s-resources-namespace             1      5d
monitoring        kps-kube-prometheus-stack-k8s-resources-node                  1      5d
monitoring        kps-kube-prometheus-stack-k8s-resources-pod                   1      5d
monitoring        kps-kube-prometheus-stack-k8s-resources-workload              1      5d
monitoring        kps-kube-prometheus-stack-k8s-resources-workloads-namespace   1      5d
monitoring        kps-kube-prometheus-stack-kubelet                             1      5d
monitoring        kps-kube-prometheus-stack-namespace-by-pod                    1      5d
monitoring        kps-kube-prometheus-stack-namespace-by-workload               1      5d
monitoring        kps-kube-prometheus-stack-node-cluster-rsrc-use               1      5d
monitoring        kps-kube-prometheus-stack-node-rsrc-use                       1      5d
monitoring        kps-kube-prometheus-stack-nodes                               1      5d
monitoring        kps-kube-prometheus-stack-nodes-aix                           1      5d
monitoring        kps-kube-prometheus-stack-nodes-darwin                        1      5d
monitoring        kps-kube-prometheus-stack-persistentvolumesusage              1      5d
monitoring        kps-kube-prometheus-stack-pod-total                           1      5d
monitoring        kps-kube-prometheus-stack-prometheus                          1      5d
monitoring        kps-kube-prometheus-stack-proxy                               1      5d
monitoring        kps-kube-prometheus-stack-scheduler                           1      5d
monitoring        kps-kube-prometheus-stack-workload-total                      1      5d
monitoring        kube-root-ca.crt                                              1      5d
monitoring        prometheus-kps-kube-prometheus-stack-prometheus-rulefiles-0   35     5d
open5gs           kube-root-ca.crt                                              1      8d
open5gs           upf-config                                                    1      6d19h
NAMESPACE     NAME                                                                                 TYPE                            DATA   AGE
kube-system   bootstrap-token-kg86s5                                                               bootstrap.kubernetes.io/token   6      11m
monitoring    alertmanager-kps-kube-prometheus-stack-alertmanager                                  Opaque                          1      5d
monitoring    alertmanager-kps-kube-prometheus-stack-alertmanager-cluster-tls-config               Opaque                          1      5d
monitoring    alertmanager-kps-kube-prometheus-stack-alertmanager-generated                        Opaque                          1      5d
monitoring    alertmanager-kps-kube-prometheus-stack-alertmanager-tls-assets-0                     Opaque                          0      5d
monitoring    alertmanager-kps-kube-prometheus-stack-alertmanager-web-config                       Opaque                          1      5d
monitoring    kps-grafana                                                                          Opaque                          3      5d
monitoring    kps-kube-prometheus-stack-admission                                                  Opaque                          3      5d
monitoring    prometheus-kps-kube-prometheus-stack-prometheus                                      Opaque                          1      5d
monitoring    prometheus-kps-kube-prometheus-stack-prometheus-thanos-prometheus-http-client-file   Opaque                          1      5d
monitoring    prometheus-kps-kube-prometheus-stack-prometheus-tls-assets-0                         Opaque                          1      5d
monitoring    prometheus-kps-kube-prometheus-stack-prometheus-web-config                           Opaque                          1      5d
monitoring    sh.helm.release.v1.kps.v1                                                            helm.sh/release.v1              1      5d
monitoring    sh.helm.release.v1.kps.v2                                                            helm.sh/release.v1              1      4d6h
monitoring    sh.helm.release.v1.kps.v3                                                            helm.sh/release.v1              1      4d6h
root@MasterK8s:~#

### Xóa

root@MasterK8s:~# kubectl delete namespace mec
kubectl delete namespace monitoring
kubectl delete namespace open5gs
namespace "mec" deleted
namespace "monitoring" deleted
namespace "open5gs" deleted
root@MasterK8s:~# kubectl get pods -A
kubectl get deployments -A
kubectl get svc -A
NAMESPACE     NAME                                       READY   STATUS    RESTARTS        AGE
default       multus-test-pod                            1/1     Running   387 (48m ago)   16d
kube-system   calico-kube-controllers-78b7fdb74b-554nk   1/1     Running   0               16d
kube-system   calico-node-5jw9g                          0/1     Running   0               5d2h
kube-system   calico-node-m8vg8                          0/1     Running   0               16d
kube-system   coredns-55cb58b774-2gpc8                   1/1     Running   0               17d
kube-system   coredns-55cb58b774-hqzcw                   1/1     Running   0               17d
kube-system   etcd-masterk8s                             1/1     Running   0               17d
kube-system   kube-apiserver-masterk8s                   1/1     Running   0               17d
kube-system   kube-controller-manager-masterk8s          1/1     Running   0               17d
kube-system   kube-multus-ds-47bmx                       1/1     Running   0               5d1h
kube-system   kube-multus-ds-v2wx2                       1/1     Running   0               5d1h
kube-system   kube-proxy-4bdf7                           1/1     Running   0               16d
kube-system   kube-proxy-xrpdq                           1/1     Running   0               17d
kube-system   kube-scheduler-masterk8s                   1/1     Running   0               17d
NAMESPACE     NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
kube-system   calico-kube-controllers   1/1     1            1           16d
kube-system   coredns                   2/2     2            2           17d
NAMESPACE     NAME                                                TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                        AGE
default       kubernetes                                          ClusterIP   10.96.0.1    <none>        443/TCP                        17d
kube-system   kps-kube-prometheus-stack-coredns                   ClusterIP   None         <none>        9153/TCP                       5d
kube-system   kps-kube-prometheus-stack-kube-controller-manager   ClusterIP   None         <none>        10257/TCP                      5d
kube-system   kps-kube-prometheus-stack-kube-etcd                 ClusterIP   None         <none>        2381/TCP                       5d
kube-system   kps-kube-prometheus-stack-kube-proxy                ClusterIP   None         <none>        10249/TCP                      5d
kube-system   kps-kube-prometheus-stack-kube-scheduler            ClusterIP   None         <none>        10259/TCP                      5d
kube-system   kps-kube-prometheus-stack-kubelet                   ClusterIP   None         <none>        10250/TCP,10255/TCP,4194/TCP   5d
kube-system   kube-dns                                            ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP         17d


root@MasterK8s:~# kubectl delete svc kps-kube-prometheus-stack-coredns -n kube-system
kubectl delete svc kps-kube-prometheus-stack-kube-controller-manager -n kube-system
kubectl delete svc kps-kube-prometheus-stack-kube-etcd -n kube-system
kubectl delete svc kps-kube-prometheus-stack-kube-proxy -n kube-system
kubectl delete svc kps-kube-prometheus-stack-kube-scheduler -n kube-system
kubectl delete svc kps-kube-prometheus-stack-kubelet -n kube-system
service "kps-kube-prometheus-stack-coredns" deleted
service "kps-kube-prometheus-stack-kube-controller-manager" deleted
service "kps-kube-prometheus-stack-kube-etcd" deleted
service "kps-kube-prometheus-stack-kube-proxy" deleted
service "kps-kube-prometheus-stack-kube-scheduler" deleted
service "kps-kube-prometheus-stack-kubelet" deleted
root@MasterK8s:~# kubectl delete pod multus-test-pod -n default
pod "multus-test-pod" deleted
kubectl get pods -A
kubectl get svc -A
kubectl get deployments -A

root@MasterK8s:~# kubectl get pods -A
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
kube-system   calico-kube-controllers-78b7fdb74b-554nk   1/1     Running   0          16d
kube-system   calico-node-5jw9g                          0/1     Running   0          5d2h
kube-system   calico-node-m8vg8                          0/1     Running   0          16d
kube-system   coredns-55cb58b774-2gpc8                   1/1     Running   0          17d
kube-system   coredns-55cb58b774-hqzcw                   1/1     Running   0          17d
kube-system   etcd-masterk8s                             1/1     Running   0          17d
kube-system   kube-apiserver-masterk8s                   1/1     Running   0          17d
kube-system   kube-controller-manager-masterk8s          1/1     Running   0          17d
kube-system   kube-multus-ds-47bmx                       1/1     Running   0          5d2h
kube-system   kube-multus-ds-v2wx2                       1/1     Running   0          5d2h
kube-system   kube-proxy-4bdf7                           1/1     Running   0          16d
kube-system   kube-proxy-xrpdq                           1/1     Running   0          17d
kube-system   kube-scheduler-masterk8s                   1/1     Running   0          17d
root@MasterK8s:~# kubectl get svc -A
NAMESPACE     NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
default       kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP                  17d
kube-system   kube-dns     ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   17d
root@MasterK8s:~# kubectl get deployments -A
NAMESPACE     NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
kube-system   calico-kube-controllers   1/1     1            1           16d
kube-system   coredns                   2/2     2            2           17d




